"""CAUTION: Some docstrings are generated by AI (Codeium), so they may not always make sense."""

import os
import sys
import itertools
import multiprocessing as mp
from collections import Counter


import tqdm
import pandas as pd
from tokenizers import normalizers, Regex, pre_tokenizers

# add root directory to sys path
root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) if root_dir not in sys.path else None


from data import data_aux
from tokenizer.tokenizer_aux import get_tokenizer, get_tokenizer_file_path
from config import get_random_word_set_py_config



random_word_set_save_path = root_dir + "/random_word_set/random_word_set.json"


if __name__ == "__main__":
    cfg = get_random_word_set_py_config(verbose_all=True,  verbose_changes=True)
else:
    # for pool processes name is __mp_main__, we do not want to print cfg information in them
    cfg = get_random_word_set_py_config(verbose_all=False,  verbose_changes=False)



tokenizer = get_tokenizer(cfg.tokenizer_type)


pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.WhitespaceSplit(), 
                                         pre_tokenizers.Digits(individual_digits=True),
                                         pre_tokenizers.Punctuation()])

normalizer_list = [normalizers.NFKC(),
                   normalizers.Replace(Regex('[^\w\s]'),""),
                   normalizers.Replace(Regex('\d+'),"")]


if cfg.tokenizer_type == "custom" and ("uncased" in get_tokenizer_file_path()):
    normalizer_list.insert(1, normalizers.Lowercase())
normalizer = normalizers.Sequence(normalizer_list) 



def normalize_line(line):
    """
    Normalizes a given line of text by applying the following operations:
    1. NFKC normalization
    2. Replacing all non-word characters with an empty string
    3. Replacing all digits with an empty string
    4. If the tokenizer is uncased, the line is lowercased
    
    Parameters
    ----------
    line : str
        The line of text to normalize
    
    Returns
    -------
    str
        The normalized line of text
    """
    return normalizer.normalize_str(line)

def pre_tokenize_line(line):
    """
    Pre-tokenizes a given line of text by splitting it into individual tokens 
    based on whitespace, digits, and punctuation.

    Parameters
    ----------
    line : str
        The line of text to pre-tokenize.

    Returns
    -------
    list
        A list of token strings extracted from the input line.
    """
    pretokenized_tuple = pre_tokenizer.pre_tokenize_str(line)
    return [each_tuple[0] for each_tuple in pretokenized_tuple]

def tokenize_word(row):
   """
   Tokenizes a given word string by converting it into a list of input IDs using
   the pre-trained tokenizer. The length of the tokenized list is also stored.

   Parameters
   ----------
   row : tuple
       A tuple containing the word index and the word string.

   Returns
   -------
   dict
       A dictionary containing the word index, the word string, the tokenized
       input IDs, and the length of the tokenized list.
   """
   row = row[1]
   row["token_ids"] = tokenizer(row["word"])["input_ids"]
   row["token_len"] = len(row["token_ids"])
   return row




if __name__ == "__main__":
   
   LIMIT_FOR_TOKEN_GROUP = cfg.limit_for_token_group
   MAX_WORD_LIMIT_FOR_TOKEN_GROUP = cfg.max_word_limit_for_token_group
   MIN_FREQ_FOR_WORDS = cfg.min_freq_for_words

   RANDOM_SAMPLE = cfg.random_sample
   USE_NUMBER_OF_LINE = cfg.use_number_of_line


   if os.path.exists(random_word_set_save_path):
       print(f"[INFO] {random_word_set_save_path} is already exists Terminating...")
       exit(0)


   NUM_PROCESSES = (os.cpu_count() - 1) if os.cpu_count() > 1 else 1
   print(f"[INFO] Using {NUM_PROCESSES} processes...")


   merged_file_content = data_aux.get_merged_files()
   merged_file_content = merged_file_content.splitlines()
   print("[INFO] Total number of lines: ", len(merged_file_content))

   if USE_NUMBER_OF_LINE is not None and USE_NUMBER_OF_LINE < len(merged_file_content):
       merged_file_content = merged_file_content[:USE_NUMBER_OF_LINE]
   len_merged_file_content = len(merged_file_content)
   print("[INFO] Number of lines will be used: ", len_merged_file_content)

                                  

   # multiprocessing pool
   with mp.Pool(NUM_PROCESSES) as pool:
       merged_file_content = pool.imap(normalize_line, merged_file_content, chunksize= 2048)
       merged_file_content = list(tqdm.tqdm(merged_file_content, total=len_merged_file_content, desc="[INFO] Normalizing lines..."))    

   # multiprocessing pool
   with mp.Pool(NUM_PROCESSES) as pool:
       merged_file_content = pool.imap(pre_tokenize_line, merged_file_content, chunksize= 2048)
       merged_file_content = list(tqdm.tqdm(merged_file_content, total=len_merged_file_content, desc="[INFO] Pre-tokenizing lines..."))    
   

   # list[list[str]] -> list[str]
   merged_file_content = list(itertools.chain.from_iterable(merged_file_content))

   print(f"[INFO] Counting words...")
   frequency = Counter(merged_file_content)
   most_common = frequency.most_common(50) 

   frequency_df = pd.DataFrame(frequency.items(), columns=["word", "frequency"])

   # free memory
   del merged_file_content
   del frequency 


   # New columns for token_ids and token_len
   frequency_df["token_ids"]=None
   frequency_df["token_len"]=None
   
   
   # Create a multiprocessing pool
   with mp.Pool(NUM_PROCESSES) as pool:
       iterable_freq_df = pool.imap(tokenize_word, frequency_df.iterrows(), chunksize= 216)
       frequency_df = list(tqdm.tqdm(iterable_freq_df, total=len(frequency_df), desc="[INFO] Tokenization of words..."))  
       frequency_df = pd.DataFrame(frequency_df, columns=["word", "frequency", "token_ids", "token_len"])



   print(f"[INFO] Most common 50 words: {most_common}")
   print(f"[INFO] Total number of unique words: {len(frequency_df)}")
   print(f"[INFO] Total number of all words: {frequency_df['frequency'].sum()}")

   group_list = []
   
   for group_name, group_df in frequency_df.groupby("token_len"):
       
      if int(group_name) > LIMIT_FOR_TOKEN_GROUP:
          continue

      group_df = group_df[group_df["frequency"] >= MIN_FREQ_FOR_WORDS]

      number_of_sample = len(group_df) if len(group_df) < MAX_WORD_LIMIT_FOR_TOKEN_GROUP else MAX_WORD_LIMIT_FOR_TOKEN_GROUP


      if RANDOM_SAMPLE:
          group_list.append(group_df.sample(n=number_of_sample, random_state=13013))
      else:
          group_list.append(group_df.sort_values(by="frequency", ascending=False).iloc[:number_of_sample])
          

   del frequency_df


   print(f"[INFO] Saving {random_word_set_save_path} ... ")
   
   total_df = pd.concat(group_list)
   total_df.to_json(random_word_set_save_path, orient="records", lines=True, force_ascii=False)