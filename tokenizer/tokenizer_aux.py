"""CAUTION: Some docstrings are generated by AI (Codeium), so they may not always make sense."""

import os

from transformers import PreTrainedTokenizerFast


root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


def get_tokenizer_file_path():
    """
    Retrieves the file path of the tokenizer JSON file.

    This function searches for tokenizer files matching the pattern 
    'tr_wordpiece_tokenizer_*.json' within the tokenizer directory. 
    If no file is found, it raises a FileNotFoundError. If more than 
    one file is found, it raises a ValueError, indicating that only 
    one tokenizer file should be present.

    Returns:
        str: The file path of the tokenizer JSON file.

    Raises:
        FileNotFoundError: If no tokenizer file is found.
        ValueError: If more than one tokenizer file is found.
    """
    import glob
    tokenizer_file = glob.glob(root_dir + f"/tokenizer/tr_wordpiece_tokenizer_*.json")
    if len(tokenizer_file) == 0:
        raise FileNotFoundError(f"there is no tokenizer file in {root_dir + f'/tokenizer'}")
    if len(tokenizer_file) > 1:
        raise ValueError(f"there is more than one tokenizer file in {root_dir + f'/tokenizer'}, please keep only one tokenizer file that you want to use...")
    return tokenizer_file[0]


def get_tokenizer(custom: bool = True):
    """
        tokenizer_postfix can be: [custom_cased, custom_uncased, hf]
    """
    if custom == False:
        from transformers import AutoTokenizer
        return AutoTokenizer.from_pretrained("dbmdz/bert-base-turkish-cased")
    
    tokenizer = PreTrainedTokenizerFast(
        tokenizer_file = get_tokenizer_file_path(), 
        unk_token="[UNK]",
        pad_token="[PAD]",
        cls_token="[CLS]",
        sep_token="[SEP]",
        mask_token="[MASK]",
        clean_up_tokenization_spaces=True   
    )
    return tokenizer