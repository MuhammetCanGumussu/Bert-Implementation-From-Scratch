"""some docstrings are generated by ai (codeium)"""

import os
import argparse
from dataclasses import dataclass, field, fields
from typing import Tuple, List, Type

import torch

from model.model import load_checkpoint


@dataclass
class DataConfig:
    block_size: int = field(default=256, metadata={"description": "Block size"})
    num_of_docs_per_shard: int = field(default=4_000, metadata={"description": "Number of docs per shard (for doc_shards and ab_shards creation)"})
    num_tokens_per_shard: int = field(default=10_000_000, metadata={"description": "Number of tokens per shard (for xy_shards creation)"})
    overlap: int = field(default=128, metadata={"description": "Overlap, how much overlap between windows when ab samples are generated"})
    edge_buffer: int = field(default=10, metadata={"description": "prevents the use of a and b seperators near window ends by this many tokens (makes a and b more similar length-wise) (for ab_shards creation)"})
    seed: int = field(default=13013, metadata={"description": "Seed, for reproducibility"})
    rate_of_untouched_words: float = field(default=0.85, metadata={"description": "Rate of untouched words, these are not gonna replaced by [mask, identity, replaced] tokens"})
    mask_ratio: float = field(default=0.80, metadata={"description": "Mask ratio"})
    replace_ratio: float = field(default=0.10, metadata={"description": "Replace ratio"})
    identity_ratio: float = field(default=0.10, metadata={"description": "Identity ratio"})
    tokenizer_type: str = field(default="custom", metadata={"description": "Tokenizer type, Data has been prepared/tokenized with. choices: [custom, hf]"})

@dataclass
class TrainTokenizerConfig:
    vocab_size: int = field(default=32_000, metadata={"description": "Vocab size of token embeddings"})
    limit_alphabet: int = field(default=250, metadata={"description": "Limit for alphabet"})
    min_frequency: int = field(default=2, metadata={"description": "Min frequency"})


@dataclass
class RandomWordSetConfig:
    limit_for_token_group: int = field(default=5, metadata={"description": "Limit for token group"})
    max_word_limit_for_token_group: int = field(default=5_000, metadata={"description": "Max word limit for token group"})
    min_freq_for_words: int = field(default=150, metadata={"description": "Min freq for words"})
    random_sample: bool = field(default=False, metadata={"description": "Randomly sample words (not directly most frequent ones)"})
    use_number_of_line: int = field(default=None, metadata={"description": "Use specified number of lines for training"})



@dataclass
class BertConfig:
    vocab_size: int = field(default=32000, metadata={"description": "Vocabulary size of token embeddings"})
    hidden_size: int = field(default=768, metadata={"description": "Hidden size of feed-forward layers"})
    num_hidden_layers: int = field(default=12, metadata={"description": "Number of layers in encoder"})
    num_attention_heads : int = field(default=12, metadata={"description": "Number of attention heads"})
    hidden_act: str = field(default="gelu", metadata={"description": "Activation function type choices: [gelu, relu]"})
    intermediate_size: int = field(default=3072, metadata={"description": "Size of the intermediate layer in feed-forward"}) 
    hidden_dropout_prob: float = field(default=0.1, metadata={"description": "Dropout probability for hidden layers"})
    attention_probs_dropout_prob: float = field(default=0.1, metadata={"description": "Dropout probability for attention"})
    max_position_embeddings: int = field(default=512, metadata={"description": "Maximum number of position embeddings (max block size of the model)"})
    initializer_range: float = field(default=0.02, metadata={"description": "Standard deviation for linear and embedding initialization"})
    layer_norm_eps: float = field(default=1e-12, metadata={"description": "Layer norm epsilon"})
    type_vocab_size: int = field(default=2, metadata={"description": "Type vocabulary size (for token type embeddings)"})
    classifier_dropout: float = field(default=0.1, metadata={"description": "Dropout probability for classifier"})
    # bakılacak: şimdilik 0 değerini verdim (pad_token id) ancak bunu -100 yapacaksın
    # bakılacak: ASLINDA modelin configinde ignore index olmaması gerekiyor, data config'de olmalı, model ignore index'ini (loss için) data config'den almalı!
    ignore_index: int = field(default=0, metadata={"description": "Pad token id"})   



@dataclass
class PreTrainBertConfig:
    do_train_custom: bool = field(default=True, metadata={"description": "Do train with custom model or with BERTurk hf model weights."})
    do_eval_from_best_ckpt: bool = field(default=False, metadata={"description": "Just evaluate the model from_best_ckpt but not training."})
    do_eval_from_huggingface: bool = field(default=False, metadata={"description": "Just evaluate the model from_huggingface but not training."})
    resume: bool = field(default=False, metadata={"description": "Resume training from the last step"})
    stage1_ratio: float = field(default=0.9, metadata={"description": "Ratio of stage1 (e.g. 0.9 means use block_size_s1 and train_batch_size_s1 until the end of %90 training then switch to stage2)"})
    block_size_s1: int = field(default=512, metadata={"description": "Block size for stage1"})
    block_size_s2: int = field(default=512, metadata={"description": "Block size for stage2"})
    train_batch_size_s1: int = field(default=32, metadata={"description": "Training batch size for stage1"})
    train_batch_size_s2: int = field(default=32, metadata={"description": "Training batch size for stage2"})
    val_block_size: int = field(default=512, metadata={"description": "Validation block size"})
    val_batch_size: int = field(default=8, metadata={"description": "Validation batch size"})
    grad_accum_steps: int = field(default=1, metadata={"description": "Gradient accumulation steps (micro steps)"})
    max_learning_rate: float = field(default=1e-4, metadata={"description": "Maximum learning rate"})
    min_learning_rate: float = field(default=1e-4 * 0.01, metadata={"description": "Minimum learning rate"})
    lr_scheduler: str = field(default="cosine", metadata={"description": "Learning rate scheduler choices: [linear, cosine]"})
    num_train_steps: int = field(default=1000, metadata={"description": "Number of training steps"}) 
    num_warmup_steps: int = field(default=100, metadata={"description": "Number of warmup steps"})
    save_checkpoints_steps: int = field(default=50, metadata={"description": "Save checkpoints steps"})
    val_check_steps: int = field(default=50, metadata={"description": "Validation check steps"})
    device: str = field(default="cuda", metadata={"description": "Device choices: [cpu, cuda, mps]"})
    max_eval_steps: int = field(default=50, metadata={"description": "Maximum evaluation steps (if validation set is small then max_eval_steps is gonna be treated as validation set size)"})
    weight_decay: float = field(default=0.01, metadata={"description": "Weight decay (L2 regularization)"})
    max_ckpt: int = field(default=5, metadata={"description": "Maximum number of last checkpoints to keep"})
    seed: int = field(default=13013, metadata={"description": "Random seed"})
    generate_samples: bool = field(default=True, metadata={"description": "Try model on predefined samples"})
    mlflow_tracking: bool = field(default=True, metadata={"description": "MLflow tracking"})      
    tokenizer_type: str = field(default="custom", metadata={"description": "Tokenizer type, Data has been prepared/tokenized with. choices: [custom, hf]"})





def get_last_ckpt_idx():
    """
    Retrieves the index of the last checkpoint from the model checkpoints directory.

    This function checks the "model/model_ckpts" directory for checkpoint files, 
    and determines the highest numerical index among them, ignoring any files 
    with the "best" postfix. If no checkpoint files are found, an error is raised.

    Returns:
        int: The index of the last checkpoint.

    Raises:
        FileNotFoundError: If the "model/model_ckpts" directory does not exist.
        argparse.ArgumentTypeError: If no checkpoint files are found in the directory.
    """
    
    if not os.path.exists("model/model_ckpts"):
        raise FileNotFoundError("model/model_ckpts dir is not exists, please train first")

    ckpt_files = os.listdir("model/model_ckpts")

    last_ckpt_idx = 0

    if len(ckpt_files) == 0:
        raise argparse.ArgumentTypeError("no ckpt found to resume")

    for idx, post_fix in enumerate(list(map(lambda x: x.split("_")[-1].split(".")[0], ckpt_files))):

        if post_fix == "best":
            continue

        last_ckpt_idx = int(post_fix) if int(post_fix) > last_ckpt_idx else last_ckpt_idx

    return last_ckpt_idx
    


def is_bool(value):
    """
    Determines if a given string value represents a boolean 'true' or 'false'.

    Args:
        value (str): The input string to evaluate as a boolean.

    Returns:
        bool: Returns True if the input is 'true' or '1', and False if the input is 'false' or '0'.

    Raises:
        argparse.ArgumentTypeError: If the input value cannot be interpreted as a boolean.
    """
    
    if value.lower() in ('true', '1'):
        return True
    elif value.lower() in ('false', '0'):
        return False
    raise argparse.ArgumentTypeError(f"'{value}' is not a valid boolean value.")


def compare_cfgs_if_changed(cfg: dataclass, overridden_cfg: dataclass, dont_look_fields: List[str] = [], error_msg:str = None):
    """
    Compares two dataclass configurations and raises an error if any field values differ,
    except for those specified in `dont_look_fields`.

    Args:
        cfg (dataclass): The original configuration dataclass to compare.
        overridden_cfg (dataclass): The overridden configuration dataclass to compare against.
        dont_look_fields (List[str], optional): A list of field names to ignore during comparison.
        error_msg (str, optional): The error message to display if a mismatch is found.

    Raises:
        TypeError: If `dont_look_fields` is not a list or if `cfg` and `overridden_cfg` are not of the same type.
        argparse.ArgumentTypeError: If a field value differs between `cfg` and `overridden_cfg` and the field is not in `dont_look_fields`.
    """
    
    if not isinstance(dont_look_fields, list):
        raise TypeError("dont_look_fields must be a list of strings")
    
    if type(cfg) != type(overridden_cfg):
        raise TypeError("cfg and overridden_cfg must be of same type")

    for field in fields(cfg):
        if getattr(cfg, field.name) != getattr(overridden_cfg, field.name) and field.name not in dont_look_fields:
            raise argparse.ArgumentTypeError(f"{error_msg}. You gave at least one argument for config : {field.name}: {getattr(cfg, field.name)} -> {getattr(overridden_cfg, field.name)}...")


def _parse_args(cfg_classes: Tuple[Type], verbose_changes=True, verbose_all=True) -> Tuple[dataclass]:
    """
    Parses command line arguments and overrides the default values of the given configuration classes.

    Args:
        cfg_classes (Tuple[Type]): A tuple of dataclass configuration classes to parse arguments for.
        verbose_changes (bool, optional): If True, prints information about which fields have been overridden by command line arguments. Defaults to True.
        verbose_all (bool, optional): If True, prints information about all fields.

    Returns:
        Tuple[dataclass]: A tuple of overridden configuration objects.
    """
    
    parser = argparse.ArgumentParser(description='PreTrainBertConfig and BertConfig arguments...')

    # add all config fields to parser
    for cfg_class in cfg_classes:
        for field in cfg_class.__dataclass_fields__.values():
            parser.add_argument('--' + field.name, type=field.type if field.type is not bool else is_bool, help=field.metadata["description"])

    args = parser.parse_args()
    
    # create default configs (they will be overridden by given user args)
    overridden_cfgs = [cls() for cls in cfg_classes]

    # let's override configs by given args (for not given arg use default value of config field)
    for cfg in overridden_cfgs:
        for field in cfg.__dataclass_fields__.values():
            setattr(cfg, field.name, getattr(cfg, field.name) if getattr(args, field.name) is None else getattr(args, field.name))
            if verbose_changes and getattr(cfg, field.name) != field.default:
                print(f"changed {field.name}: {field.default} -----------> {field.name}: {getattr(cfg, field.name)}")
            elif verbose_all:
                print(f"default {field.name}: {getattr(cfg, field.name)}")
    
    return overridden_cfgs




def get_train_tokenizer_py_config(verbose_changes=True, verbose_all=True) -> TrainTokenizerConfig:
    """
    Parses command line arguments and overrides the default values of the TrainTokenizerConfig class.

    Args:
        verbose_changes (bool, optional): If True, prints information about which fields have been overridden by command line arguments. Defaults to True.
        verbose_all (bool, optional): If True, prints information about all fields.

    Returns:
        TrainTokenizerConfig: An overridden TrainTokenizerConfig object.
    """
    
    overridden_cfg = _parse_args(cfg_classes=(TrainTokenizerConfig,),
                                 verbose_changes=verbose_changes,
                                 verbose_all=verbose_all)[0]
    
    # let's check all number fields are positive
    for field in fields(overridden_cfg):
        if field.type is int and getattr(overridden_cfg, field.name) < 0:
            raise argparse.ArgumentTypeError(f"{field.name} must be positive")
        
    return overridden_cfg


def get_data_py_config(verbose_changes=True, verbose_all=True) -> DataConfig:
    """
    Parses command line arguments and overrides the default values of the DataConfig class.

    Args:
        verbose_changes (bool, optional): If True, prints information about which fields have been overridden by command line arguments. Defaults to True.
        verbose_all (bool, optional): If True, prints information about all fields.

    Returns:
        DataConfig: An overridden DataConfig object.

    Raises:
        argparse.ArgumentTypeError: If any of the integer fields are negative.
        argparse.ArgumentTypeError: If mask_ratio + replace_ratio + identity_ratio is not 1.
        argparse.ArgumentTypeError: If rate_of_untouched_words is not between 0 and 1.
        argparse.ArgumentTypeError: If tokenizer_type is not 'custom' or 'hf'.
    """
    
    overridden_cfg = _parse_args(cfg_classes=(DataConfig,),
                                 verbose_changes=verbose_changes,
                                 verbose_all=verbose_all)[0]
    
    # let's check all number fields are positive
    for field in fields(overridden_cfg):
        if field.type is int and getattr(overridden_cfg, field.name) < 0:
            raise argparse.ArgumentTypeError(f"{field.name} must be positive")
    
    
    if overridden_cfg.mask_ratio + overridden_cfg.replace_ratio + overridden_cfg.identity_ratio != 1:
        raise argparse.ArgumentTypeError("mask_ratio + replace_ratio + identity_ratio must be 1 (these are also probabilities)")
    if overridden_cfg.rate_of_untouched_words > 1 or overridden_cfg.rate_of_untouched_words < 0:
        raise argparse.ArgumentTypeError("rate_of_untouched_words must be between 0 and 1")
    if overridden_cfg.tokenizer_type not in ["custom", "hf"]:
        raise argparse.ArgumentTypeError("tokenizer_type must be 'custom' or 'hf'")
    
    return overridden_cfg




def get_random_word_set_py_config(verbose_changes=True, verbose_all=True) -> RandomWordSetConfig:

    """
    Parses command line arguments and overrides the default values of the RandomWordSetConfig class.

    Args:
        verbose_changes (bool, optional): If True, prints information about which fields have been overridden by command line arguments. Defaults to True.
        verbose_all (bool, optional): If True, prints information about all fields.

    Returns:
        RandomWordSetConfig: An overridden RandomWordSetConfig object.

    Raises:
        argparse.ArgumentTypeError: If any of the integer fields are negative.
    """
    
    overridden_cfg = _parse_args(cfg_classes=(RandomWordSetConfig,),
                                 verbose_changes=verbose_changes,
                                 verbose_all=verbose_all)[0]
    
    # let's check all number fields are positive
    for field in fields(overridden_cfg):
        if field.type is int and getattr(overridden_cfg, field.name) < 0:
            raise argparse.ArgumentTypeError(f"{field.name} must be positive")
        
    return overridden_cfg
    



def get_pretrain_bert_py_configs(verbose_changes=True, verbose_all=True) -> Tuple[BertConfig, PreTrainBertConfig]:

    """
    Parses command line arguments and overrides the default values of the BertConfig and PreTrainBertConfig classes.

    Args:
        verbose_changes (bool, optional): If True, prints information about which fields have been overridden by command line arguments. Defaults to True.
        verbose_all (bool, optional): If True, prints information about all fields.

    Returns:
        Tuple[BertConfig, PreTrainBertConfig]: A tuple of overridden configuration objects.

    Raises:
        Some important combination of cfg parameters are checked and an error is raised if they are not met. (for details see code)
    """
    
    overridden_bert_cfg, overridden_pretrain_cfg = _parse_args(cfg_classes=(BertConfig, PreTrainBertConfig),
                                                               verbose_changes=verbose_changes,
                                                               verbose_all=verbose_all)
    
    # let's create default configs
    default_bert_cfg = BertConfig()
    default_pretrain_cfg = PreTrainBertConfig()

    if overridden_pretrain_cfg.resume:
        # get_last_ckpt_idx() will raise error if no ckpt found
        last_ckpt_idx = get_last_ckpt_idx()
        ckpt_dict = load_checkpoint(postfix=last_ckpt_idx)
        ckpt_bert_cfg = ckpt_dict["bert_config"]
        ckpt_pretrain_cfg = ckpt_dict["pretrain_config"]

        compare_cfgs_if_changed(cfg=ckpt_bert_cfg, overridden_cfg=overridden_bert_cfg, dont_look_fields=["resume"],
                                error_msg="for resuming training you should not give any different bert cfg args (it should use its saved bert cfg)...")
        
        # we can change some pretrain cfg args when resuming
        all_fields = set(overridden_pretrain_cfg.__dataclass_fields__.keys())
        dont_change_fields = set(["max_ckpt", "seed", "do_train_custom", "do_eval_from_best_ckpt", "do_eval_from_huggingface", "tokenizer_type"]) 

        # all_fields - dont_change_fields -> returns fields that we can change by args (so we should not look at them when comparing)
        dont_look_fields_for_pretrain = all_fields - dont_change_fields
        compare_cfgs_if_changed(cfg=ckpt_pretrain_cfg, overridden_cfg=overridden_pretrain_cfg, dont_look_fields=dont_look_fields_for_pretrain,
                                error_msg=f"for resuming training you can only give/change some pretrain cfg args: {dont_look_fields_for_pretrain}...")
        

    # you cannot change bert_config if do train with hf model weights (default config is fixed for hf model weights (BERTurk)) 
    if overridden_pretrain_cfg.do_train_custom == False:
        compare_cfgs_if_changed(cfg=default_bert_cfg, overridden_cfg=overridden_bert_cfg,
                                error_msg="for training with hf model weights (BERTurk) bert_config should be fixed (default).")

    if overridden_pretrain_cfg.do_eval_from_best_ckpt and overridden_pretrain_cfg.do_eval_from_huggingface:
        raise argparse.ArgumentTypeError("do_eval_from_best_ckpt and do_eval_from_huggingface cannot be True at the same time")
    
    # you cannot give model args for do_eval_from_best_ckpt or do_eval_from_huggingface, you can give some args for pretrain args (seed, max_eval_steps, device, val_block_size, val_batch_size)
    elif overridden_pretrain_cfg.do_eval_from_best_ckpt or overridden_pretrain_cfg.do_eval_from_huggingface:
        
        if overridden_pretrain_cfg.generate_samples == False:
            raise argparse.ArgumentTypeError("generate_samples should be True if do_eval_from_best_ckpt or do_eval_from_huggingface is True")
        
        do_eval_what = "do_eval_from_best_ckpt" if overridden_pretrain_cfg.do_eval_from_best_ckpt else "do_eval_from_huggingface"
        dont_look_fields = [do_eval_what, "seed", "max_eval_steps", "device", "val_block_size", "val_batch_size"]
        
        compare_cfgs_if_changed(cfg=default_pretrain_cfg, overridden_cfg=overridden_pretrain_cfg,
                                error_msg=f"for do_eval you should not give any pretrain cfg args rather than : {dont_look_fields}.", dont_look_fields=dont_look_fields)
        
        compare_cfgs_if_changed(cfg=default_bert_cfg, overridden_cfg=overridden_bert_cfg,
                                error_msg="for do_eval you should not give bert cfg args, it will use either best ckpt model cfg (already saved in ckpt file) or hf model default cfg...")


    if overridden_bert_cfg.hidden_act not in ["gelu", "relu"]:
        raise argparse.ArgumentTypeError("hidden_act should be gelu or relu")
    if overridden_pretrain_cfg.min_learning_rate > overridden_pretrain_cfg.max_learning_rate:
        raise argparse.ArgumentTypeError("min_learning_rate should be less than max_learning_rate")
    if overridden_pretrain_cfg.device not in ["cpu", "cuda", "mps"]:
        raise argparse.ArgumentTypeError("device should be cpu, cuda or mps")
    if overridden_pretrain_cfg.device == "cuda" and not torch.cuda.is_available():
        raise argparse.ArgumentTypeError("cuda is not available")
    if overridden_pretrain_cfg.device == "mps" and not torch.backends.mps.is_available():
        raise argparse.ArgumentTypeError("mps is not available")
    if overridden_pretrain_cfg.lr_scheduler not in ["linear", "cosine"]:
        raise argparse.ArgumentTypeError("lr_scheduler should be linear or cosine")
    if overridden_pretrain_cfg.tokenizer_type not in ["custom", "hf"]:
        raise argparse.ArgumentTypeError("tokenizer_type should be custom or hf")
    

    return overridden_bert_cfg, overridden_pretrain_cfg