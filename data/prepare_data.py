"""CAUTION: Some docstrings are generated by AI (Codeium), so they may not always make sense."""

# TODO: Make it work with custom datasets as well (it is working with tr-wiki at the moment).
# Instead of converting all files in the raw data into a single string, doc shards will be created within a loop. 
# (Indexing and other adjustments will be necessary during doc shard creation.). For custom .txt files,
# documents must be separated by newlines.


import os
import sys
import random
import multiprocessing as mp
from typing import List, Tuple

import spacy
import tqdm
import numpy as np
import pandas as pd


# add root directory to sys path
root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) if root_dir not in sys.path else None


from data_aux import FillInput, OneSampleStat, ModelInput, Stat, get_merged_files, get_last_shard_idx
from random_word_set.random_word_set_aux import get_random_word_set
from tokenizer.tokenizer_aux import get_tokenizer
from config import get_prepare_data_py_config



root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


if __name__ == "__main__":
    cfg = get_prepare_data_py_config(verbose_all=True,  verbose_changes=True)
else:
    # for pool processes name is __mp_main__, we do not want to print cfg information in them
    cfg = get_prepare_data_py_config(verbose_all=False,  verbose_changes=False)


# prepare_data.py configs
BLOCK_SIZE = cfg.block_size                                 # default: 256
NUM_OF_DOCS_PER_SHARD = cfg.num_of_docs_per_shard           # default: 4_000
NUM_TOKENS_PER_SHARD = cfg.num_tokens_per_shard             # default: 10_000_000
OVERLAP = cfg.overlap                                       # default: 128  [suggestion: make block_size//2]
EDGE_BUFFER = cfg.edge_buffer                               # default: 10
SEED = cfg.seed                                             # default: 13013
RATE_OF_UNTOUCHED_WORDS = cfg.rate_of_untouched_words       # default: 0.85
MASK_RATIO = cfg.mask_ratio                                 # default: 0.80
REPLACE_RATIO = cfg.replace_ratio                           # default: 0.10
IDENTITY_RATIO = cfg.identity_ratio                         # default: 0.10
TOKENIZER_TYPE = cfg.tokenizer_type                         # default: "custom"




NUM_PROCESSES = (os.cpu_count() - 1) if os.cpu_count() > 1 else 1

tokenizer = get_tokenizer(custom=TOKENIZER_TYPE)
word_dict = get_random_word_set()

CLS_TOKEN_ID = tokenizer.convert_tokens_to_ids("[CLS]")
SEP_TOKEN_ID = tokenizer.convert_tokens_to_ids("[SEP]")
PAD_TOKEN_ID = tokenizer.convert_tokens_to_ids("[PAD]")
MASK_TOKEN_ID = tokenizer.convert_tokens_to_ids("[MASK]")
UNK_TOKEN_ID = tokenizer.convert_tokens_to_ids("[UNK]")

# multi language sentence tokenizer
sent_seperator = spacy.load("xx_sent_ud_sm")


def appy_seed():
    random.seed(SEED) 


def delete_subtitles_from_docs(docs):
    """
    Deletes subtitles from given list of documents. Subtitles are lines with less than 5 words.

    Args:
        docs (list): List of documents. Each document is a string, where each line is a sentence.

    Returns:
        list: List of cleaned documents. Each document is a string, where each line is a sentence. Document with 0 sentences is not in the list.
    """
    new_docs = []
    for doc in docs:
        new_doc = []
        for line in doc.splitlines():
            if len(line.split(" ")) > 5:
                new_doc.append(line)
                
        # for example, doc can have 1 sentence and it has less then 5 words
        if new_doc: 
            # let's convert in the same format (join lines again)
            new_docs.append("\n".join(new_doc))
    
    
    assert not any(len(doc) == 0 for doc in new_docs), "[INFO] Unexpected, empty docs list has empty doc list..."

    return new_docs

def split_titles_and_docs(content):
    """
    Splits given content into titles and documents.

    Args:
        content (str): Content from trwiki-67 files.

    Returns:
        tuple: Tuple of two lists. First list contains titles and second list contains documents. Document with 0 sentences is not in the list.
    """
    
    titles = []
    docs = []

    
    lines = content.splitlines()
    doc_lines = []

    for line in lines:

        if line.startswith('== ') and line.endswith(' == '):
            titles.append(line)

            if doc_lines:

                docs.append("\n".join(doc_lines))
                doc_lines = []
        else:
            doc_lines.append(line)

    docs.append("\n".join(doc_lines))

    assert len(titles) == len(docs), f"[ERROR] len(titles) and len(docs) are not same!... num titles: {len(titles)}, num docs: {len(docs)}"

    return titles, docs

def get_docs_df() -> pd.DataFrame:
    """
    Returns a DataFrame containing the raw documents

    Returns
    -------
    pd.DataFrame
        A DataFrame with a single column "doc" containing the raw documents
    """

    print(f"[INFO] Docs dataframe is being created...")

    merged_content = get_merged_files()
    _, docs = split_titles_and_docs(merged_content)

    del merged_content

    # inplace operation
    random.shuffle(docs)

    docs = delete_subtitles_from_docs(docs)

    return pd.DataFrame(docs, columns=["doc"])




def create_doc_shards(docs_df):
    """
    Splits a DataFrame of documents into smaller shards and saves them as JSON files.

    This function divides the given DataFrame `docs_df` into multiple shards, each containing
    num_shards = len(docs_df) // NUM_OF_DOCS_PER_SHARD
    a specified number of documents. The shards are saved as JSON files in the `doc_shards` directory.

    Parameters
    ----------
    docs_df : pd.DataFrame
        A DataFrame containing documents to be split into shards.

    Notes
    -----
    - The number of documents per shard is determined by the global config `NUM_OF_DOCS_PER_SHARD`.
    - If the total number of documents is not evenly divisible by `NUM_OF_DOCS_PER_SHARD`, an extra
      shard is created to accommodate the remaining documents.
    - If the expected number of shard files already exists in the `doc_shards` directory, the function
      exits early without creating new shards.
    """
    
    num_shards = num_shards + 1 if len(docs_df) % NUM_OF_DOCS_PER_SHARD else num_shards
    extra_shard_len = len(docs_df) % NUM_OF_DOCS_PER_SHARD if len(docs_df) % NUM_OF_DOCS_PER_SHARD else 0

    doc_shards_dir = root_dir + "/data/doc_shards"
    os.makedirs(doc_shards_dir, exist_ok=True)

    last_shard_idx = get_last_shard_idx(doc_shards_dir) + 1

    if last_shard_idx == num_shards:
        print(f"[INFO] Expected number of doc shard files are already exists...")
        return
    
    print(f"[INFO] Creating doc shard files, starts from 0...")

    if extra_shard_len:
        print(f"[INFO] Extra shard needed. Number of doc will be placed in extra shard: {extra_shard_len}")

    
    df_start_idx = 0
    df_end_idx = NUM_OF_DOCS_PER_SHARD



    with tqdm.tqdm(total=num_shards, desc="Doc shards are being created") as pbar:
        for last_shard_idx in range(0, num_shards):

            # last iteration (extra shard)
            if last_shard_idx == num_shards - 1:
                df_end_idx = df_start_idx + extra_shard_len

                assert df_end_idx == len(docs_df), f"df_end_idx: {df_end_idx}, len(docs_df): {len(docs_df)}"
                assert (df_end_idx - df_start_idx) == extra_shard_len

                docs_df.iloc[df_start_idx:df_end_idx].to_json(doc_shards_dir + f"/doc_shard_{last_shard_idx}.json",
                                                          orient="records",
                                                          lines=True,
                                                          force_ascii=False)
                pbar.update()
                break

            docs_df.iloc[df_start_idx:df_end_idx].to_json(doc_shards_dir + f"/doc_shard_{last_shard_idx}.json",
                                                          orient="records",
                                                          lines=True,
                                                          force_ascii=False)
            df_start_idx = df_end_idx
            df_end_idx += NUM_OF_DOCS_PER_SHARD
            pbar.update()



def read_shard(shard_dir: str, shard_idx: int, return_type: str) -> pd.DataFrame | dict[str, list] | np.ndarray:
    """
    Reads a shard file from the specified directory and returns its contents in the specified format.

    Args:
        shard_dir (str): The directory containing the shard files. The directory name must start with either "doc", "ab", or "xy".
        shard_idx (int): The index of the shard to read. Must be within the valid range of shard indices.
        return_type (str): The format to return the shard data. Must be "pd" for a pandas DataFrame, "dict" for a dictionary,
                           or "np" for a numpy array if the directory starts with "xy".

    Returns:
        pd.DataFrame | dict[str, list] | np.ndarray: The contents of the shard file in the specified format.

    Raises:
        FileNotFoundError: If the specified shard directory is empty.
        IndexError: If the `shard_idx` is out of the valid range of available shards.
        ValueError: If the `return_type` is invalid for the specified shard directory type.
    """
    
    last_idx = get_last_shard_idx(shard_dir)

    if last_idx == -1:
        raise FileNotFoundError(f"{shard_dir} is empty...")

    if shard_idx > last_idx or shard_idx < 0:
        raise IndexError(f"shard idx must be >= 0 and <= {last_idx}, shard_idx you gave was: {shard_idx}")
    
    if shard_dir.split("/")[-1].startswith("xy"):
        if return_type != "np":
            raise ValueError(f"'return_type' parameter must be 'np' for xy_shards dir...")
        return np.load(shard_dir + f"/xy_shard_{shard_idx}.npy")
    

    prefix = "doc" if shard_dir.split("/")[-1].startswith("doc") else "ab"
    temp_df = pd.read_json(shard_dir + f"/{prefix}_shard_{shard_idx}.json",
                       orient="records",
                       lines=True,
                       encoding="utf-8")
    
    if return_type == "pd":
        return temp_df
    if return_type == "dict":
        return temp_df.to_dict("list")

    raise ValueError(f"'return_type' parameter must be 'pd' or 'dict' for doc or ab shards...")
    


def tokenize_and_sent_idx(doc: pd.Series)-> dict[str, list]:
    """
    Tokenizes a document and returns a dictionary with the tokenized document, word and sentence indices.

    Args:
        doc (pd.Series): A pandas Series containing the document text.

    Returns:
        dict[str, list]: A dictionary with the following keys:
            - "doc": The original document text.
            - "token_ids": The tokenized document as a list of token indices.
            - "word_ids": The tokenized document as a list of word indices.
            - "sent_idx": The sentence indices as a list of integer indices, shifted left by 1 (i.e. the first sentence starts at index 0).
    """
    if type(doc) == tuple:
        doc = doc[1]

    temp = {"doc":doc["doc"], "token_ids":[], "word_ids":[], "sent_idx":[]}

    old_idx = 0
    last_word_id = -1
    
    for sentence in list(sent_seperator(doc["doc"]).sents):
        encoding = tokenizer(sentence.text)
        if encoding["input_ids"] == []:
            continue
        old_idx = len(encoding["input_ids"]) + old_idx
        
        temp["sent_idx"].append(old_idx)
        temp["token_ids"].extend(encoding["input_ids"])
        temp["word_ids"].extend((np.array(encoding.word_ids()) + last_word_id + 1).tolist())
        last_word_id = temp["word_ids"][-1]


    # shift sent_idx left by 1
    temp["sent_idx"] = [ idx - 1 for idx in temp["sent_idx"]]

    return temp

def _get_random_sample(docs: dict[str, list], len_of_random_b):
    """
    Finds a random sample of length `len_of_random_b` from the given docs.

    Args:
        docs (dict[str, list]): A dictionary containing the tokenized documents, word and sentence indices.
        len_of_random_b (int): The length of the random sample to be found.

    Returns:
        dict[str, list] | None: A dictionary containing the random sample, or None if no sample of the given length could be found.

    Raises:
        ValueError: If `len_of_random_b` is less than or equal to zero.
    """
    random_b_dict = {"token_ids":[], "word_ids":[]}

    if len_of_random_b <= 0:
        raise ValueError(f"[ERROR] Len of number cannot be less than or equal to zero! {len_of_random_b} ")

    for rand_doc_idx in random.sample(range(len(docs["doc"])), k=len(docs["doc"])):
        doc_sent_idx_list = [0] + docs["sent_idx"][rand_doc_idx][:-1]

        for rand_sent_start_idx in random.sample(doc_sent_idx_list, k=len(doc_sent_idx_list)):
            if len(docs["token_ids"][rand_doc_idx][rand_sent_start_idx + 1:]) >= len_of_random_b:
                rand_sent_end_idx = rand_sent_start_idx + len_of_random_b
                random_b_dict["token_ids"] = docs["token_ids"][rand_doc_idx][rand_sent_start_idx + 1:rand_sent_end_idx + 1]
                random_b_dict["word_ids"] = docs["word_ids"][rand_doc_idx][rand_sent_start_idx + 1:rand_sent_end_idx + 1]
                return random_b_dict

    print(f"[INFO] Random sample couldn't be found in this shard interestingly...")
    return None

def _update_random_word_id(random_b_word_ids:List, last_word_id_of_A:int):
    """
    Updates the word ids of a random sample B to ensure it is a valid continuation of A.
    
    If the first word id of B is greater than the last word id of A, then the word ids of B are 
    shifted down by (first word id of B - last word id of A - 1). Otherwise, the word ids of B are
    shifted up by (last word id of A - first word id of B + 1). This ensures that the word ids of B 
    are contiguous with the word ids of A.
    
    Args:
        random_b_word_ids (List): The word ids of the random sample B.
        last_word_id_of_A (int): The last word id of A.
    
    Returns:
        List: The updated word ids of B.
    """

    temp = np.array(random_b_word_ids)
    
    if random_b_word_ids[0] > last_word_id_of_A:
        random_b_word_ids = temp - (random_b_word_ids[0] - last_word_id_of_A - 1)
    else:
        random_b_word_ids = temp + (last_word_id_of_A - random_b_word_ids[0] + 1)
    return random_b_word_ids.tolist()

def _fill_ab_from_block(ab_dict, doc, block_start_idx, mid_sent_idx, block_end_idx):
    """
    Fills in the ab_dict with the tokens and word ids of a block in the document.
    
    The block is split into two parts at the midpoint, and the first part is used as A, and the
    second part is used as B. The isNext label is set to True.
    
    Args:
        ab_dict (dict): A dictionary with keys "A_token_ids", "A_word_ids", "B_token_ids", "B_word_ids", and "isNext"
        doc (dict): A dictionary with keys "token_ids" and "word_ids" representing the document
        block_start_idx (int): The start index of the block
        mid_sent_idx (int): The index of the midpoint of the block
        block_end_idx (int): The end index of the block
    """
    ab_dict["A_token_ids"].append(doc["token_ids"][block_start_idx:mid_sent_idx])
    ab_dict["A_word_ids"].append(doc["word_ids"][block_start_idx:mid_sent_idx])
    ab_dict["B_token_ids"].append(doc["token_ids"][mid_sent_idx:block_end_idx])
    ab_dict["B_word_ids"].append(doc["word_ids"][mid_sent_idx:block_end_idx])
    ab_dict["isNext"].append(True)

def _fill_a_from_block_b_from_random(ab_dict, doc, docs, block_start_idx, mid_sent_idx, len_of_random_b):
    """
    Fills in the ab_dict with tokens and word ids for A from the document and B from a random sample.

    The function retrieves a random sample B of a specified length from the provided documents. If a valid sample is
    found, it appends the token ids and word ids of A from the specified indices of the given document, and the token 
    ids and updated word ids of the random sample B to the ab_dict. The isNext label is set to False.

    Args:
        ab_dict (dict): A dictionary with keys "A_token_ids", "A_word_ids", "B_token_ids", "B_word_ids", and "isNext".
        doc (dict): A dictionary representing the document with keys "token_ids" and "word_ids".
        docs (dict): A dictionary containing multiple documents with their tokenized data.
        block_start_idx (int): The start index of the block in the document.
        mid_sent_idx (int): The midpoint index of the block in the document, marking the end of A.
        len_of_random_b (int): The desired length of the random sample B.
    """
    random_b = _get_random_sample(docs, len_of_random_b)
    if random_b is not None:
        ab_dict["A_token_ids"].append(doc["token_ids"][block_start_idx:mid_sent_idx])
        ab_dict["A_word_ids"].append(doc["word_ids"][block_start_idx:mid_sent_idx])
        ab_dict["B_token_ids"].append(random_b["token_ids"][:])
        ab_dict["B_word_ids"].append(_update_random_word_id(random_b["word_ids"][:], ab_dict["A_word_ids"][-1][-1])) 
        ab_dict["isNext"].append(False)


def convert_doc_to_ab(args: Tuple)-> dict[str, list]:
    """
    Converts a document to A and B samples according to the given arguments.

    The function takes a tuple of documents and a document index as input, and returns a dictionary containing the A and B
    samples, with their respective token ids, word ids, and isNext labels.

    The function first finds the sentence indices in the block, and then randomly selects either to fill the ab_dict with
    tokens and word ids from the document (A and B) or to fill with tokens and word ids from a random sample (B).

    If the block contains no sentences, the function skips the block. If the block contains an odd number of sentences, it
    selects the midpoint index as the end of A and the start of B. If the block contains an even number of sentences, it
    selects the midpoint index and the index before it as the end of A and the start of B.

    The function also handles the case where the block is at the end of the document, by filling the ab_dict with tokens
    and word ids from the document (A and B) or from a random sample (B).

    Args:
        args (Tuple): A tuple containing the documents and the document index.

    Returns:
        dict[str, list]: A dictionary containing the A and B samples, with their respective token ids, word ids, and isNext labels.
    """

    block_size = BLOCK_SIZE
    overlap = OVERLAP
    edge_buffer = EDGE_BUFFER
    
    docs = args[0]
    doc_idx = args[1]

    doc = {key: value[doc_idx] for key, value in docs.items()}

    if type(docs) == pd.DataFrame:
        raise TypeError("[ERROR] Docs must be DictProxy or dict[str, list] type...")


    block_size_raw = block_size - 3     # special tokens (cls, sep, sep)
    block_start_idx = 0
    block_end_idx = block_size_raw

    ab_dict = {"A_token_ids":[], "B_token_ids":[], "A_word_ids":[], "B_word_ids":[], "isNext": []}


    doc_len = len(doc["token_ids"])
    while block_end_idx <= (doc_len):

        block_sent_idx = [sent_idx for sent_idx in doc["sent_idx"] if (block_start_idx + edge_buffer) < sent_idx and (block_end_idx - edge_buffer) > sent_idx]
        number_of_sent = len(block_sent_idx)


        if number_of_sent == 0:

            block_start_idx += overlap
            block_end_idx += overlap
            continue

        if number_of_sent % 2 != 0:
            mid_sent_idx = block_sent_idx[number_of_sent // 2] + 1  
        else:
            a = number_of_sent // 2
            b = a - 1
            mid_sent_idx = (block_sent_idx[a] + 1) if abs(block_sent_idx[a] - block_size_raw // 2) < abs(block_sent_idx[b] - block_size_raw // 2) else (block_sent_idx[b] + 1)

        if random.random() > 0.5:
            len_of_random_b = block_size_raw - (mid_sent_idx - block_start_idx)

            _fill_a_from_block_b_from_random(ab_dict, doc, docs, block_start_idx, mid_sent_idx, len_of_random_b)
            continue 
        

        _fill_ab_from_block(ab_dict, doc, block_start_idx, mid_sent_idx, block_end_idx)

        block_start_idx += overlap
        block_end_idx += overlap
    
    if (block_start_idx + edge_buffer) < doc_len:
        len_of_random_b = block_end_idx - doc_len
        if len_of_random_b < edge_buffer:
           return ab_dict
        _fill_a_from_block_b_from_random(ab_dict, doc, docs, block_start_idx, doc_len, len_of_random_b)

    return ab_dict




def create_ab_shards() -> None:
    """
    Converts doc shards to ab shards. If there is no shard file in doc_shards, then raise error (ab shards can't be created).
    """
    
    ab_dir = root_dir + f"/data/ab_shards_{TOKENIZER_TYPE}_{BLOCK_SIZE}"
    os.makedirs(ab_dir, exist_ok=True)

    doc_dir = root_dir + "/data/doc_shards"
    num_shards = len(os.listdir(doc_dir))

    last_shard_idx = get_last_shard_idx(ab_dir) + 1
    
    if last_shard_idx == num_shards:
        print(f"[INFO] Expected number of ab shard {BLOCK_SIZE} files are already exists in {ab_dir}...")
        return
    
    print(f"[INFO] Creating ab shard files, continues (or starts) from {last_shard_idx}...")

    for shard_idx in range(last_shard_idx, num_shards):
        docs_shard_df = read_shard(doc_dir, shard_idx, "pd")
    
        docs_shard_df["token_ids"] = None
        docs_shard_df["word_ids"] = None
        docs_shard_df["sent_idx"] = None

        
        with mp.Pool(NUM_PROCESSES) as pool:
            docs_tokenized_pool = pool.imap(tokenize_and_sent_idx, docs_shard_df.iterrows(), chunksize= 200)
            list_of_docs_tokenized = list(tqdm.tqdm(docs_tokenized_pool, total=len(docs_shard_df), desc=f"[INFO] Tokenization and sent_idx, shard_{shard_idx} / {num_shards - 1}"))

        del docs_shard_df
        del docs_tokenized_pool

        shared_docs_tokenized_dict = pd.DataFrame(list_of_docs_tokenized).to_dict("list")
        len_of_docs = len(list_of_docs_tokenized)
        del list_of_docs_tokenized


        with mp.Pool(NUM_PROCESSES) as pool:
            ab_pool = pool.imap(convert_doc_to_ab, [(shared_docs_tokenized_dict, idx) for idx in range(0, len_of_docs)], chunksize= 200)
            list_of_ab_dicts = list(tqdm.tqdm(ab_pool, total=len_of_docs, desc=f"[INFO] Creating ab, shard_{shard_idx}"))
            ab_df = pd.concat([pd.DataFrame(each_ab_dict) for each_ab_dict in list_of_ab_dicts])

        del ab_pool
        del list_of_ab_dicts

        ab_df = ab_df.sample(frac=1)
        ab_df.to_json(ab_dir + f"/ab_shard_{shard_idx}.json",
                       orient="records",
                       lines=True,
                       force_ascii=False)



def get_random_tokens(number_of_token_need: int)-> np.ndarray | None:
    """
    Retrieves a random set of tokens of a specified length from a pre-defined word dictionary.

    Args:
        number_of_token_need (int): The number of tokens needed.

    Returns:
        np.ndarray | None: An array of randomly selected tokens of the specified length if available,
        otherwise None if the token group is not found in the word dictionary.
    """
    temp = f"token_group_{number_of_token_need}"
    if temp not in word_dict.keys():
        # edge case
        # print(f"[INFO] Token group not found for this value: {number_of_token_need}")
        return None
    return np.array(random.choice(word_dict[temp]), dtype=np.uint16)

def _fill_mask(fill_input: FillInput, word_ids: np.ndarray, x: np.ndarray, y: np.ndarray, one_sample_stat: OneSampleStat=None) -> None:
    """
    Fills the input with the mask token id for given word ids in the word_ids array.

    Args:
        fill_input (FillInput): An object containing the arrays to fill.
        word_ids (np.ndarray): Array of word ids.
        x (np.ndarray): Array of input token ids to fill.
        y (np.ndarray): Array of output token ids to fill.
        one_sample_stat (OneSampleStat, optional): An object to keep track of statistics of one sample. Defaults to None.
    """
    mask_word_array = fill_input.mask_word_array
    for mask_word_id in mask_word_array:
        # binary search
        word_token_slice_start_idx = np.searchsorted(word_ids, mask_word_id, side="left")
        word_token_slice_end_idx = word_token_slice_start_idx + 1

        for idx in range(word_token_slice_start_idx + 1, len(word_ids)):
            if word_ids[idx] != mask_word_id:
                break
            word_token_slice_end_idx += 1

        if one_sample_stat is not None:
            one_sample_stat.number_of_mask_token_count += word_token_slice_end_idx - word_token_slice_start_idx
        
        word_token_slice = x[word_token_slice_start_idx:word_token_slice_end_idx].copy()
        x[word_token_slice_start_idx:word_token_slice_end_idx] = MASK_TOKEN_ID
        y[word_token_slice_start_idx:word_token_slice_end_idx] = word_token_slice
    


def _fill_replace(fill_input: FillInput, word_ids: np.ndarray, x: np.ndarray, y: np.ndarray, one_sample_stat: OneSampleStat=None) -> None:
    """
    Fills the input with the replace token id for given word ids in the word_ids array.

    Args:
        fill_input (FillInput): An object containing the arrays to fill.
        word_ids (np.ndarray): Array of word ids.
        x (np.ndarray): Array of input token ids to fill.
        y (np.ndarray): Array of output token ids to fill.
        one_sample_stat (OneSampleStat, optional): An object to keep track of statistics of one sample. Defaults to None.
    """
    replace_word_array = fill_input.replace_word_array

    for replace_word_id in replace_word_array:
        # binary search
        word_token_slice_start_idx = np.searchsorted(word_ids, replace_word_id, side="left")
        word_token_slice_end_idx = word_token_slice_start_idx + 1

        for idx in range(word_token_slice_start_idx + 1, len(word_ids)):
            if word_ids[idx] != replace_word_id:
                break
            word_token_slice_end_idx += 1

        number_of_token_need = word_token_slice_end_idx - word_token_slice_start_idx
        random_token_slice = get_random_tokens(number_of_token_need)

        if random_token_slice is None:
            if one_sample_stat is not None:
                one_sample_stat.number_of_not_accepted_word += 1
            continue

        if one_sample_stat is not None:
            one_sample_stat.number_of_replace_token_count += number_of_token_need


        word_token_slice = x[word_token_slice_start_idx:word_token_slice_end_idx].copy()
        y[word_token_slice_start_idx:word_token_slice_end_idx] = word_token_slice
        x[word_token_slice_start_idx:word_token_slice_end_idx] = random_token_slice


def _fill_identity(fill_input: FillInput, word_ids: np.ndarray, x: np.ndarray, y: np.ndarray, one_sample_stat: OneSampleStat=None) -> None:
    """
    Fills the input with the identity token id for given word ids in the word_ids array.

    Args:
        fill_input (FillInput): An object containing the arrays to fill.
        word_ids (np.ndarray): Array of word ids.
        x (np.ndarray): Array of input token ids to fill.
        y (np.ndarray): Array of output token ids to fill.
        one_sample_stat (OneSampleStat, optional): An object to keep track of statistics of one sample. Defaults to None.
    """
    identity_word_array = fill_input.identity_word_array

    for identity_word_id in identity_word_array:
        # binary search
        word_token_slice_start_idx = np.searchsorted(word_ids, identity_word_id, side="left")
        word_token_slice_end_idx = word_token_slice_start_idx + 1

        for idx in range(word_token_slice_start_idx + 1, len(word_ids)):
            if word_ids[idx] != identity_word_id:
                break
            word_token_slice_end_idx += 1

        if one_sample_stat is not None:
            one_sample_stat.number_of_identity_token_count += word_token_slice_end_idx - word_token_slice_start_idx
        
        word_token_slice = x[word_token_slice_start_idx:word_token_slice_end_idx]
        y[word_token_slice_start_idx:word_token_slice_end_idx] = word_token_slice


def _fill_xy(fill_input: FillInput, word_ids: np.ndarray, x: np.ndarray, y: np.ndarray, one_sample_stat: OneSampleStat = None) -> None:
        """
        Fills the input (x) and output (y) with the mask, replace, and identity token ids according to the given word ids and fill input.

        Args:
            fill_input (FillInput): An object containing the arrays to fill.
            word_ids (np.ndarray): Array of word ids.
            x (np.ndarray): Array of input token ids to fill.
            y (np.ndarray): Array of output token ids to fill.
            one_sample_stat (OneSampleStat, optional): An object to keep track of statistics of one sample. Defaults to None.
        """
        
        funcs = [_fill_mask, _fill_identity, _fill_replace]

        for func in funcs:
            if one_sample_stat is not None:
                func(fill_input, word_ids, x, y, one_sample_stat)
            else:
                func(fill_input, word_ids, x, y)


def _create_fill_input_for_sample(word_ids, one_sample_stat: OneSampleStat = None) -> FillInput | Tuple[FillInput, OneSampleStat]:
        """
        Creates a FillInput object for a given sample of word ids.

        Args:
            word_ids (np.ndarray): Array of word ids.
            one_sample_stat (OneSampleStat, optional): An object to keep track of statistics of one sample. Defaults to None.

        Returns:
            FillInput | Tuple[FillInput, OneSampleStat]: A FillInput object or a tuple of FillInput and OneSampleStat objects.
        """
        word_count = word_ids[-1]
        
        word_array = np.arange(word_count, dtype=np.uint16)
        prop_of_words = np.random.rand(word_count)

        
        # (1 - 0,85) * 0,80 + 0,85 = 0,970
        # (1 - 0,85) * 0,10 + 0,97 = 0,985
        # (1 - 0,85) * 0,10 + 0,985 = 1,000
        mask_bound = (1 - RATE_OF_UNTOUCHED_WORDS) * MASK_RATIO + RATE_OF_UNTOUCHED_WORDS
        replace_bound = (1 - RATE_OF_UNTOUCHED_WORDS) * REPLACE_RATIO + mask_bound
        identity_bound = (1 - RATE_OF_UNTOUCHED_WORDS) * IDENTITY_RATIO + replace_bound

        mask_of_mask = (prop_of_words > RATE_OF_UNTOUCHED_WORDS) & (prop_of_words <= mask_bound)            # (0.85, 0.97]
        mask_of_replace =  (prop_of_words > mask_bound) & (prop_of_words <= replace_bound)                  # (0.97, 0,985]
        mask_of_identity = (prop_of_words > replace_bound) & (prop_of_words <= identity_bound)              # (0,985, 1.0]
        
        
        mask_word_array = word_array[mask_of_mask]
        replace_word_array = word_array[mask_of_replace]
        identity_word_array = word_array[mask_of_identity]

        if np.all(prop_of_words <= 0.85):
            mask_word_array = np.array([np.random.randint(word_count)])

        if one_sample_stat is not None:

            one_sample_stat.number_of_word = word_count
            one_sample_stat.number_of_mask_word = len(mask_word_array)
            one_sample_stat.number_of_replace_word = len(replace_word_array)
            one_sample_stat.number_of_identity_word = len(identity_word_array)

            return FillInput(mask_word_array, identity_word_array, replace_word_array), one_sample_stat
        
        return FillInput(mask_word_array, identity_word_array, replace_word_array)


def _create_model_inputs(x: np.ndarray, y: np.ndarray, len_A_tokens: int, isNext: bool, blocks_full: bool = True) -> ModelInput:
        """
        Creates a ModelInput object for a given input (x) and output (y) numpy arrays, the length of A tokens, 
        and a boolean indicating whether the next sentence is the same or not.

        Args:
            x (np.ndarray): Array of input token ids.
            y (np.ndarray): Array of output token ids.
            len_A_tokens (int): The length of A tokens.
            isNext (bool): Whether the next sentence is the same or not.
            blocks_full (bool, optional): Whether the blocks are full or not. Defaults to True.

        Returns:
            ModelInput: A ModelInput object.
        """
        # CLS token append
        x = np.concatenate((np.array([CLS_TOKEN_ID], dtype=np.uint16), x), dtype=np.uint16)
        y = np.concatenate((np.array([PAD_TOKEN_ID], dtype=np.uint16), y), dtype=np.uint16)
        # SEP token append middle
        x = np.concatenate((x[:len_A_tokens + 1], np.array([SEP_TOKEN_ID], dtype=np.uint16), x[len_A_tokens + 1:]), dtype=np.uint16)
        y = np.concatenate((y[:len_A_tokens + 1], np.array([PAD_TOKEN_ID], dtype=np.uint16), y[len_A_tokens + 1:]), dtype=np.uint16)
        # SEP token append end
        x = np.concatenate((x, np.array([SEP_TOKEN_ID], dtype=np.uint16)), dtype=np.uint16)
        y = np.concatenate((y, np.array([PAD_TOKEN_ID], dtype=np.uint16)), dtype=np.uint16)
        next_sentence_label = np.array([isNext], dtype=np.uint16)
    

        # segment ids
        segment_ids = np.zeros(x.shape, dtype=np.uint16)
        segment_ids[len_A_tokens + 2:] = 1

        attention_mask = np.ones(x.shape, dtype=np.uint16)

        if blocks_full == False:
            for i in range((len(x) - 1), -1, -1):
                if x[i] != PAD_TOKEN_ID:
                    attention_mask[i:] = 0

        
        return ModelInput(input_ids=x, labels=y, attention_mask=attention_mask, token_type_ids=segment_ids, next_sentence_label=next_sentence_label)


def _create_xy(ab_row: pd.Series | tuple[int, pd.Series], stat_needed: bool = True)-> ModelInput | Tuple[ModelInput, OneSampleStat]:
    """
    Converts an input row from the ab dataset into a ModelInput object.

    This function processes a given ab_row, which contains token and word ids for segments A and B, 
    to create input and output arrays suitable for model input. It optionally returns statistical 
    information about the sample.

    Args:
        ab_row (pd.Series | tuple[int, pd.Series]): A pandas Series or a tuple containing index and Series 
            with keys "A_token_ids", "B_token_ids", "A_word_ids", "B_word_ids", and "isNext".
        stat_needed (bool, optional): Whether to return statistical information for the sample. 
            Defaults to True.

    Returns:
        ModelInput | Tuple[ModelInput, OneSampleStat]: A ModelInput object containing processed arrays 
        for input IDs, labels, attention masks, token type IDs, and next sentence label. If stat_needed 
        is True, also returns a OneSampleStat object containing statistics of the sample.
    """
    if isinstance(ab_row, tuple):
        ab_row = ab_row[1]

    len_A_tokens = len(ab_row["A_token_ids"])
    len_B_tokens = len(ab_row["B_token_ids"])


    if PAD_TOKEN_ID == 0:
        y = np.zeros((len_A_tokens + len_B_tokens), dtype=np.uint16)
    else:
        y = np.ones((len_A_tokens + len_B_tokens), dtype=np.uint16) * PAD_TOKEN_ID
    

    a_array = np.array(ab_row["A_token_ids"], dtype=np.uint16)
    b_array = np.array(ab_row["B_token_ids"], dtype=np.uint16)
    x = np.concatenate([a_array, b_array], dtype=np.uint16)
    del a_array
    del b_array


    a_word_ids = np.array(ab_row["A_word_ids"], dtype=np.uint16)
    b_word_ids = np.array(ab_row["B_word_ids"], dtype=np.uint16)
    word_ids = np.concatenate([a_word_ids, b_word_ids], dtype=np.uint16)
    del a_word_ids
    del b_word_ids
    
    word_ids -= word_ids[0] 

    one_sample_stat = OneSampleStat(isNext=ab_row["isNext"])    
    fill_input, one_sample_stat = _create_fill_input_for_sample(word_ids, one_sample_stat)

    _fill_xy(fill_input, word_ids, x, y, one_sample_stat) 
    
    if stat_needed:
        return _create_model_inputs(x, y, len_A_tokens, ab_row["isNext"], len_A_tokens), one_sample_stat

    return _create_model_inputs(x, y, len_A_tokens, ab_row["isNext"], len_A_tokens)



def get_num_lines_from_ab_dir() -> int:
    """read and count all lines of spesified dir"""
    num_lines = 0
    for shard_name in os.listdir(root_dir + f"/data/ab_shards_{TOKENIZER_TYPE}_{BLOCK_SIZE}"):
        with open(root_dir + f"/data/ab_shards_{TOKENIZER_TYPE}_{BLOCK_SIZE}/{shard_name}", "r", encoding="utf-8") as f:
            for _ in f:
                num_lines += 1
    return num_lines



def save_xy_shard(placeholder_array, shard_idx) -> None:
    """
    Saves a numpy array of xy shard to a specified directory.

    Args:
        placeholder_array (numpy array): The numpy array to be saved.
        shard_idx (int): The index of the shard to be saved.
    """
    np.save(root_dir + f"/data/xy_shards_{TOKENIZER_TYPE}_{BLOCK_SIZE}/xy_shard_{shard_idx}.npy", placeholder_array)



def create_xy_shards() -> None:
    """
    Converts ab shards to xy shards. If there is no shard file in ab_shards, then raise error (xy shards can't be created).

    """
    xy_dir = root_dir + f"/data/xy_shards_{TOKENIZER_TYPE}_{BLOCK_SIZE}"
    ab_dir = root_dir + f"/data/ab_shards_{TOKENIZER_TYPE}_{BLOCK_SIZE}"
    os.makedirs(xy_dir, exist_ok=True)

    number_of_sample_per_shard = NUM_TOKENS_PER_SHARD // BLOCK_SIZE
    
    # if there is no shard file in ab_shards, then raise error (xy shards can't be created)
    if get_last_shard_idx(ab_dir) == -1:
        raise FileNotFoundError(f"There is no shard file in {ab_dir}. To create xy shards please create ab_shards first.")

    total_lines_in_ab = get_num_lines_from_ab_dir() 
    number_of_shard = total_lines_in_ab // number_of_sample_per_shard if total_lines_in_ab % number_of_sample_per_shard == 0 else (total_lines_in_ab // number_of_sample_per_shard) + 1

    last_shard_idx_of_xy = get_last_shard_idx(xy_dir) + 1

    
    if last_shard_idx_of_xy == number_of_shard:
        print(f"[INFO] Expected number of files are already exists in {xy_dir}...")
        return
    
    print(f"[INFO] Creating xy shard {TOKENIZER_TYPE}, {BLOCK_SIZE} files, starts from 0 to {number_of_shard - 1} (total number of shards: {number_of_shard})...")

    last_shard_idx_of_xy = 0

    # for stat.txt
    stat = Stat(block_size = BLOCK_SIZE)

    with mp.Pool(NUM_PROCESSES) as pool:

        width = (BLOCK_SIZE * 4) + 1
        
        placeholder_array = np.empty((number_of_sample_per_shard, width), dtype=np.uint16)
        last_row_index = 0
        
        for ab_shard_idx in range(len(os.listdir(ab_dir))): 
            ab_shard_df = read_shard(ab_dir, ab_shard_idx, return_type="pd")

            xy_map_iterator = pool.imap(_create_xy, ab_shard_df.iterrows(), chunksize= 200)

            for model_input, one_sample_stat in tqdm.tqdm(xy_map_iterator, total=len(ab_shard_df), desc=f"[INFO] Converting ab shards {ab_shard_idx} to xy shards {last_shard_idx_of_xy} / {number_of_shard - 1}..."):
                
                stat.update_stat_with_another_stat(one_sample_stat)

                if last_row_index < number_of_sample_per_shard:
                    placeholder_array[last_row_index] = np.concatenate([model_input.input_ids, model_input.labels, model_input.token_type_ids, model_input.attention_mask, model_input.next_sentence_label], dtype=np.uint16)
                    last_row_index += 1
                else:
                    save_xy_shard(placeholder_array, last_shard_idx_of_xy)
                    placeholder_array = np.empty((number_of_sample_per_shard, width), dtype=np.uint16)
                    last_shard_idx_of_xy += 1
                    last_row_index = 0
            
        if last_row_index > 0:
            print(f"[INFO] Converting ab shards {ab_shard_idx} to xy shards {last_shard_idx_of_xy} / {number_of_shard - 1}, Last shard...")
            save_xy_shard(placeholder_array[:last_row_index], last_shard_idx_of_xy)

        stat.save_stat(xy_dir + "/stat.txt", cfg=cfg)




if __name__ == "__main__":

    print(f"[INFO] Number of processes: {NUM_PROCESSES}")

    appy_seed()

    docs_df = get_docs_df()

    create_doc_shards(docs_df)

    # free resource
    del docs_df

    create_ab_shards()

    create_xy_shards()

    