{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAŞLIK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create random word set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: KAYDEDİLEN DİCT'TE SADECE TOKEN LİSTLERİ OLSA YETERLİ, TOKEN LEN GRUPLARI ZATEN KEY'LER ARACILIĞI İLE ERİŞİLEBİLİR + OLARAK HER GRUBUN LENLERİ KAYDEDİLEBİLİR\n",
    "# TODO: çince arapça vs bu alfabeleri de temizlemeli!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Word Set Oluşturma Adımları\n",
    "\n",
    "1. **Kelimeleri Bulma**  \n",
    "   Girilen metin içerisinde, kelimeleri bulmalıyım. Bunu `tokenizer.normalize_str` ve `tokenizer.pre_tokenize_str` kullanarak gerçekleştireceğim ki böylece tokenizer pipeline'nına bağlı kalalım. (sayı ve punctiation karakterlerini random olarak koymak istemediğimden random word set'ten atacağım)\n",
    "\n",
    "2. **Tokenizasyon**  \n",
    "   Bulunan kelimeler, tokenize edilmelidir.\n",
    "\n",
    "3. **Statistiksel Analiz ve Temizleme**  \n",
    "   Nadir kelimeler atılmalı. Daha sonra token_len'e göre gruplaştırma yapılmalı. Bu seferde gruplara göre stat bakılmalı: 3 tokenlık kaç kelime var vs gibi. Noktalama, sayılar, semboller de atılmalı!\n",
    "\n",
    "4. **Son Temsil**  \n",
    "   Son temsil aşağıdaki formatta olmalıdır:\n",
    "   ```plaintext\n",
    "   (token_size: 5, 5_tokenlık_kelime_listesi_tokenized_btw, word_count: 5644) \n",
    "   ```\n",
    "   - `word_count` atılmamalı, rastgele indeks alırken kullanılacaktır.\n",
    "\n",
    "5. **Dosya Kaydetme**  \n",
    "   Temizlenmiş veri, bir JSON dosyası olarak kaydedilmelidir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bert_implementation_tr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# local library\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbert_implementation_tr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mold_data\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mold_data\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom_word_set\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bert_implementation_tr'"
     ]
    }
   ],
   "source": [
    "# standard library\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "from collections import Counter\n",
    "\n",
    "# third party library\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# local library\n",
    "import bert_implementation_tr.data.old_data as old_data\n",
    "import random_word_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_tokenizer = random_word_set.slow_tokenizer\n",
    "fast_tokenizer = random_word_set.fast_tokenizer\n",
    "\n",
    "\n",
    "\n",
    "LIMIT_FOR_TOKEN_GROUP = 5                   # 5 ten fazla token'a sahip kelimeleri istemiyoruz (token_len <= 5)\n",
    "MAX_WORD_LIMIT_FOR_TOKEN_GROUP = 5_000      # gruplarda da maximum belirtilen sayıda kelime olabilecek\n",
    "MIN_FREQ_FOR_WORDS = 150                    # gruplarda az occur olan kelimelerin atılmasını gerekiyor (çince, arapça vs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_merged_files() got an unexpected keyword argument 'files'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# to able to use mp pool\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m merged_file_content \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_merged_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# TODO dev time arttırmak için slice yapacağım\u001b[39;00m\n\u001b[0;32m      9\u001b[0m merged_file_content \u001b[38;5;241m=\u001b[39m merged_file_content\u001b[38;5;241m.\u001b[39msplitlines()\n",
      "\u001b[1;31mTypeError\u001b[0m: get_merged_files() got an unexpected keyword argument 'files'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    files = [\"raw/\" + file for file in os.listdir(\"raw\")]\n",
    "    # to able to use mp pool\n",
    "    merged_file_content = old_data.get_merged_files(files=files)\n",
    "\n",
    "    # TODO dev time arttırmak için slice yapacağım\n",
    "    merged_file_content = merged_file_content.splitlines()\n",
    "    len_merged_file_content = len(merged_file_content)\n",
    "\n",
    "                                   \n",
    "\n",
    "    # Create a multiprocessing pool\n",
    "    with mp.Pool(old_data.NUM_PROCESSES) as pool:\n",
    "        merged_file_content = pool.imap(random_word_set.normalize_line, merged_file_content, chunksize= 2048)\n",
    "        merged_file_content = list(tqdm.tqdm(merged_file_content, total=len_merged_file_content, desc=\"[INFO] Normalizing lines...\"))    \n",
    "\n",
    "    # Create a multiprocessing pool\n",
    "    with mp.Pool(old_data.NUM_PROCESSES) as pool:\n",
    "        merged_file_content = pool.imap(random_word_set.pre_tokenize_line, merged_file_content, chunksize= 2048)\n",
    "        merged_file_content = list(tqdm.tqdm(merged_file_content, total=len_merged_file_content, desc=\"[INFO] Pre-tokenizing lines...\"))    \n",
    "    \n",
    "\n",
    "\n",
    "    # list[list[str]] -> list[str]\n",
    "    merged_file_content = list(itertools.chain.from_iterable(merged_file_content))\n",
    "\n",
    "    print(f\"[INFO] Counting words...\")\n",
    "    frequency = Counter(merged_file_content)\n",
    "    most_common = frequency.most_common(50) \n",
    "\n",
    "    frequency_df = pd.DataFrame(frequency.items(), columns=[\"word\", \"frequency\"])\n",
    "\n",
    "    # free memory\n",
    "    del merged_file_content\n",
    "    del frequency \n",
    "\n",
    "\n",
    "    # New columns for token_ids and token_len\n",
    "    frequency_df[\"token_ids\"]=None\n",
    "    frequency_df[\"token_len\"]=None\n",
    "\n",
    "    \n",
    "    # Create a multiprocessing pool\n",
    "    with mp.Pool(old_data.NUM_PROCESSES) as pool:\n",
    "        iterable_freq_df = pool.imap(random_word_set.tokenize_word, frequency_df.iterrows(), chunksize= 216)\n",
    "        frequency_df = list(tqdm.tqdm(iterable_freq_df, total=len(frequency_df), desc=\"[INFO] Tokenization of words...\"))  \n",
    "        frequency_df = pd.DataFrame(frequency_df, columns=[\"word\", \"frequency\", \"token_ids\", \"token_len\"])\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"[INFO] Most common 50 words: {most_common}\")\n",
    "    print(f\"[INFO] Total number of unique words: {len(frequency_df)}\")\n",
    "    print(f\"[INFO] Total number of all words: {frequency_df['frequency'].sum()}\")\n",
    "\n",
    "\n",
    "    group_list = []\n",
    "    \n",
    "    for group_name, group_df in frequency_df.groupby(\"token_len\"):\n",
    "        \n",
    "        # bundan büyük token grupları ile işimiz yok\n",
    "        if int(group_name) > LIMIT_FOR_TOKEN_GROUP:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # rasgele sampling yapmadan önce grupta belli bir frequency altındaki kelimeleri atmalıyız\n",
    "        group_df = group_df[group_df[\"frequency\"] >= MIN_FREQ_FOR_WORDS]\n",
    "\n",
    "\n",
    "        number_of_sample = len(group_df) if len(group_df) < MAX_WORD_LIMIT_FOR_TOKEN_GROUP else MAX_WORD_LIMIT_FOR_TOKEN_GROUP\n",
    "\n",
    "\n",
    "        # her gruptan belirtilen sayıda sampling yapılacak\n",
    "        group_list.append(group_df.sample(n=number_of_sample, random_state=13013))\n",
    "\n",
    "\n",
    "    del frequency_df\n",
    "\n",
    "\n",
    "    print(f\"[INFO] Saving random_word_set.json file... \")\n",
    "    \n",
    "    total_df = pd.concat(group_list)\n",
    "    total_df.to_json(\"random_word_set.json\", orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'doc': 'Valongo Portekiz\\'de aynı adı taşıyan Valongo belediyesine bağlı olan, 20,66\\xa0km² yüzölçümüne sahip, 18.698 nüfuslu (2001) bir bucaktır (Portekizce: \"freguesia\")', 'token_ids': [3501, 3853, 1020, 4618, 11, 1960, 2413, 2382, 5054, 3501, 3853, 1020, 10877, 2366, 2065, \n",
    "16, 22, 20, 16, 26, 26, 2449, 22, 10255, 2427, 16, 21, 28, 18, 26, 29, 28, 9455, 12, 22, 20, 20, 21, 13, 1941, 13106, 12, 9940, 30, 6, 11812, 6, 13], 'word_ids': [0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 7, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43], 'sent_idx': [47]}\n",
    "{'doc': \"Benoit Richaud (16 Ocak, 1988; Avignon, Fransa) Fransız buz patenci. 2008 yılında Terra Findlay ile birlikte kaymaya başlamıştır. \n",
    "Çift 2008-2009 ISU Junior Grand Prix çerçevesinden Courchevel, Fransa'da düzenlenen yarışmada 4. olmuştur.\\nRichaud 2005-2007 yılları arasında Elodie Brouiller ile birlikte çalışmıştır. Takım 2006 Fransa Gençler şampiyonu ve 2007 Fransa Gençler ikincisi olmuştur. Çift iki kez Dünya Gençler Artistik Patinaj Şampiyonalarına katılmış ve 2007 Dünya Gençlerde 7. olmuştur.\\nBrouiller ile kaymaya başlamadan önce Scarlett Rouzet ile çalışmıştır. 2008 yılında Terra Findlay ile birlikte bir takım oluşturdu ve çift ilk yarışmalarına 2008-2009 ISU Junior Grand Prix çerçevesinde Fransa'da yapılan yarışmaya katıldılar. Çift zorunlu dansı ilk sırada bitirdi. Orijinal dans sırasında ise düştüler ve bu \n",
    "bölümü 11. sırada tamamladılar. Serbest dansda 3. olarak genel toplamda 4. sıraya yükseldiler\", 'token_ids': [2566, 16720, 5530, 5275, 1018, 12, 21, 26, 2797, 16, 21, 29, 28, 28, 31, 27508, 7994, 1936, 16, 3224, 13, 3142, 4541, 28169, 18, 22, 20, 20, 28, 2090, 30690, 2603, 29897, 1946, 2023, 2484, 20692, 1975, 4161, 18, 3447, 22, 20, 20, 28, 17, 22, 20, 20, 29, 24480, 15348, 6116, 8073, 9844, 1008, 21945, 5328, 7691, 16, 3224, 11, 1964, 4040, 8277, 24, 18, 2695, 18, 5530, 5275, 1018, 22, 20, 20, 25, 17, 22, 20, 20, 27, 2966, 2277, 2328, 10998, 1007, \n",
    "4491, 14564, 1937, 2023, 2484, 7953, 18, 2265, 22, 20, 20, 26, 3224, 7194, 5620, 1933, 22, 20, 20, 27, 3224, 7194, 10421, 2695, 18, 3447, 2319, 2766, 2458, 7194, 13776, 17697, 12608, 2453, 9175, 1933, 22, 20, 20, 27, 2458, 7194, 1934, 27, 18, 2695, 18, 4491, 14564, 1937, 2023, \n",
    "20692, 1975, 14208, 2523, 18831, 20512, 2440, 2149, 1948, 2023, 7953, 18, 22, 20, 20, 28, 2090, 30690, 2603, 29897, 1946, 2023, 2484, 1941, 2265, 7324, 1933, 3447, 2155, 19025, 2391, 22, 20, 20, 28, 17, 22, 20, 20, 29, 24480, 15348, 6116, 8073, 9844, 3224, 11, 1964, 2632, 10040, 20955, 18, 3447, 7993, 16729, 2155, 3376, 6450, 18, 5019, 5161, 2949, 2219, 6432, 1937, 1933, 1989, 3804, 21, 21, 18, 3376, 4966, 1927, 18, 4122, 5161, 1931, 23, 18, 2013, 2330, 5851, 24, 18, 9474, 5751, 1937], 'word_ids': [0, 0, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 26, 26, 27, 28, 29, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, \n",
    "43, 44, 45, 46, 46, 47, 47, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 58, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 70, 70, 71, 71, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 101, 102, 103, 104, 105, 106, 107, 108, 109, 109, 110, 111, 112, 113, 114, 114, 114, 115, 116, 116, 117, 118, 119, 119, 120, 120, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 130, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 172, 173, 174, 175, 176, 177, 178, 179, 180, 180, 181, 182, 183, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 192], 'sent_idx': [24, 39, 68, 92, \n",
    "110, 148, 188, 195, 211, 224]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[3107, 7027, 15038, 1926],\n",
       "  [25230, 1008, 1030, 1934],\n",
       "  [23164, 10465, 17465, 1011],\n",
       "  [18175, 9338, 2789, 2140],\n",
       "  [5578, 1948, 4270, 27026],\n",
       "  [158, 1033, 1178, 2223],\n",
       "  [151, 2501, 4481, 2381],\n",
       "  [2318, 3485, 9907, 5217],\n",
       "  [24599, 21665, 3332, 2002],\n",
       "  [20321, 12643, 1005, 2085],\n",
       "  [3248, 2046, 19699, 10331],\n",
       "  [6377, 11545, 22538, 1011],\n",
       "  [14248, 4844, 2222, 6184],\n",
       "  [7792, 2091, 2988, 5931],\n",
       "  [4180, 2276, 7587, 1016],\n",
       "  [8062, 4496, 2102, 8722],\n",
       "  [7856, 1009, 13621, 2327],\n",
       "  [14737, 20650, 1005, 1009],\n",
       "  [8150, 3776, 2994, 2577],\n",
       "  [3971, 2134, 9539, 7329],\n",
       "  [2676, 17408, 10634, 2022],\n",
       "  [4140, 4112, 6302, 2022],\n",
       "  [13450, 1921, 16228, 1019],\n",
       "  [6583, 25002, 4895, 3221],\n",
       "  [4334, 3837, 2536, 1963],\n",
       "  [2603, 4197, 3250, 3570],\n",
       "  [97, 2063, 8444, 1008],\n",
       "  [17986, 22538, 4012, 1016],\n",
       "  [2569, 20964, 18327, 1011],\n",
       "  [8812, 7587, 10861, 1025],\n",
       "  [18259, 2537, 5245, 1027],\n",
       "  [48, 2223, 18135, 1020],\n",
       "  [3592, 18121, 1201, 1030],\n",
       "  [11368, 1078, 2456, 1924],\n",
       "  [3191, 23630, 4109, 2501],\n",
       "  [13875, 3476, 6058, 1959],\n",
       "  [30675, 1957, 11657, 1009],\n",
       "  [30514, 1008, 25565, 1938],\n",
       "  [2947, 2790, 2073, 1008],\n",
       "  [3568, 9539, 13544, 12103],\n",
       "  [20878, 2058, 4392, 1016],\n",
       "  [22035, 11933, 2129, 1988],\n",
       "  [13041, 12135, 1009, 3465],\n",
       "  [21171, 1016, 2021, 1938],\n",
       "  [14823, 1021, 7838, 1011],\n",
       "  [14641, 16945, 7813, 10939],\n",
       "  [28776, 18394, 11886, 18394],\n",
       "  [15052, 5528, 4512, 2180],\n",
       "  [5269, 1016, 13654, 3218],\n",
       "  [11670, 1990, 14915, 2022],\n",
       "  [4426, 1940, 2264, 2124],\n",
       "  [14641, 16945, 7813, 2479],\n",
       "  [12899, 3461, 5734, 1008],\n",
       "  [25142, 1005, 5410, 1016],\n",
       "  [2272, 1922, 6037, 1940],\n",
       "  [21940, 2616, 1027, 2036],\n",
       "  [14849, 14778, 9729, 2124],\n",
       "  [4866, 7132, 22538, 1921],\n",
       "  [2214, 6726, 22538, 1921],\n",
       "  [9389, 1016, 22688, 1923],\n",
       "  [10449, 1922, 29897, 20801],\n",
       "  [15780, 2537, 5245, 1027],\n",
       "  [19811, 1984, 2129, 1991],\n",
       "  [2827, 3762, 6358, 2396],\n",
       "  [11927, 13809, 6149, 1011],\n",
       "  [6132, 8852, 22538, 1926],\n",
       "  [6916, 27775, 2105, 3218],\n",
       "  [9351, 3883, 10991, 8405],\n",
       "  [16575, 3415, 25902, 4555],\n",
       "  [3321, 3149, 5554, 4512],\n",
       "  [9417, 3729, 2831, 1012],\n",
       "  [17517, 4352, 3332, 1931],\n",
       "  [26287, 9316, 12519, 1012],\n",
       "  [31957, 1009, 5478, 1010],\n",
       "  [10517, 1922, 3638, 1937],\n",
       "  [7895, 2063, 1008, 2046],\n",
       "  [2931, 3247, 3218, 23869],\n",
       "  [3107, 3024, 3385, 2442],\n",
       "  [2001, 1957, 1026, 1084],\n",
       "  [2336, 2922, 15264, 3716],\n",
       "  [2193, 3407, 2751, 11297],\n",
       "  [21139, 2134, 5351, 2061],\n",
       "  [3630, 23647, 20801, 3218],\n",
       "  [9922, 1920, 14005, 19073],\n",
       "  [254, 1073, 1069, 30972],\n",
       "  [21764, 22538, 3247, 2012],\n",
       "  [4179, 12597, 16275, 2877],\n",
       "  [2838, 13736, 10277, 6555],\n",
       "  [21400, 1966, 1016, 2501],\n",
       "  [3441, 10824, 5665, 1010],\n",
       "  [2205, 15706, 4270, 3716],\n",
       "  [5904, 8424, 2095, 23527],\n",
       "  [55, 3247, 2021, 6480],\n",
       "  [28455, 4828, 1027, 1077],\n",
       "  [3102, 17880, 9069, 2391],\n",
       "  [27815, 5379, 22538, 1921],\n",
       "  [17498, 12480, 17429, 1008],\n",
       "  [12045, 4259, 3655, 1005],\n",
       "  [23063, 1920, 24173, 1009],\n",
       "  [11565, 26692, 6615, 1014],\n",
       "  [395, 1206, 19437, 21269],\n",
       "  [23225, 1982, 15303, 12901],\n",
       "  [8839, 5208, 20719, 1943],\n",
       "  [7023, 18472, 12528, 1922],\n",
       "  [2755, 3576, 4844, 1025],\n",
       "  [15617, 13688, 5513, 7991],\n",
       "  [3665, 13518, 1016, 1978],\n",
       "  [2270, 12326, 25553, 1007],\n",
       "  [19298, 7148, 11897, 2025],\n",
       "  [14517, 7027, 11697, 1923],\n",
       "  [3224, 5631, 19074, 1011],\n",
       "  [2983, 3741, 13288, 1008],\n",
       "  [8897, 18161, 22538, 1011],\n",
       "  [8604, 3342, 26944, 23110],\n",
       "  [7470, 18923, 17429, 1008],\n",
       "  [4174, 4771, 2689, 1028],\n",
       "  [21067, 1016, 14536, 1026],\n",
       "  [2931, 1947, 4270, 1957],\n",
       "  [3635, 30115, 22538, 1926],\n",
       "  [4226, 19181, 1008, 4830],\n",
       "  [8493, 1111, 10602, 1111],\n",
       "  [15503, 2524, 5731, 13201],\n",
       "  [26450, 1005, 26800, 1008],\n",
       "  [8255, 7009, 10271, 1011],\n",
       "  [30722, 22538, 4929, 1016],\n",
       "  [22563, 2129, 4289, 1010],\n",
       "  [11358, 13544, 4012, 1016],\n",
       "  [2592, 2972, 13809, 1011],\n",
       "  [4747, 10525, 5466, 1927],\n",
       "  [4949, 2021, 18838, 3119],\n",
       "  [9230, 17344, 22339, 17299],\n",
       "  [2631, 2882, 1030, 3435],\n",
       "  [13008, 1057, 30121, 2259],\n",
       "  [65, 2809, 2042, 1008],\n",
       "  [17719, 7027, 20768, 2140],\n",
       "  [9074, 2483, 27497, 1927],\n",
       "  [415, 1150, 1107, 1150],\n",
       "  [2449, 1014, 1008, 2036],\n",
       "  [3514, 7196, 2002, 12804],\n",
       "  [66, 1018, 1193, 2364],\n",
       "  [4459, 8324, 22538, 1921],\n",
       "  [15830, 2915, 14821, 2087],\n",
       "  [14363, 4209, 23548, 12135],\n",
       "  [6331, 14977, 3967, 2259],\n",
       "  [7729, 2416, 3291, 1009],\n",
       "  [16022, 2116, 10592, 2019],\n",
       "  [4921, 5613, 1008, 1032],\n",
       "  [5757, 27148, 2576, 1927],\n",
       "  [10130, 1968, 20689, 1009],\n",
       "  [2110, 9020, 5558, 1953],\n",
       "  [7856, 1009, 13621, 1025],\n",
       "  [26937, 1979, 1019, 2716],\n",
       "  [3434, 1935, 3467, 18209],\n",
       "  [7856, 1009, 13621, 1927],\n",
       "  [2288, 1978, 3564, 1008],\n",
       "  [9038, 17344, 3254, 5952],\n",
       "  [18447, 2037, 5502, 1924],\n",
       "  [8540, 3182, 2148, 2035],\n",
       "  [2764, 1957, 4486, 1990],\n",
       "  [2135, 3717, 1938, 1963],\n",
       "  [3198, 10197, 2576, 1920],\n",
       "  [7699, 1006, 13654, 1979],\n",
       "  [2059, 9611, 1012, 3201],\n",
       "  [3093, 18635, 22538, 1007],\n",
       "  [28181, 8564, 1019, 1927],\n",
       "  [12218, 3758, 1006, 7408],\n",
       "  [10219, 13933, 3791, 7908],\n",
       "  [2409, 7676, 4678, 1008],\n",
       "  [23613, 1018, 22031, 1032],\n",
       "  [27287, 1016, 1254, 1026],\n",
       "  [2325, 7027, 4345, 6037],\n",
       "  [16829, 19140, 13627, 1926],\n",
       "  [9230, 5862, 3991, 2616],\n",
       "  [15740, 1019, 21054, 1923],\n",
       "  [48, 1019, 1023, 8986],\n",
       "  [10074, 2320, 2809, 27069],\n",
       "  [3257, 14780, 22339, 1932],\n",
       "  [9389, 13081, 5542, 1019],\n",
       "  [11565, 26692, 18078, 1015],\n",
       "  [10775, 4197, 2221, 1010],\n",
       "  [16382, 23548, 1922, 1934],\n",
       "  [18413, 8952, 25059, 1926],\n",
       "  [8487, 29725, 2869, 9847],\n",
       "  [10078, 1023, 12901, 3218],\n",
       "  [8812, 3355, 10509, 13201],\n",
       "  [1960, 6037, 1010, 2383],\n",
       "  [13009, 2383, 3002, 2035],\n",
       "  [3392, 16710, 2027, 1029],\n",
       "  [24888, 3067, 1928, 1012],\n",
       "  [4169, 7645, 9729, 1008],\n",
       "  [2376, 6370, 30730, 3655],\n",
       "  [27356, 12994, 5807, 3414],\n",
       "  [12775, 1021, 20690, 1938],\n",
       "  [2295, 6427, 1928, 1059],\n",
       "  [2325, 1922, 8052, 2567],\n",
       "  [18447, 7528, 27774, 1008],\n",
       "  [2328, 2256, 2117, 1988],\n",
       "  [17356, 20576, 12103, 1923],\n",
       "  [278, 9763, 21541, 1196],\n",
       "  [31779, 1016, 31403, 2308],\n",
       "  [23000, 2978, 2558, 1011],\n",
       "  [17719, 7027, 20768, 1937],\n",
       "  [4997, 3303, 1010, 19780],\n",
       "  [66, 7646, 8656, 1949],\n",
       "  [97, 3999, 1970, 1048],\n",
       "  [13682, 7579, 2119, 1006],\n",
       "  [18529, 10770, 10032, 2169],\n",
       "  [26975, 3344, 1005, 2039],\n",
       "  [21635, 4273, 19798, 1987],\n",
       "  [30235, 1028, 22489, 14309],\n",
       "  [3105, 2690, 10673, 1979],\n",
       "  [3034, 19745, 5014, 1009],\n",
       "  [21139, 2134, 5351, 1012],\n",
       "  [9503, 26692, 13540, 1011],\n",
       "  [14196, 12871, 21354, 2544],\n",
       "  [21445, 2294, 2313, 1931],\n",
       "  [3278, 3342, 23548, 18378],\n",
       "  [6759, 1024, 5322, 1006],\n",
       "  [3249, 5952, 9611, 1006],\n",
       "  [2174, 6292, 2294, 1011],\n",
       "  [9916, 2209, 29233, 7692],\n",
       "  [110, 8790, 6051, 1979],\n",
       "  [4981, 2316, 9205, 1931],\n",
       "  [3217, 3255, 4481, 2381],\n",
       "  [4069, 2050, 3016, 1016],\n",
       "  [29137, 1984, 8048, 1030],\n",
       "  [4482, 1031, 5825, 1007],\n",
       "  [3373, 15324, 9729, 1008],\n",
       "  [7792, 4727, 25107, 1007],\n",
       "  [8493, 3250, 2689, 2022],\n",
       "  [262, 1043, 1268, 19916],\n",
       "  [12357, 3951, 22538, 1011],\n",
       "  [18743, 9611, 2141, 3081],\n",
       "  [3647, 3303, 31339, 1020],\n",
       "  [47, 31508, 1201, 1030],\n",
       "  [28356, 6164, 2309, 1016],\n",
       "  [25845, 3163, 5502, 1007],\n",
       "  [16490, 2256, 8250, 2511],\n",
       "  [4823, 7017, 1028, 1012],\n",
       "  [3324, 1026, 7240, 1011],\n",
       "  [2325, 31032, 22470, 2169],\n",
       "  [2663, 9521, 1030, 1025],\n",
       "  [2104, 1921, 13824, 4112],\n",
       "  [16310, 9803, 1940, 1020],\n",
       "  [21579, 1932, 26990, 1011],\n",
       "  [6386, 16418, 2689, 1931],\n",
       "  [13213, 31770, 4932, 2223],\n",
       "  [15503, 2524, 9439, 6724],\n",
       "  [12829, 7329, 8591, 1025],\n",
       "  [17375, 17429, 3541, 4457],\n",
       "  [7693, 3791, 11958, 17413],\n",
       "  [7775, 7018, 11886, 1007],\n",
       "  [2594, 2933, 2087, 1979],\n",
       "  [294, 15841, 15829, 22670],\n",
       "  [7699, 7613, 16945, 1008],\n",
       "  [1962, 7329, 1924, 2118],\n",
       "  [3636, 2069, 26909, 8725],\n",
       "  [2336, 2405, 2356, 1008],\n",
       "  [21277, 13538, 1021, 7789],\n",
       "  [3398, 3280, 5631, 1922],\n",
       "  [8966, 1008, 2391, 1971],\n",
       "  [10219, 2605, 1154, 1026],\n",
       "  [19424, 1922, 20869, 1007],\n",
       "  [8181, 13918, 4065, 26015],\n",
       "  [10334, 6427, 14381, 1979],\n",
       "  [30607, 12620, 1008, 2910],\n",
       "  [3069, 2005, 1030, 1934],\n",
       "  [9929, 23111, 14659, 1006],\n",
       "  [2521, 29155, 3415, 1949],\n",
       "  [3398, 3467, 16719, 1938],\n",
       "  [20199, 2291, 23246, 4391],\n",
       "  [400, 1107, 1051, 1207],\n",
       "  [15801, 2988, 1019, 29723],\n",
       "  [25787, 1019, 28815, 1011],\n",
       "  [3158, 1025, 1598, 1023],\n",
       "  [7643, 6307, 6477, 2309],\n",
       "  [10348, 3041, 6988, 2027],\n",
       "  [11577, 12472, 2050, 17429],\n",
       "  [2947, 1154, 1177, 2223],\n",
       "  [4334, 6939, 17429, 1008],\n",
       "  [5577, 7560, 4256, 1011],\n",
       "  [233, 3043, 1988, 1010],\n",
       "  [11532, 1014, 14558, 2002],\n",
       "  [5158, 1970, 10602, 3024],\n",
       "  [47, 3509, 25059, 20693],\n",
       "  [3822, 1026, 3863, 1019],\n",
       "  [9389, 2922, 7258, 1931],\n",
       "  [4710, 2223, 1028, 3664],\n",
       "  [64, 1010, 1010, 1990],\n",
       "  [2856, 7587, 11321, 5807],\n",
       "  [20836, 2242, 4676, 2391],\n",
       "  [21793, 4775, 14557, 2123],\n",
       "  [2513, 5744, 10824, 2087],\n",
       "  [5023, 26470, 8365, 1953],\n",
       "  [2563, 1946, 8149, 1008],\n",
       "  [7949, 17408, 1970, 8876],\n",
       "  [3630, 7879, 1008, 11628],\n",
       "  [24599, 21665, 3332, 1009],\n",
       "  [22771, 2008, 6254, 8986],\n",
       "  [9916, 2209, 29233, 1020],\n",
       "  [21793, 4775, 14557, 1924],\n",
       "  [17498, 8644, 4024, 2807],\n",
       "  [8062, 4496, 2102, 2057],\n",
       "  [2691, 2068, 11023, 1979],\n",
       "  [3869, 5545, 23548, 1006],\n",
       "  [7143, 9272, 2954, 3487],\n",
       "  [9251, 3344, 1005, 4299],\n",
       "  [4497, 2531, 2789, 2278],\n",
       "  [27425, 19153, 2264, 2391],\n",
       "  [2197, 4112, 5749, 3407],\n",
       "  [2099, 2084, 19942, 1982],\n",
       "  [50, 1154, 4397, 1024],\n",
       "  [2421, 23219, 4771, 1007],\n",
       "  [6673, 2221, 10991, 1009],\n",
       "  [15517, 4476, 6584, 1014],\n",
       "  [6150, 2235, 2292, 1011],\n",
       "  [1989, 1033, 5931, 1016],\n",
       "  [2440, 1949, 3429, 1938],\n",
       "  [7856, 1009, 13621, 2261],\n",
       "  [15522, 7813, 28955, 2294],\n",
       "  [24377, 12393, 4065, 2391],\n",
       "  [11660, 1026, 1030, 2932],\n",
       "  [2059, 9611, 1994, 6004],\n",
       "  [13506, 5807, 2223, 2018],\n",
       "  [13577, 2021, 1154, 8238],\n",
       "  [16023, 2106, 1112, 1931],\n",
       "  [25787, 7569, 17720, 2479],\n",
       "  [22461, 3464, 26056, 1016],\n",
       "  [3636, 27628, 11001, 1931],\n",
       "  [24451, 5331, 27462, 11214],\n",
       "  [27884, 18327, 3991, 2225],\n",
       "  [4664, 2045, 14145, 1016],\n",
       "  [12434, 1028, 19953, 1975],\n",
       "  [4500, 2037, 1963, 2057],\n",
       "  [3958, 1990, 8743, 3092],\n",
       "  [2176, 7310, 13288, 1011],\n",
       "  [6610, 1053, 29943, 1008],\n",
       "  [2177, 4250, 2893, 1924],\n",
       "  [24889, 22538, 1936, 13108],\n",
       "  [9111, 25265, 5153, 1025],\n",
       "  [2498, 6677, 25776, 1011],\n",
       "  [3816, 1966, 1016, 1927],\n",
       "  [4518, 2386, 1966, 1016],\n",
       "  [14985, 2536, 26692, 2129],\n",
       "  [5873, 21354, 31184, 1008],\n",
       "  [4169, 1949, 19606, 1007],\n",
       "  [20364, 11673, 2880, 1931],\n",
       "  [5654, 3883, 20928, 1011],\n",
       "  [2318, 4328, 5734, 2391],\n",
       "  [2466, 3596, 4844, 1995],\n",
       "  [9576, 2962, 3998, 1011],\n",
       "  [5481, 17990, 12622, 3218],\n",
       "  [398, 1093, 1107, 1052],\n",
       "  [421, 1093, 1259, 1273],\n",
       "  [126, 17344, 3254, 1138],\n",
       "  [29023, 3461, 1021, 1059],\n",
       "  [5875, 14310, 4065, 1938],\n",
       "  [4944, 1026, 1113, 1008],\n",
       "  [4019, 5322, 2531, 1954],\n",
       "  [50, 1178, 2137, 1020],\n",
       "  [3237, 24374, 17429, 1008],\n",
       "  [2823, 5814, 22538, 1007],\n",
       "  [21930, 6988, 1016, 1014],\n",
       "  [2072, 2221, 1024, 25697],\n",
       "  [30718, 13333, 5116, 1925],\n",
       "  [4800, 5047, 9209, 1006],\n",
       "  [21793, 4775, 14557, 1011],\n",
       "  [3257, 24156, 4112, 1010],\n",
       "  [14342, 2008, 1030, 1010],\n",
       "  [9350, 5408, 20999, 1014],\n",
       "  [16523, 1934, 21905, 7310],\n",
       "  [15576, 1005, 13561, 8876],\n",
       "  [2281, 6228, 2789, 1032],\n",
       "  [22490, 1966, 1008, 2046],\n",
       "  [4664, 2032, 4345, 1975],\n",
       "  [3427, 26246, 13544, 1011],\n",
       "  [13255, 5275, 15422, 6037],\n",
       "  [2016, 1006, 7456, 8986],\n",
       "  [2053, 16851, 22373, 2616],\n",
       "  [3900, 10781, 23320, 1011],\n",
       "  [16457, 1968, 1053, 2046],\n",
       "  [14671, 2223, 21815, 1025],\n",
       "  [19951, 2021, 17720, 1007],\n",
       "  [18557, 4481, 13438, 2134],\n",
       "  [9038, 2490, 1024, 1978],\n",
       "  [9389, 13081, 4258, 1008],\n",
       "  [14028, 5085, 1005, 3003],\n",
       "  [3010, 2309, 3303, 1976],\n",
       "  [4180, 2341, 2055, 14659],\n",
       "  [12443, 27377, 18121, 1935],\n",
       "  [14582, 7129, 1922, 3866],\n",
       "  [2699, 4539, 16889, 1923],\n",
       "  [7693, 10271, 11456, 1921],\n",
       "  [11510, 22514, 12080, 1927],\n",
       "  [8496, 2129, 11897, 2582],\n",
       "  [3993, 2077, 1954, 1966],\n",
       "  [3601, 30379, 2605, 2898],\n",
       "  [3517, 7545, 2105, 1934],\n",
       "  [3630, 23723, 15038, 1936],\n",
       "  [51, 1254, 9292, 1926],\n",
       "  [17112, 2751, 3342, 10391],\n",
       "  [2053, 3073, 23548, 2127],\n",
       "  [5950, 3837, 23548, 1920],\n",
       "  [2860, 2113, 23938, 1005],\n",
       "  [17817, 1943, 14525, 2035],\n",
       "  [9351, 9338, 17429, 1008],\n",
       "  [9038, 17344, 3254, 2035],\n",
       "  [584, 5275, 1444, 1948],\n",
       "  [14172, 5561, 1009, 2036],\n",
       "  [7388, 14123, 20801, 1026],\n",
       "  [6108, 4065, 4197, 1971],\n",
       "  [3869, 2316, 4283, 1011],\n",
       "  [5563, 1059, 7017, 2073],\n",
       "  [4220, 1053, 8162, 1990],\n",
       "  [4658, 2939, 4137, 1014],\n",
       "  [3764, 4112, 3255, 3516],\n",
       "  [6800, 11919, 8504, 1005],\n",
       "  [13875, 23583, 29724, 1987],\n",
       "  [1985, 17429, 17429, 1008],\n",
       "  [30235, 1028, 22489, 2140],\n",
       "  [4315, 11549, 1008, 3163],\n",
       "  [17841, 16096, 15725, 1932],\n",
       "  [31704, 6988, 16798, 1007],\n",
       "  [3197, 2037, 2391, 5245],\n",
       "  [5538, 2116, 2428, 1924],\n",
       "  [11769, 5749, 10870, 1934],\n",
       "  [2520, 16719, 5901, 1007],\n",
       "  [6422, 10377, 11897, 1026],\n",
       "  [18883, 3099, 2918, 1006],\n",
       "  [10918, 3294, 1029, 2473],\n",
       "  [7820, 29141, 17429, 1008],\n",
       "  [51, 1018, 7747, 1926],\n",
       "  [9149, 11320, 6458, 1006],\n",
       "  [3869, 3594, 15038, 1936],\n",
       "  [6404, 2223, 24357, 1922],\n",
       "  [7523, 1028, 4051, 1006],\n",
       "  [14196, 13140, 5134, 2057],\n",
       "  [8839, 3294, 20719, 1943],\n",
       "  [1960, 3320, 6307, 1014],\n",
       "  [6404, 2022, 13544, 1926],\n",
       "  [30905, 20298, 2165, 1934],\n",
       "  [4174, 10602, 23548, 1923],\n",
       "  [3135, 9612, 2391, 6677],\n",
       "  [19965, 5749, 18279, 17505],\n",
       "  [2510, 3837, 12579, 1011],\n",
       "  [2086, 2809, 2690, 2309],\n",
       "  [379, 1388, 1388, 1272],\n",
       "  [26885, 2063, 6037, 1028],\n",
       "  [4726, 24156, 4775, 1972],\n",
       "  [17932, 2839, 4654, 1990],\n",
       "  [23493, 25059, 18508, 1008],\n",
       "  [3158, 12847, 6477, 11624],\n",
       "  [2686, 1976, 18812, 1923],\n",
       "  [6695, 4345, 11190, 2391],\n",
       "  [4225, 1922, 13314, 1926],\n",
       "  [15503, 1019, 14642, 3605],\n",
       "  [2841, 23417, 22339, 1020],\n",
       "  [24735, 1978, 4481, 1028],\n",
       "  [17745, 3247, 1030, 16684],\n",
       "  [2128, 3263, 3221, 2693],\n",
       "  [10111, 3182, 8782, 1011],\n",
       "  [4470, 4397, 1026, 3605],\n",
       "  [3757, 1077, 5883, 27313],\n",
       "  [5854, 3771, 3073, 1927],\n",
       "  [3232, 3085, 18280, 1011],\n",
       "  [5979, 2232, 13218, 2391],\n",
       "  [90, 3016, 1010, 1971],\n",
       "  [6331, 14977, 3967, 22323],\n",
       "  [4504, 2005, 1030, 2932],\n",
       "  [21862, 1009, 8894, 16596],\n",
       "  [30388, 16014, 1944, 1035],\n",
       "  [6390, 5665, 22339, 1012],\n",
       "  [8572, 6754, 7613, 1923],\n",
       "  [30235, 1028, 22489, 1934],\n",
       "  [11599, 2320, 1943, 6395],\n",
       "  [3737, 14806, 2180, 2788],\n",
       "  [5530, 4828, 3182, 1926],\n",
       "  [9318, 6988, 9272, 1012],\n",
       "  [16055, 9723, 6427, 1006],\n",
       "  [5248, 22107, 1027, 2339],\n",
       "  [3248, 1971, 2342, 2689],\n",
       "  [3278, 2215, 2019, 1931],\n",
       "  [26247, 10629, 11168, 1923],\n",
       "  [6361, 1947, 4270, 2112],\n",
       "  [4368, 4289, 11877, 1972],\n",
       "  [28087, 4257, 11886, 2169],\n",
       "  [21617, 22538, 1925, 2113],\n",
       "  [2154, 27316, 3835, 1009],\n",
       "  [2205, 14525, 11592, 1943],\n",
       "  [3757, 2704, 25105, 1016],\n",
       "  [9220, 7122, 10027, 8498],\n",
       "  [2845, 4846, 2005, 1030],\n",
       "  [13559, 2972, 13809, 1011],\n",
       "  [16534, 2022, 5907, 1937],\n",
       "  [6460, 7490, 2102, 1957],\n",
       "  [20511, 4057, 25932, 14557],\n",
       "  [11507, 4828, 3355, 1007],\n",
       "  [2389, 2052, 22964, 6302],\n",
       "  [21400, 1966, 25059, 1012],\n",
       "  [4470, 1967, 2483, 1928],\n",
       "  [3150, 31028, 13230, 1938],\n",
       "  [23836, 2276, 1048, 23417],\n",
       "  [22994, 20742, 17429, 1008],\n",
       "  [2999, 1006, 1146, 1937],\n",
       "  [2274, 4197, 2417, 2710],\n",
       "  [2481, 8755, 16203, 1027],\n",
       "  [2072, 3522, 5561, 2616],\n",
       "  [2543, 21068, 1008, 1059],\n",
       "  [2348, 11910, 4569, 3540],\n",
       "  [2059, 9611, 1994, 2159],\n",
       "  [2888, 4012, 1005, 2113],\n",
       "  [271, 1089, 1108, 1197],\n",
       "  [16521, 2002, 7263, 2042],\n",
       "  [19824, 3540, 5469, 1949],\n",
       "  [3300, 1053, 8684, 1926],\n",
       "  [3765, 25531, 17429, 1008],\n",
       "  [7183, 3824, 5596, 2000],\n",
       "  [5062, 12326, 1936, 1924],\n",
       "  [5908, 3075, 2138, 1975],\n",
       "  [22559, 6109, 6427, 1020],\n",
       "  [12655, 1006, 2032, 17237],\n",
       "  [13490, 1990, 17880, 1966],\n",
       "  [3625, 22227, 17395, 1940],\n",
       "  [8066, 1028, 2044, 7172],\n",
       "  [3994, 5907, 23527, 1025],\n",
       "  [21679, 13544, 9934, 1016],\n",
       "  [95, 1193, 2785, 1011],\n",
       "  [6522, 3461, 13544, 2577],\n",
       "  [30681, 22419, 6477, 1016],\n",
       "  [8995, 19833, 2264, 1008],\n",
       "  [19519, 9539, 17429, 1008],\n",
       "  [8769, 13344, 22538, 2461],\n",
       "  [24107, 30296, 5043, 22052],\n",
       "  [2740, 3255, 1030, 2383],\n",
       "  [64, 10629, 1077, 2062],\n",
       "  [9361, 22063, 1957, 2002],\n",
       "  [7470, 1966, 6856, 1934],\n",
       "  [12826, 2235, 2117, 7118],\n",
       "  [3177, 2582, 5611, 9145],\n",
       "  [4181, 12080, 5228, 1937],\n",
       "  [2328, 6302, 4112, 1024],\n",
       "  [8524, 1036, 1949, 1931],\n",
       "  [2251, 9187, 2119, 1020],\n",
       "  [4169, 3414, 17429, 1008],\n",
       "  [15854, 7637, 22538, 1007],\n",
       "  [4933, 10190, 11759, 1011],\n",
       "  [4893, 31664, 5032, 1009],\n",
       "  [2660, 11886, 2264, 1008],\n",
       "  [10847, 13634, 1019, 1940],\n",
       "  [17488, 6870, 17869, 4191],\n",
       "  [42, 1278, 3723, 1056],\n",
       "  [14329, 2972, 6052, 1016],\n",
       "  [12501, 22538, 4012, 1016],\n",
       "  [19759, 4321, 9989, 22741],\n",
       "  [14115, 1965, 6486, 2102],\n",
       "  [7470, 2392, 2715, 1057],\n",
       "  [2592, 9934, 17429, 1008],\n",
       "  [16663, 22538, 4551, 1939],\n",
       "  [619, 1243, 1194, 1074],\n",
       "  [2488, 3803, 1019, 2582],\n",
       "  [3398, 3280, 14354, 1006],\n",
       "  [8340, 6307, 1022, 4030],\n",
       "  [9391, 2718, 3202, 6615],\n",
       "  [58, 1078, 17933, 1922],\n",
       "  [2305, 3251, 1008, 2046],\n",
       "  [14272, 5749, 5901, 1951],\n",
       "  [6113, 28744, 2021, 1937],\n",
       "  [21007, 15324, 18221, 8737],\n",
       "  [5454, 17770, 14140, 1014],\n",
       "  [28060, 1005, 7906, 1018],\n",
       "  [6759, 3291, 2962, 3309],\n",
       "  [16365, 4629, 22538, 1007],\n",
       "  [277, 1067, 1268, 1079],\n",
       "  [25145, 12367, 25265, 2112],\n",
       "  [19171, 13140, 2479, 5502],\n",
       "  [3699, 8755, 7269, 7646],\n",
       "  [9906, 1049, 1050, 1052],\n",
       "  [4379, 6509, 4175, 1971],\n",
       "  [25272, 4569, 5734, 1008],\n",
       "  [3683, 19116, 2605, 1030],\n",
       "  [3345, 4191, 7622, 2616],\n",
       "  [1962, 1931, 1021, 1924],\n",
       "  [22798, 5447, 15980, 2882],\n",
       "  [61, 1057, 6037, 1005],\n",
       "  [10226, 5611, 6302, 9145],\n",
       "  [9662, 2353, 1021, 2027],\n",
       "  [7741, 2456, 2961, 3889],\n",
       "  [2708, 18082, 22538, 1926],\n",
       "  [2157, 1010, 1978, 1036],\n",
       "  [15411, 3370, 15038, 4575],\n",
       "  [44, 1308, 4359, 12285],\n",
       "  [9674, 4273, 2087, 2556],\n",
       "  [4507, 3212, 3031, 1934],\n",
       "  [1983, 2025, 1940, 1006],\n",
       "  [2015, 5596, 22538, 1011],\n",
       "  [6036, 13538, 2079, 2159],\n",
       "  [10578, 3485, 4384, 9539],\n",
       "  [2899, 2725, 26852, 2124],\n",
       "  [2448, 20693, 29048, 2022],\n",
       "  [2448, 7560, 1005, 3003],\n",
       "  [11719, 1010, 2882, 7646],\n",
       "  [4226, 1015, 1308, 18173],\n",
       "  [2177, 6576, 15514, 1011],\n",
       "  [23123, 18838, 2178, 1994],\n",
       "  [2060, 2055, 31819, 6894],\n",
       "  [2970, 19953, 1005, 19239],\n",
       "  [9990, 2951, 1008, 4397],\n",
       "  [8062, 17961, 1019, 2647],\n",
       "  [2723, 3292, 4175, 1931],\n",
       "  [2210, 1932, 23647, 2892],\n",
       "  [16850, 16945, 22394, 1025],\n",
       "  [3219, 3067, 4569, 1018],\n",
       "  [2711, 29962, 16945, 1014],\n",
       "  [419, 1107, 1230, 1287],\n",
       "  [9990, 2951, 7614, 1007],\n",
       "  [7128, 28778, 2303, 3154],\n",
       "  [61, 4065, 22393, 1027],\n",
       "  [7399, 3999, 9934, 1008],\n",
       "  [31753, 1016, 2119, 1007],\n",
       "  [2211, 1019, 5274, 5513],\n",
       "  [3757, 31821, 3664, 1926],\n",
       "  [30235, 1028, 22489, 9433],\n",
       "  [17368, 9603, 11624, 11252],\n",
       "  [4081, 3771, 27014, 1007],\n",
       "  [8004, 1026, 2831, 1971],\n",
       "  [26513, 12597, 5611, 1975],\n",
       "  [3392, 16710, 2027, 19953],\n",
       "  [6195, 5804, 26056, 1016],\n",
       "  [2389, 2052, 2070, 5274],\n",
       "  [2211, 2106, 26884, 1014],\n",
       "  [7459, 15876, 2008, 4210],\n",
       "  [2336, 27227, 28448, 1938],\n",
       "  [25099, 26944, 3332, 1931],\n",
       "  [14849, 14778, 9729, 17429],\n",
       "  [21220, 3119, 3677, 3621],\n",
       "  [29754, 1019, 3644, 7550],\n",
       "  [3994, 24138, 17429, 1008],\n",
       "  [5255, 1048, 4065, 1977],\n",
       "  [23693, 14787, 3564, 1014],\n",
       "  [29451, 2978, 17933, 1920],\n",
       "  [2314, 1009, 5594, 10400],\n",
       "  [3630, 1013, 1276, 1019],\n",
       "  [2015, 2294, 6073, 2339],\n",
       "  [4893, 8082, 5410, 1016],\n",
       "  [3782, 2414, 1016, 2306],\n",
       "  [4276, 25269, 3644, 7646],\n",
       "  [8275, 2064, 13307, 1011],\n",
       "  [14688, 1922, 3638, 2140],\n",
       "  [11855, 4973, 3302, 2339],\n",
       "  [22348, 3976, 5777, 1923],\n",
       "  [264, 29189, 1079, 29596],\n",
       "  [8337, 2045, 2116, 1966],\n",
       "  [10437, 1005, 29402, 8764],\n",
       "  [4231, 6452, 17429, 1008],\n",
       "  [5908, 2073, 23548, 1920],\n",
       "  [2177, 5622, 3972, 1032],\n",
       "  [20060, 13544, 4012, 1016],\n",
       "  [5321, 9338, 25059, 1006],\n",
       "  [1985, 4261, 2751, 2775],\n",
       "  [16672, 12994, 2022, 1026],\n",
       "  [2488, 5144, 4551, 1939],\n",
       "  [3232, 17440, 23395, 2102],\n",
       "  [2281, 31508, 5331, 3883],\n",
       "  [18992, 4328, 5734, 7009],\n",
       "  [124, 7770, 1926, 1193],\n",
       "  [3257, 1968, 30730, 2689],\n",
       "  [5373, 1048, 15379, 1023],\n",
       "  [31085, 2165, 2286, 1018],\n",
       "  [21992, 29942, 3160, 1937],\n",
       "  [5696, 3997, 3408, 14310],\n",
       "  [3891, 2391, 1436, 1011],\n",
       "  [6250, 4012, 1005, 5955],\n",
       "  [4497, 7077, 2309, 1016],\n",
       "  [124, 2221, 1138, 1007],\n",
       "  [29334, 12579, 3762, 26589],\n",
       "  [15868, 9623, 20576, 1923],\n",
       "  [2907, 3541, 6670, 1920],\n",
       "  [29660, 11779, 6870, 1020],\n",
       "  [11763, 1009, 1976, 1033],\n",
       "  [19386, 1920, 10930, 1975],\n",
       "  [3056, 2309, 9272, 1924],\n",
       "  [5854, 3771, 3073, 2334],\n",
       "  [8269, 7550, 7320, 2055],\n",
       "  [6137, 2537, 3303, 2349],\n",
       "  [21383, 1024, 2320, 3154],\n",
       "  [2467, 24135, 1030, 9792],\n",
       "  [2389, 17105, 3991, 2616],\n",
       "  [9193, 2050, 1975, 6061],\n",
       "  [4075, 2250, 9985, 1025],\n",
       "  [3384, 31409, 29637, 1007],\n",
       "  [16534, 5398, 2829, 1922],\n",
       "  [13213, 12098, 26015, 1008],\n",
       "  [9111, 25265, 5153, 1938],\n",
       "  [9149, 11320, 6458, 1923],\n",
       "  [408, 1040, 1150, 1206],\n",
       "  [26371, 9069, 22538, 1011],\n",
       "  [6126, 1928, 2032, 1011],\n",
       "  [5552, 1053, 1995, 14559],\n",
       "  [2543, 25439, 8457, 2536],\n",
       "  [11763, 6882, 20719, 10286],\n",
       "  [2946, 1019, 1010, 1990],\n",
       "  [5185, 2127, 6392, 1927],\n",
       "  [6753, 3657, 1922, 1924],\n",
       "  [18105, 1029, 12021, 22612],\n",
       "  [3499, 2978, 1019, 1990],\n",
       "  [15503, 2127, 5085, 2084],\n",
       "  [3636, 1007, 26588, 1014],\n",
       "  [3647, 3303, 15422, 1025],\n",
       "  [1983, 7097, 6427, 1925],\n",
       "  [25171, 29683, 3022, 5557],\n",
       "  [29002, 2730, 5274, 5901],\n",
       "  [4438, 6426, 1016, 2530],\n",
       "  [2154, 27316, 8172, 1007],\n",
       "  [18117, 6661, 2582, 1940],\n",
       "  [2448, 2763, 3255, 2223],\n",
       "  [16807, 2091, 5681, 1007],\n",
       "  [2321, 4112, 8876, 1924],\n",
       "  [5654, 3261, 10786, 7215],\n",
       "  [5037, 31571, 2381, 4656],\n",
       "  [14606, 10948, 1944, 1028],\n",
       "  [21688, 2223, 2893, 2061],\n",
       "  [3554, 4973, 4771, 1007],\n",
       "  [4664, 2032, 4345, 3852],\n",
       "  [3288, 3029, 5047, 1921],\n",
       "  [31084, 4109, 2751, 3342],\n",
       "  [3971, 10190, 2112, 8724],\n",
       "  [4732, 11600, 7587, 1940],\n",
       "  [9739, 18121, 17832, 1940],\n",
       "  [9483, 18121, 17125, 1990],\n",
       "  [3629, 17720, 7528, 2215],\n",
       "  [6759, 8424, 2149, 17237],\n",
       "  [4154, 2276, 7906, 4522],\n",
       "  [2243, 3099, 13539, 13918],\n",
       "  [12521, 5351, 31344, 8052],\n",
       "  [2347, 26058, 1978, 1007],\n",
       "  [3082, 11216, 1009, 19579],\n",
       "  [15451, 20226, 1994, 1988],\n",
       "  [20199, 3291, 11910, 4602],\n",
       "  [7856, 1009, 13621, 11050],\n",
       "  [4277, 9462, 17429, 1008],\n",
       "  [2251, 20498, 22562, 1006],\n",
       "  [4667, 29144, 3247, 2495],\n",
       "  [7399, 6603, 1007, 1180],\n",
       "  [16300, 1968, 2479, 27104],\n",
       "  [22622, 3991, 3664, 4555],\n",
       "  [9608, 23627, 8427, 6307],\n",
       "  [18447, 1950, 2259, 1048],\n",
       "  [3822, 1026, 3540, 1005],\n",
       "  [3756, 15996, 26440, 2475],\n",
       "  [24565, 1036, 3741, 1967],\n",
       "  [2849, 23059, 3644, 2790],\n",
       "  [9567, 22538, 4012, 1016],\n",
       "  [1999, 3467, 1929, 13918],\n",
       "  [2652, 1957, 12519, 2054],\n",
       "  [25796, 13544, 4012, 1016],\n",
       "  [2513, 5744, 3081, 3080],\n",
       "  [3318, 2068, 2139, 2009],\n",
       "  [3635, 3677, 7838, 6480],\n",
       "  [10594, 17440, 14525, 1014],\n",
       "  [2199, 1930, 7906, 1007],\n",
       "  [6118, 5275, 29851, 1927],\n",
       "  [6144, 1928, 20282, 1010],\n",
       "  [16181, 3813, 8234, 1932],\n",
       "  [14080, 14557, 26015, 1008],\n",
       "  [5745, 5055, 11544, 8986],\n",
       "  [16260, 4397, 1970, 4787],\n",
       "  [23344, 1029, 25219, 1923],\n",
       "  [2176, 2172, 2341, 1020],\n",
       "  [20055, 14324, 22538, 3115],\n",
       "  [2060, 2022, 7142, 1938],\n",
       "  [10243, 2918, 1928, 2123],\n",
       "  [2093, 3645, 1921, 8876],\n",
       "  [8157, 2039, 15709, 1975],\n",
       "  [10094, 23869, 4197, 1971],\n",
       "  [6537, 1987, 2046, 1018],\n",
       "  [4140, 10082, 2492, 1011],\n",
       "  [26137, 1014, 4569, 1015],\n",
       "  [4664, 1922, 11456, 1924],\n",
       "  [5207, 5528, 1033, 2002],\n",
       "  [1964, 31183, 4068, 1008],\n",
       "  [17253, 4654, 23548, 1011],\n",
       "  [9922, 23189, 3789, 1930],\n",
       "  [6331, 19953, 3967, 2259],\n",
       "  [3070, 2198, 5290, 1010],\n",
       "  [24394, 8937, 13132, 3883],\n",
       "  [4957, 3758, 26921, 1020],\n",
       "  [158, 1292, 12375, 1021],\n",
       "  [22652, 13933, 5395, 1011],\n",
       "  [16615, 1934, 3489, 1006],\n",
       "  [8546, 26726, 7009, 1006],\n",
       "  [2671, 4233, 14806, 1945],\n",
       "  [10928, 11886, 3103, 1966],\n",
       "  [13577, 4476, 2322, 1923],\n",
       "  [3012, 2386, 1966, 1027],\n",
       "  [6929, 2814, 3458, 2217],\n",
       "  [9509, 13544, 9934, 1016],\n",
       "  [31797, 11912, 4844, 1995],\n",
       "  [13855, 2161, 2021, 1957],\n",
       "  [5515, 2037, 5782, 22776],\n",
       "  [2906, 12181, 3596, 8986],\n",
       "  [4145, 5274, 26788, 31127],\n",
       "  [7869, 7386, 2106, 1974],\n",
       "  [9328, 4860, 22538, 1011],\n",
       "  [3709, 16204, 24743, 1923],\n",
       "  [4232, 2918, 1932, 8876],\n",
       "  [4879, 3357, 29896, 1930],\n",
       "  [23679, 2168, 2259, 29533],\n",
       "  [7359, 7476, 22538, 1926],\n",
       "  [8443, 2027, 6576, 3771],\n",
       "  [31447, 2127, 11247, 2483],\n",
       "  [12045, 7336, 2831, 1957],\n",
       "  [8774, 18327, 3461, 1934],\n",
       "  [2699, 4392, 22538, 1007],\n",
       "  [17112, 2405, 8099, 9018],\n",
       "  [11528, 4397, 5208, 1010],\n",
       "  [8500, 3146, 2225, 1978],\n",
       "  [28999, 1995, 1025, 1757],\n",
       "  [6236, 1926, 16510, 14986],\n",
       "  [8101, 7611, 7637, 4555],\n",
       "  [19580, 1193, 3627, 1020],\n",
       "  [8798, 2772, 3221, 1009],\n",
       "  [2370, 3429, 11877, 1972],\n",
       "  [19203, 2063, 2063, 1008],\n",
       "  [11660, 1026, 1030, 1979],\n",
       "  [19552, 14241, 4678, 1008],\n",
       "  [2983, 2882, 25105, 1008],\n",
       "  [24889, 3016, 9668, 1019],\n",
       "  [21654, 5277, 2769, 1008],\n",
       "  [3337, 1030, 12901, 8986],\n",
       "  [3398, 14089, 1016, 6966],\n",
       "  [3765, 6477, 7009, 1979],\n",
       "  [3155, 1053, 11023, 1006],\n",
       "  [7198, 11863, 11553, 2425],\n",
       "  [1964, 2785, 6908, 1019],\n",
       "  [3694, 4177, 15159, 1008],\n",
       "  [29491, 23316, 11413, 16226],\n",
       "  [15041, 1931, 14642, 2898],\n",
       "  [7978, 3677, 1922, 2036],\n",
       "  [2389, 6037, 6480, 3280],\n",
       "  [9389, 13081, 5542, 2026],\n",
       "  [22563, 1965, 3789, 1007],\n",
       "  [3194, 2260, 19288, 1922],\n",
       "  [2610, 13509, 22538, 1926],\n",
       "  [13875, 13064, 4012, 1016],\n",
       "  [5908, 3075, 2138, 1984],\n",
       "  [4664, 1922, 11456, 1012],\n",
       "  [4332, 3085, 25753, 11278],\n",
       "  [19936, 13544, 9934, 1016],\n",
       "  [14342, 1021, 6996, 1944],\n",
       "  [19259, 1965, 3789, 8960],\n",
       "  [2251, 11912, 11697, 7622],\n",
       "  [12586, 2106, 2209, 4555],\n",
       "  [3900, 5591, 1008, 1927],\n",
       "  [7840, 4685, 12204, 1010],\n",
       "  [1986, 8724, 1926, 3664],\n",
       "  [16807, 8504, 1021, 1193],\n",
       "  [43, 4828, 1958, 10629],\n",
       "  [3150, 7027, 1054, 2396],\n",
       "  [8862, 2106, 26884, 1014],\n",
       "  [5184, 3516, 19953, 1928],\n",
       "  [18566, 14977, 3668, 4656],\n",
       "  [8500, 23057, 14659, 1007],\n",
       "  [15874, 1016, 11600, 22512],\n",
       "  [2135, 6869, 29724, 1954],\n",
       "  [28692, 5427, 2049, 1984],\n",
       "  [4664, 1922, 1930, 1923],\n",
       "  [13318, 25219, 5561, 2019],\n",
       "  [15522, 25561, 1008, 3866],\n",
       "  [6361, 2892, 3336, 1008],\n",
       "  [7700, 1030, 4345, 17586],\n",
       "  [2060, 2022, 1996, 6302],\n",
       "  [2481, 6003, 3359, 1006],\n",
       "  [3757, 1057, 5749, 27462],\n",
       "  [9503, 1923, 26692, 4733],\n",
       "  [16539, 2690, 13824, 1019],\n",
       "  [18762, 26470, 2006, 3487],\n",
       "  [26085, 16775, 2351, 2022],\n",
       "  [13560, 1981, 10881, 29795],\n",
       "  [29842, 1953, 29344, 1934],\n",
       "  [2516, 2260, 1926, 4162],\n",
       "  [26129, 7770, 4887, 3596],\n",
       "  [2950, 12871, 1180, 1923],\n",
       "  [5069, 2235, 1009, 1940],\n",
       "  [15513, 29724, 4551, 1931],\n",
       "  [2028, 3370, 10991, 3889],\n",
       "  [5824, 2073, 3644, 17691],\n",
       "  [12217, 22339, 2898, 1014],\n",
       "  [2272, 9539, 17429, 1008],\n",
       "  [21424, 4681, 25344, 1008],\n",
       "  [3159, 3540, 1030, 7587],\n",
       "  [16967, 2932, 22538, 1926],\n",
       "  [2553, 1024, 2081, 2215],\n",
       "  [25929, 2772, 6037, 2536],\n",
       "  [6341, 3304, 2021, 1937],\n",
       "  [16310, 25776, 14235, 2112],\n",
       "  [3042, 3564, 6395, 2081],\n",
       "  [5875, 3614, 8099, 1938],\n",
       "  [28776, 8051, 18133, 1938],\n",
       "  [2723, 4112, 6302, 1008],\n",
       "  [2176, 26762, 2405, 1028],\n",
       "  [8951, 2852, 4257, 2169],\n",
       "  [3222, 3655, 1005, 2809],\n",
       "  [2478, 8764, 1930, 1012],\n",
       "  [12419, 2602, 3463, 1995],\n",
       "  [15503, 2524, 5731, 1951],\n",
       "  [1985, 1058, 2127, 30876],\n",
       "  [13578, 2647, 2256, 3024],\n",
       "  [20825, 23831, 3085, 2192],\n",
       "  [2238, 4039, 2073, 3404],\n",
       "  [3237, 1021, 8989, 1008],\n",
       "  [1985, 1021, 6302, 5681],\n",
       "  [10809, 1921, 12205, 1020],\n",
       "  [2197, 4112, 6302, 2022],\n",
       "  [17840, 2536, 3284, 1931],\n",
       "  [7099, 3163, 6794, 3171],\n",
       "  [2154, 27316, 8172, 1979],\n",
       "  [17840, 1005, 28049, 1923],\n",
       "  [15694, 26017, 8644, 4319],\n",
       "  [8631, 5029, 15038, 1936],\n",
       "  [30607, 12620, 1008, 2514],\n",
       "  [31928, 2479, 4787, 1938],\n",
       "  [6233, 6392, 22962, 1938],\n",
       "  [5601, 2705, 18082, 2398],\n",
       "  [9345, 6458, 6783, 1019],\n",
       "  [3089, 3596, 5734, 2022],\n",
       "  [22521, 3302, 16945, 1008],\n",
       "  [22253, 5591, 3596, 1923],\n",
       "  [3577, 3250, 3603, 5901],\n",
       "  [14737, 12746, 9585, 1008],\n",
       "  [2572, 9278, 21122, 10592],\n",
       "  [3905, 1967, 27540, 1990],\n",
       "  [23613, 2133, 4253, 8986],\n",
       "  [19470, 2022, 1013, 5558],\n",
       "  [3434, 9252, 2022, 1014],\n",
       "  [2874, 1945, 2117, 1012],\n",
       "  [16905, 4415, 19943, 1011],\n",
       "  [11770, 1924, 13824, 1008],\n",
       "  [3010, 4569, 1931, 5823],\n",
       "  [2272, 4476, 14220, 1010],\n",
       "  [7523, 9539, 18319, 1930],\n",
       "  [9686, 8993, 2022, 1026],\n",
       "  [2448, 9555, 10991, 2316],\n",
       "  [13687, 12393, 2116, 1006],\n",
       "  [16023, 3320, 10931, 1931],\n",
       "  [2053, 5427, 1008, 4003],\n",
       "  [2936, 29897, 26015, 1008],\n",
       "  [3145, 2398, 11910, 1011],\n",
       "  [30514, 1008, 25565, 1011],\n",
       "  [6673, 6979, 1009, 2069],\n",
       "  [2328, 1058, 6988, 2303],\n",
       "  [14936, 2918, 3789, 1007],\n",
       "  [13804, 2081, 2280, 6356],\n",
       "  [7814, 5357, 1030, 2005],\n",
       "  [21617, 22538, 3596, 1008],\n",
       "  [9149, 11320, 6458, 2511],\n",
       "  [3401, 2391, 11321, 1016],\n",
       "  [2874, 3972, 2282, 1008],\n",
       "  [90, 1021, 11477, 1961],\n",
       "  [10918, 2880, 7587, 5734],\n",
       "  [18138, 3218, 3160, 3464],\n",
       "  [56, 2141, 5931, 3849],\n",
       "  [3158, 2117, 6719, 4275],\n",
       "  [3300, 2062, 1258, 1005],\n",
       "  [9389, 12426, 22862, 1990],\n",
       "  [5433, 1920, 7196, 1938],\n",
       "  [26345, 14806, 4551, 1016],\n",
       "  [3757, 2704, 25105, 7009],\n",
       "  [2505, 6509, 4157, 1011],\n",
       "  [1960, 10028, 5885, 2159],\n",
       "  [7900, 2303, 1026, 5554],\n",
       "  [5967, 1948, 2022, 1030],\n",
       "  [13609, 2393, 1009, 1005],\n",
       "  [19287, 6988, 7329, 1007],\n",
       "  [3095, 3261, 5469, 1006],\n",
       "  [6144, 1026, 1988, 1025],\n",
       "  [2053, 3006, 8885, 1009],\n",
       "  [3983, 29897, 6928, 1923],\n",
       "  [18990, 22538, 4929, 1016],\n",
       "  [15436, 4626, 22538, 1011],\n",
       "  [3102, 2109, 5712, 2446],\n",
       "  [2984, 16096, 2021, 1957],\n",
       "  [4606, 2852, 13610, 2357],\n",
       "  [7766, 1014, 17785, 17691],\n",
       "  [6929, 2814, 3458, 2435],\n",
       "  [12250, 15577, 2323, 6320],\n",
       "  [3588, 24173, 1006, 13654],\n",
       "  [5989, 6037, 1023, 9342],\n",
       "  [5119, 2312, 1019, 2291],\n",
       "  [5218, 3516, 20943, 2057],\n",
       "  [22434, 5469, 19073, 17429],\n",
       "  [2211, 2335, 3429, 1018],\n",
       "  [15781, 9588, 16798, 2152],\n",
       "  [2238, 2391, 1036, 10276],\n",
       "  [3192, 11138, 2030, 1937],\n",
       "  [2318, 2106, 1950, 1020],\n",
       "  [9223, 2077, 25987, 1942],\n",
       "  [3389, 1950, 3930, 1984],\n",
       "  [2205, 1968, 5502, 5397],\n",
       "  [4303, 3366, 1019, 2445],\n",
       "  [26021, 3426, 22538, 1007],\n",
       "  ...],\n",
       " 1887)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_word_set_dict[\"token_len_group_4\"], random_word_set_dict[\"token_len_group_4_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>14260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>12878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>18603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>24641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>24389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0     16975\n",
       "1      5859\n",
       "2     12659\n",
       "3     11148\n",
       "4     21297\n",
       "...     ...\n",
       "4995  14260\n",
       "4996  12878\n",
       "4997  18603\n",
       "4998  24641\n",
       "4999  24389\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(random_word_set_dict[\"token_len_group_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>token_ids</th>\n",
       "      <th>token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22069</th>\n",
       "      <td>olaylara</td>\n",
       "      <td>[16975]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8732</th>\n",
       "      <td>listesinde</td>\n",
       "      <td>[5859]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22035</th>\n",
       "      <td>anlayışı</td>\n",
       "      <td>[12659]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>vatandaşı</td>\n",
       "      <td>[11148]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16226</th>\n",
       "      <td>şekliyle</td>\n",
       "      <td>[21297]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23343</th>\n",
       "      <td>gordon</td>\n",
       "      <td>[14260]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32856</th>\n",
       "      <td>desteklenen</td>\n",
       "      <td>[12878]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4013</th>\n",
       "      <td>eva</td>\n",
       "      <td>[18603]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7249</th>\n",
       "      <td>bastırmak</td>\n",
       "      <td>[24641]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19892</th>\n",
       "      <td>avukatı</td>\n",
       "      <td>[24389]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word token_ids  token_len\n",
       "22069     olaylara   [16975]          1\n",
       "8732    listesinde    [5859]          1\n",
       "22035     anlayışı   [12659]          1\n",
       "6085     vatandaşı   [11148]          1\n",
       "16226     şekliyle   [21297]          1\n",
       "...            ...       ...        ...\n",
       "23343       gordon   [14260]          1\n",
       "32856  desteklenen   [12878]          1\n",
       "4013           eva   [18603]          1\n",
       "7249     bastırmak   [24641]          1\n",
       "19892      avukatı   [24389]          1\n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(random_word_set_dict[\"token_len_group_1\"]) # old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>token_ids</th>\n",
       "      <th>token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>=</td>\n",
       "      <td>47238</td>\n",
       "      <td>[32]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thomas</td>\n",
       "      <td>119</td>\n",
       "      <td>[6647]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lüthi</td>\n",
       "      <td>2</td>\n",
       "      <td>[13229, 3255]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(</td>\n",
       "      <td>20570</td>\n",
       "      <td>[12]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d</td>\n",
       "      <td>2524</td>\n",
       "      <td>[44]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.</td>\n",
       "      <td>155540</td>\n",
       "      <td>[18]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>15960</td>\n",
       "      <td>[26]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eylül</td>\n",
       "      <td>1304</td>\n",
       "      <td>[2873]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>68283</td>\n",
       "      <td>[21]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>37906</td>\n",
       "      <td>[29]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  frequency      token_ids  token_len\n",
       "0       =      47238           [32]          1\n",
       "1  thomas        119         [6647]          1\n",
       "2   lüthi          2  [13229, 3255]          2\n",
       "3       (      20570           [12]          1\n",
       "4       d       2524           [44]          1\n",
       "5       .     155540           [18]          1\n",
       "6       6      15960           [26]          1\n",
       "7   eylül       1304         [2873]          1\n",
       "8       1      68283           [21]          1\n",
       "9       9      37906           [29]          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_len</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>241632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           frequency\n",
       "token_len           \n",
       "1             241632\n",
       "2              29205\n",
       "3               8298\n",
       "4               1995\n",
       "5                395\n",
       "6                112\n",
       "7                 29\n",
       "8                  9\n",
       "9                  8\n",
       "10                 6\n",
       "11                 2\n",
       "13                 1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_df[[\"token_len\", \"frequency\"]].groupby(\"token_len\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_len</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>143386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           frequency\n",
       "token_len           \n",
       "1             143386\n",
       "2              32060\n",
       "3               9562\n",
       "4               2490\n",
       "5                668\n",
       "6                195\n",
       "7                 48\n",
       "8                 19\n",
       "9                 11\n",
       "10                 6\n",
       "11                 3\n",
       "12                 2\n",
       "13                 2\n",
       "14                 1\n",
       "15                 1\n",
       "22                 1\n",
       "29                 1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_df[[\"token_len\", \"frequency\"]].groupby(\"token_len\").sum() # after deleting numbers and punctiations from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_len</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2443197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>286093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           frequency\n",
       "token_len           \n",
       "1            2443197\n",
       "2             286093\n",
       "3              79524\n",
       "4              17962\n",
       "5               3970\n",
       "6                931\n",
       "7                318\n",
       "8                148\n",
       "9                 78\n",
       "10                58\n",
       "11                20\n",
       "12                 9\n",
       "13                 5\n",
       "14                 2\n",
       "15                 4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_df[[\"token_len\", \"frequency\"]].groupby(\"token_len\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token_len 5 ve öncesini tutacağım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>token_ids</th>\n",
       "      <th>token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>=</td>\n",
       "      <td>47238</td>\n",
       "      <td>[32]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thomas</td>\n",
       "      <td>119</td>\n",
       "      <td>[6647]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lüthi</td>\n",
       "      <td>2</td>\n",
       "      <td>[13229, 3255]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(</td>\n",
       "      <td>20570</td>\n",
       "      <td>[12]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d</td>\n",
       "      <td>2524</td>\n",
       "      <td>[44]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.</td>\n",
       "      <td>155540</td>\n",
       "      <td>[18]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>15960</td>\n",
       "      <td>[26]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eylül</td>\n",
       "      <td>1304</td>\n",
       "      <td>[2873]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>68283</td>\n",
       "      <td>[21]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>37906</td>\n",
       "      <td>[29]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>18691</td>\n",
       "      <td>[28]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>;</td>\n",
       "      <td>5450</td>\n",
       "      <td>[31]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>oberdiessbach</td>\n",
       "      <td>1</td>\n",
       "      <td>[17368, 9603, 11624, 11252]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>)</td>\n",
       "      <td>20625</td>\n",
       "      <td>[13]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>,</td>\n",
       "      <td>113641</td>\n",
       "      <td>[16]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i̇nterwetten</td>\n",
       "      <td>1</td>\n",
       "      <td>[25272, 4569, 5734, 1008]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>paddock</td>\n",
       "      <td>1</td>\n",
       "      <td>[14176, 3837, 2536]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>moto</td>\n",
       "      <td>3</td>\n",
       "      <td>[3543, 1020]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>44964</td>\n",
       "      <td>[22]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>takımı</td>\n",
       "      <td>646</td>\n",
       "      <td>[3282]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>için</td>\n",
       "      <td>10517</td>\n",
       "      <td>[2051]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>grand</td>\n",
       "      <td>209</td>\n",
       "      <td>[6116]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>prix</td>\n",
       "      <td>75</td>\n",
       "      <td>[8073]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>dünya</td>\n",
       "      <td>2445</td>\n",
       "      <td>[2458]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>şampiyonası</td>\n",
       "      <td>490</td>\n",
       "      <td>[4100]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  frequency                    token_ids  token_len\n",
       "0               =      47238                         [32]          1\n",
       "1          thomas        119                       [6647]          1\n",
       "2           lüthi          2                [13229, 3255]          2\n",
       "3               (      20570                         [12]          1\n",
       "4               d       2524                         [44]          1\n",
       "5               .     155540                         [18]          1\n",
       "6               6      15960                         [26]          1\n",
       "7           eylül       1304                       [2873]          1\n",
       "8               1      68283                         [21]          1\n",
       "9               9      37906                         [29]          1\n",
       "10              8      18691                         [28]          1\n",
       "11              ;       5450                         [31]          1\n",
       "12  oberdiessbach          1  [17368, 9603, 11624, 11252]          4\n",
       "13              )      20625                         [13]          1\n",
       "14              ,     113641                         [16]          1\n",
       "15   i̇nterwetten          1    [25272, 4569, 5734, 1008]          4\n",
       "16        paddock          1          [14176, 3837, 2536]          3\n",
       "17           moto          3                 [3543, 1020]          2\n",
       "18              2      44964                         [22]          1\n",
       "19         takımı        646                       [3282]          1\n",
       "20           için      10517                       [2051]          1\n",
       "21          grand        209                       [6116]          1\n",
       "22           prix         75                       [8073]          1\n",
       "23          dünya       2445                       [2458]          1\n",
       "24    şampiyonası        490                       [4100]          1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deneme = frequency_df.head(25)\n",
    "deneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>token_ids</th>\n",
       "      <th>token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>prix</td>\n",
       "      <td>75</td>\n",
       "      <td>[8073]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>moto</td>\n",
       "      <td>3</td>\n",
       "      <td>[3543, 1020]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eylül</td>\n",
       "      <td>1304</td>\n",
       "      <td>[2873]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>=</td>\n",
       "      <td>47238</td>\n",
       "      <td>[32]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>,</td>\n",
       "      <td>113641</td>\n",
       "      <td>[16]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>44964</td>\n",
       "      <td>[22]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>grand</td>\n",
       "      <td>209</td>\n",
       "      <td>[6116]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>dünya</td>\n",
       "      <td>2445</td>\n",
       "      <td>[2458]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>için</td>\n",
       "      <td>10517</td>\n",
       "      <td>[2051]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>68283</td>\n",
       "      <td>[21]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(</td>\n",
       "      <td>20570</td>\n",
       "      <td>[12]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>15960</td>\n",
       "      <td>[26]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>;</td>\n",
       "      <td>5450</td>\n",
       "      <td>[31]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i̇nterwetten</td>\n",
       "      <td>1</td>\n",
       "      <td>[25272, 4569, 5734, 1008]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d</td>\n",
       "      <td>2524</td>\n",
       "      <td>[44]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>takımı</td>\n",
       "      <td>646</td>\n",
       "      <td>[3282]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>paddock</td>\n",
       "      <td>1</td>\n",
       "      <td>[14176, 3837, 2536]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>37906</td>\n",
       "      <td>[29]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>)</td>\n",
       "      <td>20625</td>\n",
       "      <td>[13]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>18691</td>\n",
       "      <td>[28]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>şampiyonası</td>\n",
       "      <td>490</td>\n",
       "      <td>[4100]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>oberdiessbach</td>\n",
       "      <td>1</td>\n",
       "      <td>[17368, 9603, 11624, 11252]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.</td>\n",
       "      <td>155540</td>\n",
       "      <td>[18]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lüthi</td>\n",
       "      <td>2</td>\n",
       "      <td>[13229, 3255]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thomas</td>\n",
       "      <td>119</td>\n",
       "      <td>[6647]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  frequency                    token_ids  token_len\n",
       "22           prix         75                       [8073]          1\n",
       "17           moto          3                 [3543, 1020]          2\n",
       "7           eylül       1304                       [2873]          1\n",
       "0               =      47238                         [32]          1\n",
       "14              ,     113641                         [16]          1\n",
       "18              2      44964                         [22]          1\n",
       "21          grand        209                       [6116]          1\n",
       "23          dünya       2445                       [2458]          1\n",
       "20           için      10517                       [2051]          1\n",
       "8               1      68283                         [21]          1\n",
       "3               (      20570                         [12]          1\n",
       "6               6      15960                         [26]          1\n",
       "11              ;       5450                         [31]          1\n",
       "15   i̇nterwetten          1    [25272, 4569, 5734, 1008]          4\n",
       "4               d       2524                         [44]          1\n",
       "19         takımı        646                       [3282]          1\n",
       "16        paddock          1          [14176, 3837, 2536]          3\n",
       "9               9      37906                         [29]          1\n",
       "13              )      20625                         [13]          1\n",
       "10              8      18691                         [28]          1\n",
       "24    şampiyonası        490                       [4100]          1\n",
       "12  oberdiessbach          1  [17368, 9603, 11624, 11252]          4\n",
       "5               .     155540                         [18]          1\n",
       "2           lüthi          2                [13229, 3255]          2\n",
       "1          thomas        119                       [6647]          1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deneme.sample(n= (len(deneme) if len(deneme) < 30 else 30), random_state=13013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation marks: [',', '!', '?', '~', '~', \"'\", '^', '^', '#', '*', '/', '*', '/', '-', '+']\n",
      "Numbers: ['123', '5', '5', '5555']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, World! 123 5 5 5555? ~~'^ ^ #         aa */*/-+\"\n",
    "\n",
    "# Find punctuation\n",
    "punctuations = re.findall(r'[^\\w\\s]', text)\n",
    "print(\"Punctuation marks:\", punctuations)\n",
    "\n",
    "# Find numbers\n",
    "numbers = re.findall(r'\\d+', text)\n",
    "print(\"Numbers:\", numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = normalizers.Sequence([normalizers.NFKC(),\n",
    "                                       normalizers.Lowercase(),\n",
    "                                       normalizers.Replace(Regex('[^\\w\\s]'),\"\"),   # for numbers\n",
    "                                       normalizers.Replace(Regex('\\d+'),\"\") ])     # for punctiations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world                aa '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.normalize_str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.to_json(\"random_word_set.json\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_json(\"random_word_set.json\", orient=\"records\", lines=True, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>token_ids</th>\n",
       "      <th>token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>çadır</td>\n",
       "      <td>415</td>\n",
       "      <td>[14594]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>temelde</td>\n",
       "      <td>995</td>\n",
       "      <td>[17473]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roy</td>\n",
       "      <td>884</td>\n",
       "      <td>[13952]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oluştuğunu</td>\n",
       "      <td>634</td>\n",
       "      <td>[23155]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reklam</td>\n",
       "      <td>3040</td>\n",
       "      <td>[6580]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vektör</td>\n",
       "      <td>1135</td>\n",
       "      <td>[9327]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>faust</td>\n",
       "      <td>268</td>\n",
       "      <td>[21706]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>projesi</td>\n",
       "      <td>4675</td>\n",
       "      <td>[6874]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>orta</td>\n",
       "      <td>33740</td>\n",
       "      <td>[2673]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>suçsuz</td>\n",
       "      <td>329</td>\n",
       "      <td>[31200]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  frequency token_ids  token_len\n",
       "0       çadır        415   [14594]          1\n",
       "1     temelde        995   [17473]          1\n",
       "2         roy        884   [13952]          1\n",
       "3  oluştuğunu        634   [23155]          1\n",
       "4      reklam       3040    [6580]          1\n",
       "5      vektör       1135    [9327]          1\n",
       "6       faust        268   [21706]          1\n",
       "7     projesi       4675    [6874]          1\n",
       "8        orta      33740    [2673]          1\n",
       "9      suçsuz        329   [31200]          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word           göstermişti\n",
       "frequency              250\n",
       "token_ids    [15099, 2008]\n",
       "token_len                2\n",
       "Name: 6302, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sort_values(by=\"frequency\", ascending=False).iloc[10_500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dic = {}\n",
    "\n",
    "for name, grup in a.groupby(\"token_len\"):\n",
    "    temp_dic[f\"token_group_{name}\"] = grup[\"token_ids\"].to_list()\n",
    "    temp_dic[f\"token_group_{name}_length\"] = len(temp_dic[f\"token_group_{name}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token_group_1', 'token_group_1_length', 'token_group_2', 'token_group_2_length', 'token_group_3', 'token_group_3_length', 'token_group_4', 'token_group_4_length', 'token_group_5', 'token_group_5_length'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[17227, 22538, 1011],\n",
       " [9857, 22538, 1921],\n",
       " [14665, 22538, 1011],\n",
       " [27939, 22538, 1007],\n",
       " [7707, 22538, 1011]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dic[\"token_group_3\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_word_set():\n",
    "    if not os.path.exists(\"random_word_set.json\"):\n",
    "        print(f\"[INFO] random_word_set.json is not already exists. Try to execute random_word_set.py script to generate this file before calling this function...\")\n",
    "        exit(0)\n",
    "    \n",
    "    random_word_set_df = pd.read_json(\"random_word_set.json\", orient=\"records\", lines=True, encoding=\"utf-8\")\n",
    "    random_word_set_dict = {}\n",
    "    \n",
    "    for group_name, group in random_word_set_df.groupby(\"token_len\"):\n",
    "        random_word_set_dict[f\"token_group_{group_name}\"] = group[\"token_ids\"].to_list()\n",
    "        random_word_set_dict[f\"token_group_{group_name}_length\"] = len(random_word_set_dict[f\"token_group_{group_name}\"])\n",
    "    \n",
    "    return random_word_set_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = get_random_word_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[17227, 22538, 1011],\n",
       "  [9857, 22538, 1921],\n",
       "  [14665, 22538, 1011],\n",
       "  [27939, 22538, 1007],\n",
       "  [7707, 22538, 1011]],\n",
       " dict_keys(['token_group_1', 'token_group_1_length', 'token_group_2', 'token_group_2_length', 'token_group_3', 'token_group_3_length', 'token_group_4', 'token_group_4_length', 'token_group_5', 'token_group_5_length']))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc[\"token_group_3\"][:5], bc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAGE0 + STAGE1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The purpose of this script is to create the data for the BERT training\n",
    "\n",
    "NSP taski büyük veri setlerinde aslında avantajlı değil aksine model performansını düşürüyor (electra paper'i sanırsam buna bak)\n",
    "Ancak küçük veri setleri için kullanımı (ki benim durumum) genel model performansını olumlu yönde etkiler.\n",
    "NSP taskininde düzgün çalışabilmesi için A sonu, B başının gerçek cümlelerdeki gibi olması önemlidir.\n",
    "Diğer türlü A cümle ortası ile biter, B de cümle ortası ile başlarsa (A: Ali ata (bak) B: (Sevdiğim) şarkılar bunlar..., notNext)\n",
    "Model nsp taskinde bu tarz example'ları basit gramatik bilgiler ile (ki genelde modelin ilk öğreneceği şeylerdir) halledebilir, \n",
    "complex ilişkilendirmelere gerek duymaz.\n",
    "\n",
    "görünüşe göre pad token'ına ihtiyacım olmayacak.\n",
    "\n",
    "(., !, ?, ..., :, ;) cümleleri bu tokenlara göre split edeceğim.\n",
    "\n",
    "\"kelime\" açısından istenilen oranlar sağlansa da, span corruption yapıldığından \"token\" açısından\n",
    "istatistiği merak ediyorum yüzdesel olarak ne kadar artacak?\n",
    "\n",
    "toplam veri seti default overlap param ile yaklaşık 2 x artacak (overlap yarı old'dan 0,5 x gelecek, notNext old yani random sample\n",
    "olduğunda kernel/block hareketi/posizyonu esasında değişmeyecek bunu da havadan yeni bir example oluşturma gibi düşünebilirsin + 0,5 x\n",
    "daha toplamda yaklaşık olarak 2x artmış olur) \n",
    "\n",
    "tokenların yaklaşık yarısı bir epoch'da 3 kere gözükecek (overlap + random sample sayesinde)\n",
    "kalan yarısı da 2 kere gözükecek (sadece overlap sayesinde) \n",
    "\n",
    "random word set stage:\n",
    "    * bunu script şekilde yazdım. burada (data.py) eğer beklenen dosya yok ise bu exec edilsin\n",
    "    * word_count_per_token_len_group (örn 500 ise her token grubu için en fazla 500 tane kelime olacak )\n",
    "    * \n",
    "    * \n",
    "    *\n",
    "\n",
    "\n",
    "Her stage hali hazırda yapılmış ise atlansın\n",
    "Her stage kalınıldığı noktadan devam edebilmeli (shard index ile)\n",
    "1st Stage(documents):\n",
    "    * read trwiki-67 files and merge them (as a big one string object)\n",
    "    * split titles and docs (just keep docs btw, i will not use titles for training)(boş/null doc olmamalı)\n",
    "    * tüm docs list shuffle edilsin (raw dosya dizilimi random şekilde mi bilmiyorum, ondan her türlü bir shuffling yapalım)\n",
    "    * delete subtitles from docs (aproximately if line has less than 4 words) (list of docs)\n",
    "    * save this list of docs json in shards\n",
    "    * bu stage'in düzgün çalışıp çalışmadığını kontrol et\n",
    "    * free all resources\n",
    "\n",
    "Her stage hali hazırda yapılmış ise atlansın\n",
    "Her stage kalınıldığı noktadan devam edebilmeli (shard index ile)\n",
    "2nd Stage (ab_tokenized):\n",
    "    * 2.1 Stage (tokenization)\n",
    "        + load docs json file as pandas dataframe (fonk yaz belirtilen shard indexini yükleyen)\n",
    "        + en son kalınılan shard'tan devam edebilmeli\n",
    "        + her doc \"string\" tokenize edilecek yeni kolonlar türeyecek: token_ids, word_ids\n",
    "        + düz string kolonu dropla (list[str]) olan\n",
    "        + her doc'da sentence indexlerini bulmalı yeni kolona bunları koymalı (doc uzunluğu en sonki index'mi öyle değil ise ekle)\n",
    "        + kısaca token len için ayrı kolon olmayacak, sentence index'lerdeki en son elemandan bu çıkartılabilmeli\n",
    "        + 2 cümleden az olan (yani tek cümleli) doc'lar filtrelenecek\n",
    "        + doc [token_ids, word_ids, sent_idx]\n",
    "        + bu stage'in düzgün çalışıp çalışmadığını kontrol et\n",
    "        + 2.2'ye geçmeden free resources\n",
    "\n",
    "    * 2.2 Stage (ab creation)\n",
    "        + block_size, overlap, shard başına kaç ab sample (shard_size) gibi parametrelere göre çalışacak\n",
    "        + isnext olasılığı al, next ise:\n",
    "            - bloğu doldurulabildiği kadar doldurulacak, yer kalırsa (truncation sayesinde çoğu durumda kalmasa da document'ın sonuna\n",
    "              geldiğimizde dolduramama ihtimali var) [PAD] token'ları ile doldurulacak\n",
    "        + notNext ise:\n",
    "            - random sample alınacak, sample genişliği belli kısıtlamalar ile olacak, eğer kısıtlamalar sağlanamıyorsa başka random sample\n",
    "              yapılacak\n",
    "            - random sample alındıktan sonra bulunulan kernel/block posizyonunda gene alınabilecek kadar alınacak (sonu cümle gibi bitmeli) [çok düşük ihtimal ama ilk sentence aşarsa bir sonraki iterasyona atlanmalı]\n",
    "            - kalan yerler pad ile doldurulacak (bunda pad olasılığı yüksek)\n",
    "        + tüm shard shuffle edilmeli (ab halde shuffle et shard'ı)\n",
    "        + ab sample'ında kaç tane kelime var ise ayrı kolon yap\n",
    "        + 2.stage sonu, shard kayıtları (ab sayılarına göre yapılabilir (her doc farklı sayıda ab çıkarabilir sonuçta) ancak aynı kalsın, zaten son\n",
    "          aşamada her shard'ı token sayısı eşit olacak biçimde tasarlayacağım (o aşamaya kadar şard'larımız hep aynı sayıda doc olacak şekilde))\n",
    "        + bu stage'in düzgün çalışıp çalışmadığını kontrol et\n",
    "        + free all resources\n",
    "\n",
    "        ++ toplam veri seti default overlap param ile yaklaşık 2 x artacak (overlap yarı old'dan 0,5 x gelecek, notNext old yani random sample\n",
    "           olduğunda kernel/block hareketi/posizyonu esasında değişmeyecek bunu da havadan yeni bir example oluşturma gibi düşünebilirsin + 0,5 x\n",
    "           daha toplamda yaklaşık olarak 2x artmış olur) \n",
    "           tokenların yaklaşık yarısı bir epoch'da 3 kere gözükecek (overlap + random sample sayesinde)\n",
    "        ++ kalan yarısı da 2 kere gözükecek (sadece overlap sayesinde) \n",
    "    \n",
    "\n",
    "Her stage hali hazırda yapılmış ise atlansın\n",
    "Her stage kalınıldığı noktadan devam edebilmeli (shard index ile)\n",
    "3rd Stage(xy_numpy):\n",
    "    * xy'ler numpy arrayler vs oluşturma, SON aşama\n",
    "    * % kaç [MASK] token'i var?, % kaç yer değiştirme, aynı bırakma var vs (\"kelime\" açısından istenilen oranlar sağlansa da, span corruption yapıldığından \"token\" açısından\n",
    "    istatistiği merak ediyorum yüzdesel olarak ne kadar artacak?) \n",
    "    * ekstrem kelimelerde (8 token misal) random kelime yoksa kaçsın ya da MASK yapsın\n",
    "    * random word json, unpackleneerek sadece list[list[int]] şekline çevirip kullanmayı deneyelim (idx'ler token len)\n",
    "      böylece manager dict yerine düz Array kullanabiliriz\n",
    "    * random kelimeleri negative sample gibi düşünebiliriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# third party library\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# local\n",
    "import bert_implementation_tr.data.old_data as old_data\n",
    "random.seed(13013)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SHARDS_FOR_DOC = 100\n",
    "NUM_DOCS_PER_SHARD = 0\n",
    "NUM_SHARDS_FOR_AB = 100\n",
    "NUM_SHARDS_FOR_XY = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_subtitles_from_docs(docs):\n",
    "    \"\"\"\n",
    "    Yaklaşıl 4 kelimeden oluşan cümleler hep alt başlık gibi (wiki dump kaynaklı). Veriye bakarken bu alt başlıkların paragraflar arasında anlam bozukluğuna sebep olabileceğini düşündüm\n",
    "    Küçük veri seti kullanacak olan modellerde bu sinyallerin performansı kötü etkileyeceğini düşündüğümden (deney yapılabilir)\n",
    "    bunları silmeye karar kıldım.\n",
    "    \"\"\"\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        new_doc = []\n",
    "        for line in doc.splitlines():\n",
    "            if len(line.split(\" \")) > 5:\n",
    "                new_doc.append(line)\n",
    "        if new_doc: # for example, doc can have 1 sentence and it has less then 5 words\n",
    "            # let's convert in the same format (join lines again)\n",
    "            new_docs.append(\"\\n\".join(new_doc))\n",
    "    \n",
    "    \n",
    "    assert not any(len(doc) == 0 for doc in new_docs), \"[INFO] Unexpected, empty docs list has empty doc list...\"\n",
    "\n",
    "    return new_docs\n",
    "\n",
    "def get_docs_df():\n",
    "    \n",
    "    print(f\"[INFO] Docs dataframe is being created...\")\n",
    "\n",
    "    merged_content = old_data.get_merged_files()\n",
    "    _, docs = old_data.split_titles_and_docs(merged_content)\n",
    "\n",
    "    del merged_content\n",
    "\n",
    "    # inplace operation\n",
    "    random.shuffle(docs)\n",
    "\n",
    "    docs = delete_subtitles_from_docs(docs)\n",
    "\n",
    "    return pd.DataFrame(docs, columns=[\"doc\"])\n",
    "\n",
    "def create_doc_shards(docs_df):\n",
    "\n",
    "    global NUM_SHARDS_FOR_DOC\n",
    "    global NUM_DOCS_PER_SHARD\n",
    "\n",
    "    NUM_DOCS_PER_SHARD = len(docs_df) // NUM_SHARDS_FOR_DOC\n",
    "\n",
    "\n",
    "    if len(docs_df) % NUM_SHARDS_FOR_DOC != 0:\n",
    "        extra_shard_len = len(docs_df) % NUM_SHARDS_FOR_DOC\n",
    "        NUM_SHARDS_FOR_DOC += 1\n",
    "        \n",
    "\n",
    "\n",
    "    os.makedirs(\"doc_shards\", exist_ok=True)\n",
    "    files = os.listdir(\"./doc_shards\")\n",
    "\n",
    "\n",
    "    if len(files) == NUM_SHARDS_FOR_DOC:\n",
    "        print(f\"[INFO] Expected number of shards are already created. Exiting...\")\n",
    "        return\n",
    "    \n",
    "    # fazlalık dosyaları (eğer olursa) sil (evet biraz brute force)\n",
    "    if NUM_SHARDS_FOR_DOC < len(files):\n",
    "\n",
    "        files_finded = set(files)\n",
    "        shard_files_expected = {f\"doc_shard_{shard_idx}.json\" for shard_idx in range(NUM_SHARDS_FOR_DOC)}\n",
    "        files_will_deleted = list(files_finded - shard_files_expected)\n",
    "        if files_will_deleted:\n",
    "            print(f\"[INFO] These files are unexpected, so they are gonna be deleted: {files_will_deleted}\")\n",
    "            for file in files:\n",
    "                os.remove(\"./doc_shards/\" + file)\n",
    "\n",
    "\n",
    "    print(f\"[INFO] Total number of doc: {len(docs_df)}\")\n",
    "    print(f\"[INFO] Each shard will have {NUM_DOCS_PER_SHARD} number of docs...\")\n",
    "    print(f\"[INFO] Number of shard will be: {NUM_SHARDS_FOR_DOC}\")\n",
    "    if extra_shard_len:\n",
    "        print(f\"[INFO] Extra shard needed. Number of doc will be placed: {extra_shard_len}\")\n",
    "\n",
    "    \n",
    "    df_start_idx = 0\n",
    "    df_end_idx = NUM_DOCS_PER_SHARD\n",
    "\n",
    "\n",
    "    with tqdm.tqdm(total=NUM_SHARDS_FOR_DOC, desc=\"Doc shards are being created\") as pbar:\n",
    "        for shard_idx in range(0, NUM_SHARDS_FOR_DOC):\n",
    "            # last iteration (extra shard)\n",
    "            if shard_idx == NUM_SHARDS_FOR_DOC - 1:\n",
    "                df_end_idx = df_start_idx + extra_shard_len\n",
    "\n",
    "                assert df_end_idx == len(docs_df)\n",
    "                assert (df_end_idx - df_start_idx) == extra_shard_len\n",
    "\n",
    "                docs_df.iloc[df_start_idx:df_end_idx].to_json(\"./doc_shards/\" + f\"doc_shard_{shard_idx}.json\",\n",
    "                                                          orient=\"records\",\n",
    "                                                          lines=True,\n",
    "                                                          force_ascii=False)\n",
    "                pbar.update()\n",
    "                break\n",
    "\n",
    "            docs_df.iloc[df_start_idx:df_end_idx].to_json(\"./doc_shards/\" + f\"doc_shard_{shard_idx}.json\",\n",
    "                                                          orient=\"records\",\n",
    "                                                          lines=True,\n",
    "                                                          force_ascii=False)\n",
    "            df_start_idx = df_end_idx\n",
    "            df_end_idx += NUM_DOCS_PER_SHARD\n",
    "            pbar.update()\n",
    "\n",
    "def read_shard(shard_dir, shard_idx):\n",
    "\n",
    "    if shard_idx >= 0 and shard_idx <= NUM_SHARDS_FOR_DOC:\n",
    "\n",
    "        return pd.read_json(shard_dir + f\"doc_shard_{shard_idx}.json\",\n",
    "                            orient=\"records\",\n",
    "                            lines=True,\n",
    "                            encoding=\"utf-8\")\n",
    "    \n",
    "    else:\n",
    "        raise IndexError(\"shard_idx >= 0 and shard_idx < NUM_SHARDS_FOR_DOC\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Docs dataframe is being created...\n",
      "[INFO] Files in dir: ['trwiki-67-test.raw', 'trwiki-67-train.raw', 'trwiki-67-val.raw']...\n",
      "[INFO] Expected number of shards are already created. Exiting...\n"
     ]
    }
   ],
   "source": [
    "# MAIN BLOCK\n",
    "\n",
    "old_data.appy_seed()\n",
    "\n",
    "docs_df = get_docs_df()\n",
    "\n",
    "create_doc_shards(docs_df)\n",
    "\n",
    "# free resource\n",
    "del docs_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her stage hali hazırda yapılmış ise atlansın\n",
    "Her stage kalınıldığı noktadan devam edebilmeli (shard index ile)\n",
    "2nd Stage (ab_tokenized):\n",
    "    * 2.1 Stage (tokenization)\n",
    "        + load docs json file as pandas dataframe (fonk yaz belirtilen shard indexini yükleyen)\n",
    "        + en son kalınılan shard'tan devam edebilmeli\n",
    "        + her doc \"string\" tokenize edilecek yeni kolonlar türeyecek: token_ids, word_ids\n",
    "        + düz string kolonu dropla (list[str]) olan\n",
    "        + her doc'da sentence indexlerini bulmalı yeni kolona bunları koymalı (doc uzunluğu en sonki index'mi öyle değil ise ekle)\n",
    "        + kısaca token len için ayrı kolon olmayacak, sentence index'lerdeki en son elemandan bu çıkartılabilmeli\n",
    "        + 2 cümleden az olan (yani tek cümleli) doc'lar filtrelenecek\n",
    "        + doc [token_ids, word_ids, sent_idx]\n",
    "        + bu stage'in düzgün çalışıp çalışmadığını kontrol et\n",
    "        + 2.2'ye geçmeden free resources\n",
    "\n",
    "    * 2.2 Stage (ab creation)\n",
    "        + block_size, overlap, shard başına kaç ab sample (shard_size) gibi parametrelere göre çalışacak\n",
    "        + isnext olasılığı al, next ise:\n",
    "            - bloğu doldurulabildiği kadar doldurulacak, yer kalırsa (truncation sayesinde çoğu durumda kalmasa da document'ın sonuna\n",
    "              geldiğimizde dolduramama ihtimali var) [PAD] token'ları ile doldurulacak\n",
    "        + notNext ise:\n",
    "            - random sample alınacak, sample genişliği belli kısıtlamalar ile olacak, eğer kısıtlamalar sağlanamıyorsa başka random sample\n",
    "              yapılacak\n",
    "            - random sample alındıktan sonra bulunulan kernel/block posizyonunda gene alınabilecek kadar alınacak (sonu cümle gibi bitmeli) [çok düşük ihtimal ama ilk sentence aşarsa bir sonraki iterasyona atlanmalı]\n",
    "            - kalan yerler pad ile doldurulacak (bunda pad olasılığı yüksek)\n",
    "        + tüm shard shuffle edilmeli (ab halde shuffle et shard'ı)\n",
    "        + ab sample'ında kaç tane kelime var ise ayrı kolon yap\n",
    "        + 2.stage sonu, shard kayıtları (ab sayılarına göre yapılabilir (her doc farklı sayıda ab çıkarabilir sonuçta) ancak aynı kalsın, zaten son\n",
    "          aşamada her shard'ı token sayısı eşit olacak biçimde tasarlayacağım (o aşamaya kadar şard'larımız hep aynı sayıda doc olacak şekilde))\n",
    "        + bu stage'in düzgün çalışıp çalışmadığını kontrol et\n",
    "        + free all resources\n",
    "\n",
    "        ++ toplam veri seti default overlap param ile yaklaşık 2 x artacak (overlap yarı old'dan 0,5 x gelecek, notNext old yani random sample\n",
    "           olduğunda kernel/block hareketi/posizyonu esasında değişmeyecek bunu da havadan yeni bir example oluşturma gibi düşünebilirsin + 0,5 x\n",
    "           daha toplamda yaklaşık olarak 2x artmış olur) \n",
    "           tokenların yaklaşık yarısı bir epoch'da 3 kere gözükecek (overlap + random sample sayesinde)\n",
    "        ++ kalan yarısı da 2 kere gözükecek (sadece overlap sayesinde) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Stage (tokenization)\n",
    "\n",
    "- load docs json file as pandas dataframe (fonk yaz belirtilen shard indexini yükleyen)\n",
    "- her doc \"string\" tokenize edilecek yeni kolonlar türeyecek: `token_ids`, `word_ids`\n",
    "- düz string kolonu dropla (list[str] olan)\n",
    "- her doc'da sentence indexlerini bulmalı yeni kolona bunları koymalı (doc uzunluğu en sonki index'mi öyle değil ise ekle)\n",
    "- kısaca token len için ayrı kolon olmayacak, sentence index'lerdeki en son elemandan bu çıkartılabilmeli\n",
    "- 2 cümleden az olan (yani tek cümleli) doc'lar filtrelenecek\n",
    "- doc `[token_ids, word_ids, sent_idx]`\n",
    "- bu stage'in düzgün çalışıp çalışmadığını kontrol et\n",
    "- 2.2'ye geçmeden free resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\build_and_train_bert_model\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# standard library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from typing import List, Dict\n",
    "from itertools import chain\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "\n",
    "# third party library\n",
    "import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# local\n",
    "import bert_implementation_tr.data.old_data as old_data\n",
    "\n",
    "random.seed(13013)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\build_and_train_bert_model\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# DEFAULT VALUES\n",
    "NUM_SHARDS_FOR_DOC = 100\n",
    "NUM_DOCS_PER_SHARD = 0\n",
    "NUM_SHARDS_FOR_AB = 100\n",
    "NUM_SHARDS_FOR_XY = 100\n",
    "\n",
    "# DEFAULT VALUES (değiştirilebilir/config/param)\n",
    "BLOCK_SIZE = 256\n",
    "OVERLAP = BLOCK_SIZE // 2 # pencere hareketi/deltası kesinlikle 4 dene!\n",
    "\n",
    "TAMPON = 10  # block penceresi sınırlarını tamponluyoruz gibi düşünülebilir (tampon içinde kalan sent idx'ler kullanılmayacak)\n",
    "\n",
    "tokenizer = old_data.get_tokenizer(fast=True)\n",
    "\n",
    "# multi language sentence tokenizer\n",
    "sent_seperator = spacy.load(\"xx_sent_ud_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc_shard(shard_idx):\n",
    "    \" returns: pd.DataFrame\"\n",
    "\n",
    "    if shard_idx >= 0 and shard_idx < NUM_SHARDS_FOR_DOC:\n",
    "\n",
    "        return pd.read_json(\"./doc_shards/\" + f\"doc_shard_{shard_idx}.json\",\n",
    "                            orient=\"records\",\n",
    "                            lines=True,\n",
    "                            encoding=\"utf-8\")\n",
    "    \n",
    "    else:\n",
    "        raise IndexError(\"shard_idx >= 0 and shard_idx < NUM_SHARDS_FOR_DOC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['doc'], dtype='object'), 3936)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = read_doc_shard(0)\n",
    "temp_df.columns, len(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series,\n",
       " doc          Sırp Demokratik Partisi (Sırpça: Српска демокр...\n",
       " token_ids                                                 None\n",
       " word_ids                                                  None\n",
       " sent_idx                                                  None\n",
       " Name: 13, dtype: object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_doc = temp_df.iloc[13]\n",
    "one_doc[\"token_ids\"] = None\n",
    "one_doc[\"word_ids\"] = None\n",
    "one_doc[\"sent_idx\"] = None\n",
    "type(one_doc), one_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_sent_idx(doc: pd.Series)-> pd.Series:\n",
    "    idx = []\n",
    "\n",
    "    temp = {\"doc\":doc[\"doc\"], \"token_ids\":[], \"word_ids\":[], \"sent_idx\":[]}\n",
    "\n",
    "    old_idx = 0\n",
    "    last_word_id = -1\n",
    "    \n",
    "    for sentence in list(sent_seperator(temp[\"doc\"]).sents):\n",
    "        encoding = tokenizer(sentence.text)\n",
    "        old_idx = len(encoding[\"input_ids\"]) + old_idx\n",
    "        \n",
    "        temp[\"sent_idx\"].append(old_idx)\n",
    "        temp[\"token_ids\"].extend(encoding[\"input_ids\"])\n",
    "        temp[\"word_ids\"].extend((np.array(encoding.word_ids()) + last_word_id + 1).tolist())\n",
    "        last_word_id = temp[\"word_ids\"][-1]\n",
    "\n",
    "    # because of zero index system\n",
    "    temp[\"sent_idx\"] = [ idx - 1 for idx in temp[\"sent_idx\"]]\n",
    "\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc': 'Sırp Demokratik Partisi (Sırpça: Српска демократска странка/\"Srpska demokratska stranka\", СДС/SDS) Bosna-Hersek\\'te bir Sırp siyasi partisidir. Şu anki lideri Mirko Šarović\\'tir.\\nEkim 2006\\'daki parlamento seçimlerinde SDP, Sırp Cumhuriyeti Başbakanı Milorad Dodik liderliğindeki Bağımsız Sosyal Demokratlar İttifakı\\'na, Sırp Cumhuriyeti\\'nin önde gelen partisi ve Bosna Hersek\\'teki ana Sırp partisi konumunu kaybetti. 2010 ve 2014 seçimlerinde küçük kazanımlar elde etmesine rağmen, 2018 yılına gelindiğinde partinin parlamentoda temsil oranı %20\\'nin altına düştü ve tarihindeki en düşük sandalye sayısına sahip oldu. Sırp Demokratik Partisi, \"savaş suçu zanlılarını tutuklayıp uluslararası bir mahkemeye teslim edemediği\" gerekçesiyle ABD\\'nin yaptırımı altında bulunuyor. Yaptırımlar, Amerika Birleşik Devletleri\\'nden SDP\\'ye herhangi bir fon ve materyal transferini yasaklar ve bunun tersi durumu için de geçerlidir. Parti, Yabancı Varlıkları Kontrol Ofisi ABD ajansı tarafından Özel Olarak Belirlenmiş Vatandaşlar ve Engellenen Kişiler listesinde yer almaktadır.\\nRadovan Karadžić 1990\\'da Sırp Demokratik Partisi\\'ni kurdu. Parti, Jovan Rašković\\'in Sırp Demokratik Partisi\\'nin Hırvatistan\\'daki Sırplarla yaptığı gibi Bosnalı Sırp toplumunu birleştirmeyi ve bu iki cumhuriyetin federasyondan ayrılması durumunda Yugoslavya\\'nın (Sırbistan-Karadağ ile \"Üçüncü Yugoslavya\" olarak) bir parçası olarak kalmayı hedefliyordu.\\nEylül 1991 boyunca SDP, Bosna-Hersek genelinde çeşitli \"Sırp Özerk Bölgeleri\" kurmaya başladı. 15 Ekim 1991\\'de Bosna parlamentosunun egemenlik oylamasından sonra, Bosna-Hersek\\'teki Sırpları münhasıran temsil etmek için 24 Ekim 1991\\'de Banja Luka\\'da ayrı bir Sırp Meclisi kuruldu. Ertesi ay, Bosnalı Sırplar, Yugoslavya\\'nın bir parçası olarak Sırbistan ve Karadağ ile federal bir eyalet olarak kalmak lehine ezici bir oylama ile sonuçlanan bir referandum düzenlediler. Aralık 1991\\'de SDP liderliği tarafından \"Sırpların Bosna-Hersek\\'teki organlarının olağanüstü koşullarda örgütlenmesi ve faaliyetine ilişkin\" başlıklı çok gizli bir belge hazırlandı. Bu belge, çeşitli \"kriz merkezleri\" aracılığıyla gölge hükümetler ve hükümet dışı yapıların oluşturulması yoluyla ve Yugoslav Halk Ordusu ile koordinasyon içinde olan sadık Sırpları devralmaya hazırlayarak, ülkedeki her belediyenin ele geçirilmesi için merkezi bir programdı',\n",
       " 'token_ids': [6221,\n",
       "  6082,\n",
       "  3589,\n",
       "  12,\n",
       "  19711,\n",
       "  30,\n",
       "  278,\n",
       "  1089,\n",
       "  1199,\n",
       "  27456,\n",
       "  265,\n",
       "  1045,\n",
       "  1044,\n",
       "  1067,\n",
       "  1121,\n",
       "  18558,\n",
       "  1047,\n",
       "  27456,\n",
       "  278,\n",
       "  1047,\n",
       "  18558,\n",
       "  1046,\n",
       "  25164,\n",
       "  19,\n",
       "  6,\n",
       "  12642,\n",
       "  5901,\n",
       "  2223,\n",
       "  6779,\n",
       "  12863,\n",
       "  22382,\n",
       "  2223,\n",
       "  6,\n",
       "  16,\n",
       "  278,\n",
       "  1123,\n",
       "  1198,\n",
       "  19,\n",
       "  13216,\n",
       "  1014,\n",
       "  13,\n",
       "  7327,\n",
       "  17,\n",
       "  8786,\n",
       "  11,\n",
       "  2072,\n",
       "  1941,\n",
       "  6221,\n",
       "  3808,\n",
       "  3589,\n",
       "  1990,\n",
       "  18,\n",
       "  3340,\n",
       "  12703,\n",
       "  6025,\n",
       "  4285,\n",
       "  3516,\n",
       "  159,\n",
       "  28912,\n",
       "  12285,\n",
       "  11,\n",
       "  5588,\n",
       "  18,\n",
       "  2871,\n",
       "  22,\n",
       "  20,\n",
       "  20,\n",
       "  26,\n",
       "  11,\n",
       "  2680,\n",
       "  6906,\n",
       "  6558,\n",
       "  13216,\n",
       "  1027,\n",
       "  16,\n",
       "  6221,\n",
       "  3419,\n",
       "  7334,\n",
       "  22915,\n",
       "  5047,\n",
       "  25618,\n",
       "  1957,\n",
       "  12929,\n",
       "  3498,\n",
       "  3164,\n",
       "  25637,\n",
       "  17865,\n",
       "  11,\n",
       "  2655,\n",
       "  16,\n",
       "  6221,\n",
       "  3419,\n",
       "  11,\n",
       "  2145,\n",
       "  6520,\n",
       "  2554,\n",
       "  3589,\n",
       "  1933,\n",
       "  7327,\n",
       "  8786,\n",
       "  11,\n",
       "  5068,\n",
       "  3076,\n",
       "  6221,\n",
       "  3589,\n",
       "  16913,\n",
       "  6036,\n",
       "  18,\n",
       "  22,\n",
       "  20,\n",
       "  21,\n",
       "  20,\n",
       "  1933,\n",
       "  22,\n",
       "  20,\n",
       "  21,\n",
       "  24,\n",
       "  6558,\n",
       "  2774,\n",
       "  2429,\n",
       "  16871,\n",
       "  3306,\n",
       "  11708,\n",
       "  3213,\n",
       "  16,\n",
       "  22,\n",
       "  20,\n",
       "  21,\n",
       "  28,\n",
       "  4138,\n",
       "  9514,\n",
       "  7737,\n",
       "  28943,\n",
       "  3113,\n",
       "  5801,\n",
       "  9,\n",
       "  22,\n",
       "  20,\n",
       "  11,\n",
       "  2145,\n",
       "  5011,\n",
       "  6432,\n",
       "  1933,\n",
       "  8640,\n",
       "  2060,\n",
       "  4184,\n",
       "  13852,\n",
       "  13592,\n",
       "  2427,\n",
       "  2075,\n",
       "  18,\n",
       "  6221,\n",
       "  6082,\n",
       "  3589,\n",
       "  16,\n",
       "  6,\n",
       "  2358,\n",
       "  17997,\n",
       "  14862,\n",
       "  28006,\n",
       "  1012,\n",
       "  5315,\n",
       "  6552,\n",
       "  3079,\n",
       "  1941,\n",
       "  20238,\n",
       "  5541,\n",
       "  22444,\n",
       "  2026,\n",
       "  6,\n",
       "  8946,\n",
       "  2782,\n",
       "  11,\n",
       "  2145,\n",
       "  25890,\n",
       "  1012,\n",
       "  3157,\n",
       "  11153,\n",
       "  18,\n",
       "  25890,\n",
       "  1927,\n",
       "  16,\n",
       "  2928,\n",
       "  2979,\n",
       "  3559,\n",
       "  11,\n",
       "  3809,\n",
       "  13216,\n",
       "  1027,\n",
       "  11,\n",
       "  2541,\n",
       "  3948,\n",
       "  1941,\n",
       "  4272,\n",
       "  1933,\n",
       "  8766,\n",
       "  16293,\n",
       "  3644,\n",
       "  5348,\n",
       "  1927,\n",
       "  1933,\n",
       "  2843,\n",
       "  16726,\n",
       "  5137,\n",
       "  2051,\n",
       "  1960,\n",
       "  14522,\n",
       "  18,\n",
       "  3588,\n",
       "  16,\n",
       "  4696,\n",
       "  23128,\n",
       "  3505,\n",
       "  13153,\n",
       "  2782,\n",
       "  10463,\n",
       "  2098,\n",
       "  2331,\n",
       "  2013,\n",
       "  14017,\n",
       "  25543,\n",
       "  1933,\n",
       "  4091,\n",
       "  3030,\n",
       "  5486,\n",
       "  5859,\n",
       "  2056,\n",
       "  3539,\n",
       "  18,\n",
       "  3503,\n",
       "  10033,\n",
       "  31713,\n",
       "  1308,\n",
       "  12285,\n",
       "  21,\n",
       "  29,\n",
       "  29,\n",
       "  20,\n",
       "  11,\n",
       "  1964,\n",
       "  6221,\n",
       "  6082,\n",
       "  3589,\n",
       "  11,\n",
       "  3314,\n",
       "  4458,\n",
       "  18,\n",
       "  3588,\n",
       "  16,\n",
       "  2970,\n",
       "  2644,\n",
       "  3514,\n",
       "  1180,\n",
       "  23110,\n",
       "  11,\n",
       "  2028,\n",
       "  6221,\n",
       "  6082,\n",
       "  3589,\n",
       "  11,\n",
       "  2145,\n",
       "  8542,\n",
       "  11,\n",
       "  2680,\n",
       "  24006,\n",
       "  1940,\n",
       "  3200,\n",
       "  2244,\n",
       "  25512,\n",
       "  6221,\n",
       "  10217,\n",
       "  1025,\n",
       "  23859,\n",
       "  2616,\n",
       "  1933,\n",
       "  1989,\n",
       "  2319,\n",
       "  13638,\n",
       "  9019,\n",
       "  1971,\n",
       "  14358,\n",
       "  6444,\n",
       "  7796,\n",
       "  11,\n",
       "  2151,\n",
       "  12,\n",
       "  6896,\n",
       "  17,\n",
       "  8729,\n",
       "  2023,\n",
       "  6,\n",
       "  3513,\n",
       "  7796,\n",
       "  6,\n",
       "  2013,\n",
       "  13,\n",
       "  1941,\n",
       "  3934,\n",
       "  2013,\n",
       "  13677,\n",
       "  4376,\n",
       "  20319,\n",
       "  18,\n",
       "  2873,\n",
       "  21,\n",
       "  29,\n",
       "  29,\n",
       "  21,\n",
       "  3036,\n",
       "  13216,\n",
       "  1027,\n",
       "  16,\n",
       "  7327,\n",
       "  17,\n",
       "  8786,\n",
       "  7861,\n",
       "  2920,\n",
       "  6,\n",
       "  6221,\n",
       "  5689,\n",
       "  8639,\n",
       "  6,\n",
       "  14643,\n",
       "  2552,\n",
       "  18,\n",
       "  21,\n",
       "  25,\n",
       "  2871,\n",
       "  21,\n",
       "  29,\n",
       "  29,\n",
       "  21,\n",
       "  11,\n",
       "  1960,\n",
       "  7327,\n",
       "  9277,\n",
       "  9145,\n",
       "  17339,\n",
       "  19520,\n",
       "  22538,\n",
       "  1921,\n",
       "  2101,\n",
       "  16,\n",
       "  7327,\n",
       "  17,\n",
       "  8786,\n",
       "  11,\n",
       "  5068,\n",
       "  24006,\n",
       "  1012,\n",
       "  5854,\n",
       "  18598,\n",
       "  2320,\n",
       "  3113,\n",
       "  3523,\n",
       "  2051,\n",
       "  22,\n",
       "  24,\n",
       "  2871,\n",
       "  21,\n",
       "  29,\n",
       "  29,\n",
       "  21,\n",
       "  11,\n",
       "  1960,\n",
       "  3185,\n",
       "  6037,\n",
       "  23164,\n",
       "  11,\n",
       "  1964,\n",
       "  2403,\n",
       "  1941,\n",
       "  6221,\n",
       "  5109,\n",
       "  4443,\n",
       "  18,\n",
       "  5941,\n",
       "  2040,\n",
       "  16,\n",
       "  25512,\n",
       "  24006,\n",
       "  16,\n",
       "  7796,\n",
       "  11,\n",
       "  2151,\n",
       "  1941,\n",
       "  3934,\n",
       "  2013,\n",
       "  6896,\n",
       "  1933,\n",
       "  8729,\n",
       "  2023,\n",
       "  6349,\n",
       "  1941,\n",
       "  3166,\n",
       "  2013,\n",
       "  16391,\n",
       "  13154,\n",
       "  28886,\n",
       "  1941,\n",
       "  9914,\n",
       "  2023,\n",
       "  11766,\n",
       "  1941,\n",
       "  12406,\n",
       "  9762,\n",
       "  1937,\n",
       "  18,\n",
       "  2850,\n",
       "  21,\n",
       "  29,\n",
       "  29,\n",
       "  21,\n",
       "  11,\n",
       "  1960,\n",
       "  13216,\n",
       "  1027,\n",
       "  18970,\n",
       "  2098,\n",
       "  6,\n",
       "  24006,\n",
       "  1924,\n",
       "  7327,\n",
       "  17,\n",
       "  8786,\n",
       "  11,\n",
       "  5068,\n",
       "  17847,\n",
       "  17429,\n",
       "  1008,\n",
       "  9012,\n",
       "  19825,\n",
       "  22068,\n",
       "  2303,\n",
       "  1933,\n",
       "  22123,\n",
       "  2398,\n",
       "  6345,\n",
       "  6,\n",
       "  10011,\n",
       "  2236,\n",
       "  5265,\n",
       "  1941,\n",
       "  5848,\n",
       "  17176,\n",
       "  18,\n",
       "  1989,\n",
       "  5848,\n",
       "  16,\n",
       "  2920,\n",
       "  6,\n",
       "  8275,\n",
       "  11664,\n",
       "  6,\n",
       "  6002,\n",
       "  11434,\n",
       "  5819,\n",
       "  1937,\n",
       "  1933,\n",
       "  5819,\n",
       "  5383,\n",
       "  14532,\n",
       "  17527,\n",
       "  5817,\n",
       "  1933,\n",
       "  6889,\n",
       "  2699,\n",
       "  3842,\n",
       "  2023,\n",
       "  19828,\n",
       "  2625,\n",
       "  2065,\n",
       "  9469,\n",
       "  24006,\n",
       "  1012,\n",
       "  10910,\n",
       "  2454,\n",
       "  3131,\n",
       "  4474,\n",
       "  16,\n",
       "  10026,\n",
       "  2314,\n",
       "  10697,\n",
       "  2347,\n",
       "  21019,\n",
       "  2051,\n",
       "  2935,\n",
       "  1941,\n",
       "  2991,\n",
       "  1959],\n",
       " 'word_ids': [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  13,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  33,\n",
       "  34,\n",
       "  34,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  52,\n",
       "  53,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  129,\n",
       "  129,\n",
       "  130,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  160,\n",
       "  161,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  190,\n",
       "  191,\n",
       "  191,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  207,\n",
       "  208,\n",
       "  208,\n",
       "  208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  224,\n",
       "  225,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  230,\n",
       "  231,\n",
       "  232,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  250,\n",
       "  251,\n",
       "  251,\n",
       "  252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273,\n",
       "  274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  284,\n",
       "  284,\n",
       "  285,\n",
       "  286,\n",
       "  286,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  294,\n",
       "  295,\n",
       "  295,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300,\n",
       "  301,\n",
       "  302,\n",
       "  303,\n",
       "  304,\n",
       "  305,\n",
       "  306,\n",
       "  307,\n",
       "  308,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  311,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  315,\n",
       "  316,\n",
       "  317,\n",
       "  318,\n",
       "  319,\n",
       "  320,\n",
       "  321,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  347,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  356,\n",
       "  357,\n",
       "  358,\n",
       "  359,\n",
       "  360,\n",
       "  360,\n",
       "  361,\n",
       "  362,\n",
       "  363,\n",
       "  364,\n",
       "  365,\n",
       "  366,\n",
       "  366,\n",
       "  366,\n",
       "  367,\n",
       "  368,\n",
       "  369,\n",
       "  369,\n",
       "  370,\n",
       "  371,\n",
       "  371,\n",
       "  372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  400,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  404,\n",
       "  405,\n",
       "  406,\n",
       "  407,\n",
       "  407,\n",
       "  408,\n",
       "  408,\n",
       "  409,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  412,\n",
       "  413,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  417,\n",
       "  418,\n",
       "  419,\n",
       "  419],\n",
       " 'sent_idx': [51,\n",
       "  62,\n",
       "  107,\n",
       "  150,\n",
       "  178,\n",
       "  207,\n",
       "  228,\n",
       "  246,\n",
       "  302,\n",
       "  324,\n",
       "  375,\n",
       "  407,\n",
       "  445,\n",
       "  489]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_sent_idx(one_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenize_and_sent_idx(one_doc)[\"token_ids\"][178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490, 490)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenize_and_sent_idx(one_doc)[\"word_ids\"]), tokenize_and_sent_idx(one_doc)[\"sent_idx\"][-1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55, 1, 2, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "[55] + a[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_one_doc_df = pd.concat([one_doc.to_frame().T, one_doc.to_frame().T, one_doc.to_frame().T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>token_ids</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>sent_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sırp Demokratik Partisi (Sırpça: Српска демокр...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sırp Demokratik Partisi (Sırpça: Српска демокр...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sırp Demokratik Partisi (Sırpça: Српска демокр...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  doc token_ids word_ids  \\\n",
       "13  Sırp Demokratik Partisi (Sırpça: Српска демокр...      None     None   \n",
       "13  Sırp Demokratik Partisi (Sırpça: Српска демокр...      None     None   \n",
       "13  Sırp Demokratik Partisi (Sırpça: Српска демокр...      None     None   \n",
       "\n",
       "   sent_idx  \n",
       "13     None  \n",
       "13     None  \n",
       "13     None  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_one_doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc': ['Sırp Demokratik Partisi (Sırpça: Српска демократска странка/\"Srpska demokratska stranka\", СДС/SDS) Bosna-Hersek\\'te bir Sırp siyasi partisidir. Şu anki lideri Mirko Šarović\\'tir.\\nEkim 2006\\'daki parlamento seçimlerinde SDP, Sırp Cumhuriyeti Başbakanı Milorad Dodik liderliğindeki Bağımsız Sosyal Demokratlar İttifakı\\'na, Sırp Cumhuriyeti\\'nin önde gelen partisi ve Bosna Hersek\\'teki ana Sırp partisi konumunu kaybetti. 2010 ve 2014 seçimlerinde küçük kazanımlar elde etmesine rağmen, 2018 yılına gelindiğinde partinin parlamentoda temsil oranı %20\\'nin altına düştü ve tarihindeki en düşük sandalye sayısına sahip oldu. Sırp Demokratik Partisi, \"savaş suçu zanlılarını tutuklayıp uluslararası bir mahkemeye teslim edemediği\" gerekçesiyle ABD\\'nin yaptırımı altında bulunuyor. Yaptırımlar, Amerika Birleşik Devletleri\\'nden SDP\\'ye herhangi bir fon ve materyal transferini yasaklar ve bunun tersi durumu için de geçerlidir. Parti, Yabancı Varlıkları Kontrol Ofisi ABD ajansı tarafından Özel Olarak Belirlenmiş Vatandaşlar ve Engellenen Kişiler listesinde yer almaktadır.\\nRadovan Karadžić 1990\\'da Sırp Demokratik Partisi\\'ni kurdu. Parti, Jovan Rašković\\'in Sırp Demokratik Partisi\\'nin Hırvatistan\\'daki Sırplarla yaptığı gibi Bosnalı Sırp toplumunu birleştirmeyi ve bu iki cumhuriyetin federasyondan ayrılması durumunda Yugoslavya\\'nın (Sırbistan-Karadağ ile \"Üçüncü Yugoslavya\" olarak) bir parçası olarak kalmayı hedefliyordu.\\nEylül 1991 boyunca SDP, Bosna-Hersek genelinde çeşitli \"Sırp Özerk Bölgeleri\" kurmaya başladı. 15 Ekim 1991\\'de Bosna parlamentosunun egemenlik oylamasından sonra, Bosna-Hersek\\'teki Sırpları münhasıran temsil etmek için 24 Ekim 1991\\'de Banja Luka\\'da ayrı bir Sırp Meclisi kuruldu. Ertesi ay, Bosnalı Sırplar, Yugoslavya\\'nın bir parçası olarak Sırbistan ve Karadağ ile federal bir eyalet olarak kalmak lehine ezici bir oylama ile sonuçlanan bir referandum düzenlediler. Aralık 1991\\'de SDP liderliği tarafından \"Sırpların Bosna-Hersek\\'teki organlarının olağanüstü koşullarda örgütlenmesi ve faaliyetine ilişkin\" başlıklı çok gizli bir belge hazırlandı. Bu belge, çeşitli \"kriz merkezleri\" aracılığıyla gölge hükümetler ve hükümet dışı yapıların oluşturulması yoluyla ve Yugoslav Halk Ordusu ile koordinasyon içinde olan sadık Sırpları devralmaya hazırlayarak, ülkedeki her belediyenin ele geçirilmesi için merkezi bir programdı', 'Sırp Demokratik Partisi (Sırpça: Српска демократска странка/\"Srpska demokratska stranka\", СДС/SDS) Bosna-Hersek\\'te bir Sırp siyasi partisidir. Şu anki lideri Mirko Šarović\\'tir.\\nEkim 2006\\'daki parlamento seçimlerinde SDP, Sırp Cumhuriyeti Başbakanı Milorad Dodik liderliğindeki Bağımsız Sosyal Demokratlar İttifakı\\'na, Sırp Cumhuriyeti\\'nin önde gelen partisi ve Bosna Hersek\\'teki ana Sırp partisi konumunu kaybetti. 2010 ve 2014 seçimlerinde küçük kazanımlar elde etmesine rağmen, 2018 yılına gelindiğinde partinin parlamentoda temsil oranı %20\\'nin altına düştü ve tarihindeki en düşük sandalye sayısına sahip oldu. Sırp Demokratik Partisi, \"savaş suçu zanlılarını tutuklayıp uluslararası bir mahkemeye teslim edemediği\" gerekçesiyle ABD\\'nin yaptırımı altında bulunuyor. Yaptırımlar, Amerika Birleşik Devletleri\\'nden SDP\\'ye herhangi bir fon ve materyal transferini yasaklar ve bunun tersi durumu için de geçerlidir. Parti, Yabancı Varlıkları Kontrol Ofisi ABD ajansı tarafından Özel Olarak Belirlenmiş Vatandaşlar ve Engellenen Kişiler listesinde yer almaktadır.\\nRadovan Karadžić 1990\\'da Sırp Demokratik Partisi\\'ni kurdu. Parti, Jovan Rašković\\'in Sırp Demokratik Partisi\\'nin Hırvatistan\\'daki Sırplarla yaptığı gibi Bosnalı Sırp toplumunu birleştirmeyi ve bu iki cumhuriyetin federasyondan ayrılması durumunda Yugoslavya\\'nın (Sırbistan-Karadağ ile \"Üçüncü Yugoslavya\" olarak) bir parçası olarak kalmayı hedefliyordu.\\nEylül 1991 boyunca SDP, Bosna-Hersek genelinde çeşitli \"Sırp Özerk Bölgeleri\" kurmaya başladı. 15 Ekim 1991\\'de Bosna parlamentosunun egemenlik oylamasından sonra, Bosna-Hersek\\'teki Sırpları münhasıran temsil etmek için 24 Ekim 1991\\'de Banja Luka\\'da ayrı bir Sırp Meclisi kuruldu. Ertesi ay, Bosnalı Sırplar, Yugoslavya\\'nın bir parçası olarak Sırbistan ve Karadağ ile federal bir eyalet olarak kalmak lehine ezici bir oylama ile sonuçlanan bir referandum düzenlediler. Aralık 1991\\'de SDP liderliği tarafından \"Sırpların Bosna-Hersek\\'teki organlarının olağanüstü koşullarda örgütlenmesi ve faaliyetine ilişkin\" başlıklı çok gizli bir belge hazırlandı. Bu belge, çeşitli \"kriz merkezleri\" aracılığıyla gölge hükümetler ve hükümet dışı yapıların oluşturulması yoluyla ve Yugoslav Halk Ordusu ile koordinasyon içinde olan sadık Sırpları devralmaya hazırlayarak, ülkedeki her belediyenin ele geçirilmesi için merkezi bir programdı', 'Sırp Demokratik Partisi (Sırpça: Српска демократска странка/\"Srpska demokratska stranka\", СДС/SDS) Bosna-Hersek\\'te bir Sırp siyasi partisidir. Şu anki lideri Mirko Šarović\\'tir.\\nEkim 2006\\'daki parlamento seçimlerinde SDP, Sırp Cumhuriyeti Başbakanı Milorad Dodik liderliğindeki Bağımsız Sosyal Demokratlar İttifakı\\'na, Sırp Cumhuriyeti\\'nin önde gelen partisi ve Bosna Hersek\\'teki ana Sırp partisi konumunu kaybetti. 2010 ve 2014 seçimlerinde küçük kazanımlar elde etmesine rağmen, 2018 yılına gelindiğinde partinin parlamentoda temsil oranı %20\\'nin altına düştü ve tarihindeki en düşük sandalye sayısına sahip oldu. Sırp Demokratik Partisi, \"savaş suçu zanlılarını tutuklayıp uluslararası bir mahkemeye teslim edemediği\" gerekçesiyle ABD\\'nin yaptırımı altında bulunuyor. Yaptırımlar, Amerika Birleşik Devletleri\\'nden SDP\\'ye herhangi bir fon ve materyal transferini yasaklar ve bunun tersi durumu için de geçerlidir. Parti, Yabancı Varlıkları Kontrol Ofisi ABD ajansı tarafından Özel Olarak Belirlenmiş Vatandaşlar ve Engellenen Kişiler listesinde yer almaktadır.\\nRadovan Karadžić 1990\\'da Sırp Demokratik Partisi\\'ni kurdu. Parti, Jovan Rašković\\'in Sırp Demokratik Partisi\\'nin Hırvatistan\\'daki Sırplarla yaptığı gibi Bosnalı Sırp toplumunu birleştirmeyi ve bu iki cumhuriyetin federasyondan ayrılması durumunda Yugoslavya\\'nın (Sırbistan-Karadağ ile \"Üçüncü Yugoslavya\" olarak) bir parçası olarak kalmayı hedefliyordu.\\nEylül 1991 boyunca SDP, Bosna-Hersek genelinde çeşitli \"Sırp Özerk Bölgeleri\" kurmaya başladı. 15 Ekim 1991\\'de Bosna parlamentosunun egemenlik oylamasından sonra, Bosna-Hersek\\'teki Sırpları münhasıran temsil etmek için 24 Ekim 1991\\'de Banja Luka\\'da ayrı bir Sırp Meclisi kuruldu. Ertesi ay, Bosnalı Sırplar, Yugoslavya\\'nın bir parçası olarak Sırbistan ve Karadağ ile federal bir eyalet olarak kalmak lehine ezici bir oylama ile sonuçlanan bir referandum düzenlediler. Aralık 1991\\'de SDP liderliği tarafından \"Sırpların Bosna-Hersek\\'teki organlarının olağanüstü koşullarda örgütlenmesi ve faaliyetine ilişkin\" başlıklı çok gizli bir belge hazırlandı. Bu belge, çeşitli \"kriz merkezleri\" aracılığıyla gölge hükümetler ve hükümet dışı yapıların oluşturulması yoluyla ve Yugoslav Halk Ordusu ile koordinasyon içinde olan sadık Sırpları devralmaya hazırlayarak, ülkedeki her belediyenin ele geçirilmesi için merkezi bir programdı'], 'token_ids': [None, None, None], 'word_ids': [None, None, None], 'sent_idx': [None, None, None]}\n"
     ]
    }
   ],
   "source": [
    "print(three_one_doc_df.to_dict(\"list\"))\n",
    "three_one_doc_dict = three_one_doc_df.to_dict(\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_random_sample(docs: pd.DataFrame | dict[str, list], len_of_random_b):\n",
    "    \n",
    "    random_b_dict = {\"token_ids\":[], \"word_ids\":[]}\n",
    "\n",
    "    if len_of_random_b <= 0:\n",
    "        raise ValueError(f\"[ERROR] Len of number cannot be less than or equal to zero! {len_of_random_b} \")\n",
    "\n",
    "    # rasgele tüm doclara gidilecek (without replacement şekilde)\n",
    "    for rand_doc_idx in random.sample(range(len(docs[\"doc\"])), k=len(docs[\"doc\"])):\n",
    "        # en sonki nokta/sent_idx gereksiz: doc sonu random sample penceremin başı olamayacağına göre\n",
    "        # diğer durumların aksine bu sefer doc başı kullanışlı olabilir: random sample pencerem doc başından başlayabilir\n",
    "        doc_sent_idx_list = [0] + docs[\"sent_idx\"][rand_doc_idx][:-1]\n",
    "\n",
    "        # rasgele tüm noktalara/sent_idx'lere gidilecek (without replacement şekilde)\n",
    "        for rand_sent_start_idx in random.sample(doc_sent_idx_list, k=len(doc_sent_idx_list)):\n",
    "            # rand_sent_start_idx + 1' deki +1 : nokta/sent_idx tokeni dahil etmemeliyiz B'ye, A sonunda hali hazırda nokta/sent_idx tokeni var\n",
    "            # yoksa ab cümlelerinde A ve B arasında 2 tane nokta görürüz diye basitçe özetleyebiliriz\n",
    "            if len(docs[\"token_ids\"][rand_doc_idx][rand_sent_start_idx + 1:]) >= len_of_random_b:\n",
    "                rand_sent_end_idx = rand_sent_start_idx + len_of_random_b\n",
    "                random_b_dict[\"token_ids\"] = docs[\"token_ids\"][rand_doc_idx][rand_sent_start_idx + 1:rand_sent_end_idx + 1]\n",
    "                random_b_dict[\"word_ids\"] = docs[\"word_ids\"][rand_doc_idx][rand_sent_start_idx + 1:rand_sent_end_idx + 1]\n",
    "                return random_b_dict\n",
    "\n",
    "\n",
    "    print(f\"[INFO] Random sample couldn't be found in this shard interestingly...\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def _update_random_word_id(random_b_word_ids:List, last_word_id_of_A:int):\n",
    "    # return updated_random_b_word_ids\n",
    "    # 32  88\n",
    "    temp = np.array(random_b_word_ids)\n",
    "    \n",
    "    if random_b_word_ids[0] > last_word_id_of_A:\n",
    "        random_b_word_ids = temp - (random_b_word_ids[0] - last_word_id_of_A - 1)\n",
    "    else:\n",
    "        random_b_word_ids = temp + (last_word_id_of_A - random_b_word_ids[0] + 1)\n",
    "    return random_b_word_ids.tolist()\n",
    "\n",
    "\n",
    "def _fill_ab_from_block(ab_dict, doc, block_start_idx, mid_sent_idx, block_end_idx):\n",
    "    ab_dict[\"A_token_ids\"].append(doc[\"token_ids\"][block_start_idx:mid_sent_idx])\n",
    "    ab_dict[\"A_word_ids\"].append(doc[\"word_ids\"][block_start_idx:mid_sent_idx])\n",
    "    ab_dict[\"B_token_ids\"].append(doc[\"token_ids\"][mid_sent_idx:block_end_idx])\n",
    "    ab_dict[\"B_word_ids\"].append(doc[\"word_ids\"][mid_sent_idx:block_end_idx])\n",
    "    ab_dict[\"isNext\"].append(True)\n",
    "\n",
    "\n",
    "def _fill_a_from_block_b_from_random(ab_dict, doc, docs, block_start_idx, mid_sent_idx, len_of_random_b):\n",
    "    random_b = _get_random_sample(docs, len_of_random_b)\n",
    "    if random_b is not None:\n",
    "        ab_dict[\"A_token_ids\"].append(doc[\"token_ids\"][block_start_idx:mid_sent_idx])\n",
    "        ab_dict[\"A_word_ids\"].append(doc[\"word_ids\"][block_start_idx:mid_sent_idx])\n",
    "        ab_dict[\"B_token_ids\"].append(random_b[\"token_ids\"][:])\n",
    "        ab_dict[\"B_word_ids\"].append(_update_random_word_id(random_b[\"word_ids\"][:], ab_dict[\"A_word_ids\"][-1][-1])) # updt_rnd_b(list, int)\n",
    "        ab_dict[\"isNext\"].append(False)\n",
    "\n",
    "    # eğer random sample \"bir şekilde\" bulunamadıysa (çok zor bir ihtimal) hiç birşey yapma\n",
    "\n",
    "\n",
    "def convert_doc_to_ab(docs: pd.DataFrame | dict[str, list], doc_idx: int, block_size: int = BLOCK_SIZE, \n",
    "               overlap: int = OVERLAP, tampon:int = TAMPON)-> dict[str, list]:\n",
    "    \"\"\"doc(pd.Series): doc, token_ids, word_ids, sent_idx\"\"\"\n",
    "\n",
    "    doc = docs.iloc[doc_idx]\n",
    "\n",
    "    block_size_raw = block_size - 3     # del special tokens (cls, sep, sep)\n",
    "    block_start_idx = 0\n",
    "    block_end_idx = block_size_raw\n",
    "\n",
    "    ab_dict = {\"A_token_ids\":[], \"B_token_ids\":[], \"A_word_ids\":[], \"B_word_ids\":[], \"isNext\": []}\n",
    "\n",
    "\n",
    "    doc_len = len(doc[\"token_ids\"])\n",
    "    while block_end_idx <= (doc_len):\n",
    "\n",
    "        # block içerisindeki sent indexleri al\n",
    "        block_sent_idx = [sent_idx for sent_idx in doc[\"sent_idx\"] if (block_start_idx + tampon) < sent_idx and (block_end_idx - tampon) > sent_idx]\n",
    "        number_of_sent = len(block_sent_idx)\n",
    "\n",
    "\n",
    "        # block içerisinde sent idx yok\n",
    "        if number_of_sent == 0:\n",
    "\n",
    "            # block konumu güncelle\n",
    "            block_start_idx += overlap\n",
    "            block_end_idx += overlap\n",
    "\n",
    "            # bir sonraki iterasyona atla\n",
    "            continue\n",
    "\n",
    "         # tek sayı olduğunda\n",
    "        if number_of_sent % 2 != 0:\n",
    "            # +1 nedeni: nokta/sent_idx A'nın içinde olsun\n",
    "            mid_sent_idx = block_sent_idx[number_of_sent // 2] + 1  \n",
    "        # çift sayı olduğunda\n",
    "        else:\n",
    "            # a ve b noktalarından hangisi blocksize ortasına yakın ise o nokta (sent idx) mid_sent_idx olarak alınacak\n",
    "            a = number_of_sent // 2\n",
    "            b = a - 1\n",
    "            # +1 nedeni: nokta/sent_idx A'nın içinde olsun\n",
    "            # unutma: mid_sent_idx, nokta/sent_idx token'ını indexler! ve bu token A'ya dahil olacak (fill fonk'larda görebilirsin)\n",
    "            mid_sent_idx = (block_sent_idx[a] + 1) if abs(block_sent_idx[a] - block_size_raw // 2) < abs(block_sent_idx[b] - block_size_raw // 2) else (block_sent_idx[b] + 1)\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            # + 1 şurdan geldi: A nokta/sentidx token'ı da alacağından 1 birim sağdan kaydırdık\n",
    "            len_of_random_b = block_size_raw - (mid_sent_idx - block_start_idx)\n",
    "\n",
    "            _fill_a_from_block_b_from_random(ab_dict, doc, docs, block_start_idx, mid_sent_idx, len_of_random_b)\n",
    "\n",
    "            # block konum güncellemesi yapılmadığına dikkat\n",
    "            continue \n",
    "        \n",
    "\n",
    "        _fill_ab_from_block(ab_dict, doc, block_start_idx, mid_sent_idx, block_end_idx)\n",
    "\n",
    "        block_start_idx += overlap\n",
    "        block_end_idx += overlap\n",
    "    \n",
    "    # BURASI AÇIK OLDUĞUNDA FALSE SAYISI ~3000 DAHA FAZLA OLUYOR (DOC SAYISI KADAR). CLASS IMBALANCE'A SEBEP OLABİLİR. GERÇİ FOCAL LOSS KULLANABİLİRİZ...\n",
    "    # if (block_start_idx + tampon) < doc_len:\n",
    "    #     len_of_random_b = block_end_idx - doc_len  # A midpoint yani nokta/sent_idx tokenınıda içine alacağından bir sağ shiftledik\n",
    "    #     # random b için istenen token sayısı tampondan az ise o bu ab sample adayı kullanılmayacak (B'de çok çok az sayıda token var demektir)\n",
    "    #     if len_of_random_b < tampon:\n",
    "    #        return ab_dict\n",
    "    #     _fill_a_from_block_b_from_random(ab_dict, doc, docs, block_start_idx, doc_len, len_of_random_b)\n",
    "\n",
    "    return ab_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. ekim'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode((18, 2871))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GÜNCELLEME: doc sınır aşıldığında aşılan miktar kadar pad token demiştik ama buna gerek yok, kalan yer random sample ile doldurulsun. (doc sonu zaten bir cümle sonu gibi yani ayrıca bir problem yok)\n",
    "\n",
    "* GÜNCELLEME2: ana sistem çalışmadan önce tek sentence'a sahip tüm doclarda o sentence A olsun B için random yapılsın\n",
    "\n",
    "* UYARI: nokta derken sent idx, sent idx derkende aslında 0'ı da dahil etmeyi unutma!!! 0: doc başlangıcı, noktalar: cümle araları veya doc bitişi\n",
    "\n",
    "* UYARI: block_size - 3 'ü yani special tokenlar için yer unutma, isNext'i unutma\n",
    "\n",
    "* UYARI: block sınırlarında tampon olmalı: sent idx'ler yani noktalar  \n",
    "\n",
    "* HATIRLATMA: random B alındıktan sonra B nin word id'lerini update etmeyi unutma\n",
    "\n",
    "* sent idx ile + doc tokenlaştırılması alttaki adımlardan ayrı ve önce yapılmalı (öbür türlü random sample zamanı gelen doc'u tokenize etmek durumunda kalırız ve o doc kullanılamadan (ihtiyaçları karşılamadığı için) başka bir doca gene rasgele atlanabilir)\n",
    "\n",
    "* doc boyunca block hareket edecek (haraket deltası : overlap ile, block pencere genişliği indexleri: start/end_idx'ler ile)\n",
    "\n",
    "* block penceresi doc sınırı dışına çıktığı an bu iş biter, döngüden çıkılır(tam üst üste çakışmada gene klasik ne yapıyorsa onu yapmalı ayrı bir şey yok)\n",
    "\n",
    "* block penceresi doc sınırını aşıp çıktığında pencere indexleri güncellenecek (bunların farkı 1 ise (çok zor bir ihtimal eğer overlap parametresi block_size -1 kadar ise böyle bir ihtimal var genede dikkat edeceğim) komple iş bitti)\n",
    "\n",
    "* güncellenen block penceresi aslında gene aynı işlemleri uygulayacak (random sample olayı yapılmamalı yanlız! döngü içinde olsun kısaca) olaylar eğer fonksiyonlaştırılırsa temiz şekilde halledilir. (func call overheadi bilmiyorum ne olur ne biter), kalan token yeri kadar PAD tokeni eklemlenmeli!\n",
    "\n",
    "* block penceresi içerisinde 2 cümle yok ise (yani 1 tane sent index bile yoksa) bir sonraki it'e atla. rasgele B alınacağı durumda iterasyon donmalı (pencere start/endleri     güncellemeden bir sonraki iterasyona jumpla). \n",
    "\n",
    "* pencere içerisindeki kaç tane nokta varsa (sent index) ortasını seç\n",
    "\n",
    "* seçilen orta noktanın solu A, sağı B olacak (token id'ler, word idlerde slicelanacak)\n",
    "\n",
    "* sent indexlerde 0'ı sayma, yani tek bir cümlelik metin tek bir sent idx var \n",
    "\n",
    "* doc'ta sadece tek bir cümle varsa çık. \n",
    "\n",
    "* rasgele durumda: bulunulan pencerede nokta yoksa bir sonraki it'e atla(buna gerek yok zaten normalde de başta bu kontrol edilip bir sonraki it'e gidiliyor), block size - pencere içinde \"orta\" nokta uzunluğu = aranacak genişlik. Rasgele bir doca gidilecek bu doc genişliği bundan az ise başka random bir doc'a git. Genişliği sağlayan random doc bulunduğunda Random doc içerisindeki noktlara kümesinden rasgele nokta seç \"without replacement!\" o noktan tüm sağın genişliğine bak, bizi memnun ediyorsa onu al (token id, word id) etmiyorsa tekrar random nokta seç (ama tekrar ediyorum \"without raplacement!\" yani aynı noktayı birdaha seçememeliyim...) UYARI: nokta derken sent idx, sent idx derkende aslında 0'ı da dahil etmeyi unutma!!! 0: doc başlangıcı, noktalar: cümle araları veya doc bitişi\n",
    "\n",
    "* PAD token olayı yanlızce doc sınırından çıkıldığında var, random sample'da YOK (aranan genişliği tatmin edecek yani bloğu komple dolduracak random sample arıyoruz da ondan dolayı)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "her process dinamik şekilde list[dict] genişletip döndürecek (manager ile)\n",
    "\n",
    "lookup table olaylarında (ayrıca statik size) shared memory kullanılacak (daha hızlı, senkronizasyon sistemi ve dolaylı olarak processler arası haberleşme overhead'i olmayacak. her proc tak diye istediğine erişecek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE2 MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_shard(shard_dir, shard_idx, return_type=\"pd\"):\n",
    "\n",
    "    if shard_dir.startswith(\"doc\"):\n",
    "        prefix = \"doc\"\n",
    "    elif shard_dir.startswith(\"ab\"):\n",
    "        prefix = \"ab\"\n",
    "\n",
    "    if shard_idx >= 0 and shard_idx < len(os.listdir(prefix + \"_shards\")):\n",
    "        \n",
    "        temp = pd.read_json(shard_dir + prefix + f\"_shard_{shard_idx}.json\",\n",
    "                            orient=\"records\",\n",
    "                            lines=True,\n",
    "                            encoding=\"utf-8\")\n",
    "        \n",
    "        if return_type == \"pd\":\n",
    "            return temp\n",
    "\n",
    "        elif return_type == \"dict\":\n",
    "            return temp.to_dict(\"list\")\n",
    "\n",
    "        else:\n",
    "            print(f\"[ERROR] Unvalid input for return type, must be 'pd' or 'dict'...\")\n",
    "\n",
    "    else:\n",
    "        raise IndexError(f\"shard_idx >= 0 and shard_idx < {len(os.listdir('doc_shards'))}, shard_idx you gave was: {shard_idx}\")\n",
    "\n",
    "def create_doc_shards(docs_df):\n",
    "\n",
    "    global NUM_SHARDS_FOR_DOC\n",
    "    global NUM_DOCS_PER_SHARD\n",
    "\n",
    "    NUM_DOCS_PER_SHARD = len(docs_df) // NUM_SHARDS_FOR_DOC\n",
    "\n",
    "\n",
    "    if len(docs_df) % NUM_SHARDS_FOR_DOC != 0:\n",
    "        extra_shard_len = len(docs_df) % NUM_SHARDS_FOR_DOC\n",
    "        NUM_SHARDS_FOR_DOC += 1\n",
    "        \n",
    "\n",
    "    os.makedirs(\"doc_shards\", exist_ok=True)\n",
    "    files = os.listdir(\"./doc_shards\")\n",
    "\n",
    "\n",
    "    if len(files) == NUM_SHARDS_FOR_DOC:\n",
    "        print(f\"[INFO] Expected number of shards are already created. Exiting...\")\n",
    "        return\n",
    "    \n",
    "    # fazlalık dosyaları (eğer olursa) sil (evet biraz brute force)\n",
    "    if NUM_SHARDS_FOR_DOC < len(files):\n",
    "\n",
    "        files_finded = set(files)\n",
    "        shard_files_expected = {f\"doc_shard_{shard_idx}.json\" for shard_idx in range(NUM_SHARDS_FOR_DOC)}\n",
    "        files_will_deleted = list(files_finded - shard_files_expected)\n",
    "        if files_will_deleted:\n",
    "            print(f\"[INFO] These files are unexpected, so they are gonna be deleted: {files_will_deleted}\")\n",
    "            for file in files:\n",
    "                os.remove(\"./doc_shards/\" + file)\n",
    "\n",
    "\n",
    "    print(f\"[INFO] Total number of doc: {len(docs_df)}\")\n",
    "    print(f\"[INFO] Each shard will have {NUM_DOCS_PER_SHARD} number of docs...\")\n",
    "    print(f\"[INFO] Number of shard will be: {NUM_SHARDS_FOR_DOC}\")\n",
    "    if extra_shard_len:\n",
    "        print(f\"[INFO] Extra shard needed. Number of doc will be placed: {extra_shard_len}\")\n",
    "\n",
    "    \n",
    "    df_start_idx = 0\n",
    "    df_end_idx = NUM_DOCS_PER_SHARD\n",
    "\n",
    "\n",
    "    with tqdm.tqdm(total=NUM_SHARDS_FOR_DOC, desc=\"Doc shards are being created\") as pbar:\n",
    "        for shard_idx in range(0, NUM_SHARDS_FOR_DOC):\n",
    "            # last iteration (extra shard)\n",
    "            if shard_idx == NUM_SHARDS_FOR_DOC - 1:\n",
    "                df_end_idx = df_start_idx + extra_shard_len\n",
    "\n",
    "                assert df_end_idx == len(docs_df)\n",
    "                assert (df_end_idx - df_start_idx) == extra_shard_len\n",
    "\n",
    "                docs_df.iloc[df_start_idx:df_end_idx].to_json(\"./doc_shards/\" + f\"doc_shard_{shard_idx}.json\",\n",
    "                                                          orient=\"records\",\n",
    "                                                          lines=True,\n",
    "                                                          force_ascii=False)\n",
    "                pbar.update()\n",
    "                break\n",
    "\n",
    "            docs_df.iloc[df_start_idx:df_end_idx].to_json(\"./doc_shards/\" + f\"doc_shard_{shard_idx}.json\",\n",
    "                                                          orient=\"records\",\n",
    "                                                          lines=True,\n",
    "                                                          force_ascii=False)\n",
    "            df_start_idx = df_end_idx\n",
    "            df_end_idx += NUM_DOCS_PER_SHARD\n",
    "            pbar.update()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"doc_shards\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afrika ada ülkesi São Tomé ve Príncipe kendi i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1971'de kurulan Statistics Canada (StatCan; Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red Bay, Kanada'daki Newfoundland ve Labrador ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Denzil Holles, \"Holles Baronu\" (31 Ekim 1599 -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beyaz Rusya Devlet Kontrol Komitesi (), Belaru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Carol Hanisch (d. 1942 Iowa ABD), Amerikalı ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Mutasarrıf (), Osmanlı idari sisteminde sancak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Eş görevli sözcük ya da sözcük gruplarının ara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>\"C'est la chanson de mon amour\" (), Véronique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>South Bound Brook, Amerika Birleşik Devletleri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  doc\n",
       "0   Afrika ada ülkesi São Tomé ve Príncipe kendi i...\n",
       "1   1971'de kurulan Statistics Canada (StatCan; Fr...\n",
       "2   Red Bay, Kanada'daki Newfoundland ve Labrador ...\n",
       "3   Denzil Holles, \"Holles Baronu\" (31 Ekim 1599 -...\n",
       "4   Beyaz Rusya Devlet Kontrol Komitesi (), Belaru...\n",
       "..                                                ...\n",
       "88  Carol Hanisch (d. 1942 Iowa ABD), Amerikalı ra...\n",
       "89  Mutasarrıf (), Osmanlı idari sisteminde sancak...\n",
       "90  Eş görevli sözcük ya da sözcük gruplarının ara...\n",
       "91  \"C'est la chanson de mon amour\" (), Véronique ...\n",
       "92  South Bound Brook, Amerika Birleşik Devletleri...\n",
       "\n",
       "[93 rows x 1 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_shard(\"doc_shards/\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ab(ab: pd.Series):\n",
    "    \"\"\"visualize one ab sample\"\"\"\n",
    "\n",
    "    print(f\"A: {ab['A_token_ids']}\")\n",
    "    print(f\"B: {ab['B_token_ids']}\")\n",
    "    print(f\"A_decoded: {tokenizer.decode(ab['A_token_ids'])}\")\n",
    "    print(f\"B_decoded: {tokenizer.decode(ab['B_token_ids'])}\")\n",
    "    print(f\"len_of_A: {len(ab['A_token_ids'])}\")\n",
    "    print(f\"len_of_B: {len(ab['B_token_ids'])}\")\n",
    "    print(f\"A_word_ids: {ab['A_word_ids']}\")\n",
    "    print(f\"B_word_ids: {ab['B_word_ids']}\")\n",
    "    print(f\"len_of_A_word_ids: {len(ab['A_word_ids'])}\")\n",
    "    print(f\"len_of_B_word_ids: {len(ab['B_word_ids'])}\")\n",
    "    print(f\"sum_of_AB_tokens: {len(ab['B_word_ids']) + len(ab['A_word_ids'])}\")\n",
    "    print(f\"isNext: {ab['isNext']}\")\n",
    "    \n",
    "    print(\"---------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ab_shards():\n",
    "\n",
    "    os.makedirs(\"ab_shards\", exist_ok=True)\n",
    "\n",
    "    for shard_idx in range(len(os.listdir(\"doc_shards\"))):\n",
    "        docs_shard_df = read_shard(\"doc_shards/\", shard_idx)\n",
    "    \n",
    "        docs_shard_df[\"token_ids\"] = None\n",
    "        docs_shard_df[\"word_ids\"] = None\n",
    "        docs_shard_df[\"sent_idx\"] = None\n",
    "        \n",
    "        for doc_idx in range(len(docs_df)):\n",
    "            docs_df.iloc[doc_idx] = tokenize_and_sent_idx(docs_df.iloc[doc_idx])\n",
    "\n",
    "\n",
    "        ab_df = pd.DataFrame({\"A_token_ids\":[], \"B_token_ids\":[], \"A_word_ids\":[], \"B_word_ids\":[], \"isNext\": []})\n",
    "\n",
    "        for doc_idx in range(len(docs_df)):\n",
    "            ab_df = pd.concat([pd.DataFrame(convert_doc_to_ab(docs_df, doc_idx)), ab_df])\n",
    "\n",
    "        # shuffle\n",
    "        ab_df = ab_df.sample(frac=1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         A         B\n",
      "0   [1, 2]    [3, 4]\n",
      "1   [5, 6]    [7, 8]\n",
      "2  [9, 10]  [11, 12]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dict objelerinden oluşan bir liste, değerleri listeler\n",
    "ax = [\n",
    "    {'A': [1, 2], 'B': [3, 4]},\n",
    "    {'A': [5, 6], 'B': [7, 8]},\n",
    "    {'A': [9, 10], 'B': [11, 12]}\n",
    "]\n",
    "\n",
    "# DataFrame'e çevirirken, her bir dict'i düzleştir\n",
    "df = pd.DataFrame(ax)\n",
    "\n",
    "# DataFrame'i yazdır\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data az olduğundan dolayı vocab arttırma iyi olabilir (ne kadar parçalar isek özellikle tr dili için)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3, 2, 1, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_TOKEN_ID, SEP_TOKEN_ID, CLS_TOKEN_ID, UNK_TOKEN_ID, MASK_TOKEN_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_row = read_shard(\"ab_shards_256/\", 100).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [20, 28, 4826, 2908, 3962, 6256, 7339, 16767, 28683, 2168, 2023, 1941, 3928, 5518, 6723, 18, 2424, 5644, 2539, 2914, 13250, 6089, 16, 4215, 7528, 1007, 6442, 1933, 30556, 8360, 4883, 12285, 11, 2028, 7182, 4774, 2410, 2799, 16, 18783, 16345, 4625, 7182, 2640, 1011, 2287, 16, 2126, 2702, 1011, 2914, 6717, 2552, 18, 1989, 2319, 18813, 16781, 2159, 5191, 10276, 1008, 2799, 3228, 5313, 3513, 17180, 2141, 14016, 1012, 14855, 6432, 18]\n",
      "B: [23, 20, 2900, 22, 20, 20, 29, 2432, 16767, 28683, 2168, 2098, 4122, 8569, 18, 22, 25, 2908, 22, 20, 20, 29, 2432, 2498, 3282, 4865, 21867, 11, 2760, 22, 15, 21, 15871, 13639, 4058, 18, 4865, 21867, 2023, 7044, 23, 25, 6838, 3474, 8028, 9321, 29, 16, 26, 23, 3019, 12983, 2124, 3637, 2464, 18, 22, 26, 2900, 22, 20, 21, 21, 2432, 3656, 12392, 2023, 1941, 3928, 5518, 6723, 18, 12392, 2023, 18326, 1005, 11, 1960, 3064, 13225, 7824, 28221, 16, 2397, 5273, 5178, 2079, 11357, 13561, 11, 41, 24, 17, 22, 2023, 14707, 7044, 2826, 8629, 18, 22, 20, 21, 22, 11, 1960, 14741, 14158, 25314, 2023, 5518, 6723, 16, 2397, 5313, 2011, 9489, 2056, 12559, 16229, 16, 3610, 4327, 3483, 7339, 10759, 11997, 23957, 11499, 9723, 11, 1962, 3773, 2075, 18, 2894, 22, 20, 21, 23, 11, 2072, 8616, 3988, 1013, 7339, 2211, 5232, 6037, 22372, 15785, 1011, 11, 7795, 10513, 1933, 3825, 3341, 31816, 18, 22, 20, 21, 23, 17, 21, 24, 3825, 4241, 6896, 4327, 3483, 7339, 13080, 14234, 11, 2870, 5518, 6723, 18]\n",
      "A_decoded: 0 8 yılının ağustos ayında nba takımlarından cleveland cavaliers ile bir yıllık sözleşme imzaladı. sezon başlarında az süre almasına karşın, delonte west ve sasha pavlović'in sakatlanmalarının ardından, orlando magic maçında sakatlanana kadar, daha faza süre almaya başladı. bu iki ismin sakatlıktan kurtulmalarının ardından tekrar takımın üçüncü şutör gardı konumuna düştü.\n",
      "B_decoded: 3 0 temmuz 2 0 0 9 tarihinde cleveland cavaliers tarafından serbest bırakıldı. 2 5 ağustos 2 0 0 9 tarihinde eski takımı fenerbahçe ülker'le 2 + 1 yıllığına anlaşmaya vardı. fenerbahçe ülker ile ligde 3 5 maça çıkan kinsey 9, 6 3 sayı ortalamasıyla mücadele etti. 2 6 temmuz 2 0 1 1 tarihinde anadolu efes ile bir yıllık sözleşme imzaladı. efes ile tbl'de final oynama başarısı gösterdiler, ancak finalde beşikaş milangaz'a 4 - 2 ile kaybederek ligde ikinci oldular. 2 0 1 2'de golden state warriors ile sözleşme imzaladı, ancak takımın son kadrosunda yer alamayınca, i̇talya basketbol ligi takımlarından victoria libertas pesaro'ya transfer oldu. nisan 2 0 1 3'te liga acb takımlarından unicaja málaga'yla anlaştı ve sezonu burada noktaladı. 2 0 1 3 - 1 4 sezonu başında sırbistan basketbol ligi takımlarından kk partizan'la sözleşme imzaladı.\n",
      "len_of_A: 73\n",
      "len_of_B: 180\n",
      "A_word_ids: [236, 237, 238, 239, 240, 241, 242, 243, 244, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 258, 258, 259, 260, 261, 262, 262, 262, 263, 264, 265, 265, 265, 266, 267, 268, 269, 270, 271, 271, 271, 272, 273, 274, 275, 275, 276, 277, 278, 279, 280, 281, 282, 283, 283, 284, 284, 284, 285, 286, 287, 288, 289, 289, 290, 290, 291, 292, 293]\n",
      "B_word_ids: [294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 337, 338, 339, 340, 341, 342, 343, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 375, 376, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 406, 407, 408, 409, 410, 411, 412, 413, 413, 414, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 428, 429, 430, 430, 430, 431, 431, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459]\n",
      "len_of_A_word_ids: 73\n",
      "len_of_B_word_ids: 180\n",
      "sum_of_AB_tokens: 253\n",
      "isNext: 1\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualize_sample(ab_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, a, b, c = gecicici(ab_row, return_selected=True)\n",
    "selected_groups = (a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_decoded: [CLS] giymiştir 8 yılının ağustos ayında nba takımlarından cleveland [MASK] [MASK] ile bir yıllık sözleşme imzaladı. sezon başlarında az süre almasına [MASK], delonte west ve sasha pavlović'in sakatlanmalarının ardından, orlando magic maçında sakatlanana kadar, daha faza süre almaya başladı. bu yazı etrafındaki sakatlıktan kurtulmalarının ardından tekrar takımın üçüncü şutör [MASK] [MASK] [MASK] düştü. 3 0 temmuz 2 0 0 9 tarihinde cleveland cavaliers tarafından serbest bırakıldı. 2 5 ağustos 2 0 0 9 tarihinde eski takımı fenerbahçe ülker'le 2 + 1 yıllığına anlaşmaya vardı. fenerbahçe ülker ile ligde [MASK] 5 maça çıkan [MASK] [MASK] 9, 6 3 sayı ortalamasıyla mücadele etti. 2 6 temmuz 2 0 1 1 tarihinde anadolu efes [MASK] bir [MASK] sözleşme imzaladı. efes ile tbl'de final oynama başarısı gösterdiler [MASK] ancak finalde beşikaş milangaz'a 4 - 2 ile kaybederek ligde [MASK] oldular. 2 0 1 2'de golden state warriors ile sözleşme imzaladı, ancak takımın son kadrosunda yer alamayınca, i̇talya basketbol ligi takımlarından victoria libertas pesaro'ya transfer [MASK]. [MASK] 2 0 1 [MASK] [MASK] te liga acb takımlarından unicaja málaga'yla amp ve sezonu burada noktaladı. 2 0 1 3 - 1 4 sezonu başında sırbistan basketbol ligi takımlarından tasarlanmıştır partizan'la [MASK] imzaladı.\n",
      "y_decoded: [UNK] 0 [PAD] [PAD] [PAD] [PAD] nba [PAD] [PAD] cavaliers [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] karşın [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD], [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]. [PAD] iki ismin [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] üçüncü [PAD] [PAD] gardı konumuna [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] 3 [PAD] [PAD] [PAD] kinsey [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] ile bir yıllık [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD], [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] ikinci [PAD] [PAD] [PAD] [PAD] [PAD] 2 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] ligi [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] oldu [PAD] nisan [PAD] [PAD] [PAD] 3'[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] anlaştı [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] 1 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] kk [PAD] [PAD] [PAD] sözleşme [PAD] [PAD]\n",
      "len_of_x: 254\n",
      "len_of_y: 254\n",
      "Masked words: ; ^ b h ­ á ý į ť ơ ǒ ɕ ɾ ̌ ̡\n",
      "Replaced words: ¬ ï ʉ ̇\n",
      "Identity words: - ± ł ́\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualize_sample(gecicici(ab_row), selected_groups=selected_groups, no_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Orijinal dizi\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Dizi dilimi oluştur\n",
    "sliced_arr = arr[1:4]\n",
    "\n",
    "# Orijinal diziyi sil\n",
    "del arr\n",
    "\n",
    "# Sliced diziye erişim\n",
    "try:\n",
    "    print(sliced_arr)  # Bu erişim hala geçerli ama veri kaybı olur\n",
    "except NameError:\n",
    "    print(\"Orijinal dizi silindi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "for i in range(len(arr) - 1, -1, -1):\n",
    "    print(arr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* kaç tane kelimem var?\n",
    "* kelimlelerden numpy array oluştur 0-1 arasında olasılık verdir\n",
    "* bu kelime olasılık array'inden kelimelerin tipini belirle (mask_kelimeleri, replace_kelimeleri, identiy_kelimeleri belirle):\n",
    "    * işte spesifik bir olaslık türündeki kelimeleri alıp (condition ile value return ettir index değil!) o kelime list/belkide array'ine eklemdir\n",
    "\n",
    "* her kelime grubu için kelimelerin kaç tokenlık old u belirlemen lazım (binary search left alg kullanılacak)\n",
    "    * döngü içerisinde spesifik kelime grubundaki her kelime için birleştirilmiş word_id array'inde binarysearchleft ile o kelimenin başlangıç indexini bul. sağa doğru farklı kelimeyle karşılaşana kadar bitiş indexini arttır. böylece kelime penceresi/slice oluştu. Buradan kaç token bilgiside geleceğinden kalan olayları sen anladın zaten anlatmıyorum.\n",
    "\n",
    "\n",
    "* ab shard'ları dataframe olarak yükleniyorlar\n",
    "* ab shardlarında row row (1 sample/example) hareket edilecek: pd.Series\n",
    "* y placeholder full pad token_id'li numpy array\n",
    "* x 'te numpy array olsun\n",
    "* başta A ve B nin uzunlukarını al (len fonksiyonu O(1)) sonra A ve B yi birleştir ki işlemlerin kolay olsun (Daha sonra ayırıp araya sep tokeni koyarız)\n",
    "\n",
    "* special tokenları, isNext'i vs unutma\n",
    "\n",
    "* veri setimde tüm blockları dolu tuttuğum için (pad tokenı içerilmiyor) bundan dolayı attention mask komple 1 olacak (model eğitiminde oluşturmak gereksiz işlem kaybı olur ondan burada oluşturalım)\n",
    "* segment_ids, numpy array şeklinde olacak; 0: A, 1:B idi sanırsam bir bakarım\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* shard dosya büyüklüğü model eğitiminde tatmin etmezse diye shard dosyalarını fuse'layan ayrı bir fonk yazabilirsin!.\n",
    "* Tokenizer'ı Cased olarak kullanmak daha iyi olabilir nsp açısından\n",
    "* FOCAL LOSS KULLANIRSAK EKSRA FALSE DATASIDA KULLANILABİLİR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build_and_train_bert_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
