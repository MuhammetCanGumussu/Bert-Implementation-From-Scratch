{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Denemeleri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Önce HuggingFace'ten, hali hazırda tamamen türkçe veriler üzerinde eğitilmiş BerTurk modeli denenecek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForPreTraining\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "model = BertForPreTraining.from_pretrained(\"dbmdz/bert-base-turkish-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight torch.Size([32000, 768])\n",
      "bert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.pooler.dense.weight torch.Size([768, 768])\n",
      "bert.pooler.dense.bias torch.Size([768])\n",
      "cls.predictions.bias torch.Size([32000])\n",
      "cls.predictions.transform.dense.weight torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.bias torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.weight torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.bias torch.Size([768])\n",
      "cls.predictions.decoder.weight torch.Size([32000, 768])\n",
      "cls.predictions.decoder.bias torch.Size([32000])\n",
      "cls.seq_relationship.weight torch.Size([2, 768])\n",
      "cls.seq_relationship.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "sd_hf = model.state_dict()\n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(sd_hf[\"bert.embeddings.word_embeddings.weight\"], sd_hf[\"cls.predictions.decoder.weight\"]) # tying kontrolü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_hf[\"bert.embeddings.word_embeddings.weight\"].data_ptr() == sd_hf[\"cls.predictions.decoder.weight\"].data_ptr()  # tying kontrolü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(sd_hf[\"cls.predictions.decoder.bias\"], sd_hf[\"cls.predictions.bias\"]) # tying kontrolü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_hf[\"cls.predictions.decoder.bias\"].data_ptr() == sd_hf[\"cls.predictions.bias\"].data_ptr()  # tying kontrolü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Selamlar efendim\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 32000]), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.prediction_logits.shape, outputs.seq_relationship_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05243540555238724,\n",
       "  'token': 3944,\n",
       "  'token_str': 'Mehmet',\n",
       "  'sequence': 'Merhaba Mehmet efendi nasıl gidiyor?'},\n",
       " {'score': 0.03593067452311516,\n",
       "  'token': 7709,\n",
       "  'token_str': 'hanım',\n",
       "  'sequence': 'Merhaba hanım efendi nasıl gidiyor?'},\n",
       " {'score': 0.03291669860482216,\n",
       "  'token': 3024,\n",
       "  'token_str': 'bey',\n",
       "  'sequence': 'Merhaba bey efendi nasıl gidiyor?'},\n",
       " {'score': 0.031684163957834244,\n",
       "  'token': 7983,\n",
       "  'token_str': 'hoca',\n",
       "  'sequence': 'Merhaba hoca efendi nasıl gidiyor?'},\n",
       " {'score': 0.030081957578659058,\n",
       "  'token': 10997,\n",
       "  'token_str': 'Salih',\n",
       "  'sequence': 'Merhaba Salih efendi nasıl gidiyor?'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_masker = pipeline(task=\"fill-mask\", model=\"dbmdz/bert-base-turkish-cased\")\n",
    "fill_masker(\"Merhaba [MASK] efendi nasıl gidiyor?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import BertForPreTraining, BertConfig\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = BertConfig()\n",
    "custom_model = BertForPreTraining(default_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight torch.Size([32000, 768]) -------- bert.embeddings.word_embeddings.weight torch.Size([32000, 768])\n",
      "bert.embeddings.position_embeddings.weight torch.Size([512, 768]) -------- bert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight torch.Size([2, 768]) -------- bert.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight torch.Size([768]) -------- bert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias torch.Size([768]) -------- bert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.pooler.dense.weight torch.Size([768, 768]) -------- bert.pooler.dense.weight torch.Size([768, 768])\n",
      "bert.pooler.dense.bias torch.Size([768]) -------- bert.pooler.dense.bias torch.Size([768])\n",
      "cls.predictions.bias torch.Size([32000]) -------- cls.predictions.bias torch.Size([32000])\n",
      "cls.predictions.transform.dense.weight torch.Size([768, 768]) -------- cls.predictions.transform.dense.weight torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.bias torch.Size([768]) -------- cls.predictions.transform.dense.bias torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.weight torch.Size([768]) -------- cls.predictions.transform.LayerNorm.weight torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.bias torch.Size([768]) -------- cls.predictions.transform.LayerNorm.bias torch.Size([768])\n",
      "cls.predictions.decoder.weight torch.Size([32000, 768]) -------- cls.predictions.decoder.weight torch.Size([32000, 768])\n",
      "cls.predictions.decoder.bias torch.Size([32000]) -------- cls.predictions.decoder.bias torch.Size([32000])\n",
      "cls.seq_relationship.weight torch.Size([2, 768]) -------- cls.seq_relationship.weight torch.Size([2, 768])\n",
      "cls.seq_relationship.bias torch.Size([2]) -------- cls.seq_relationship.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "custom_sd = custom_model.state_dict()\n",
    "\n",
    "assert len(custom_sd) == len(sd_hf)\n",
    "\n",
    "for custom, hf in zip(custom_sd.items(), sd_hf.items()):\n",
    "    k, v = custom\n",
    "    k_hf, v_hf = hf\n",
    "    assert k == k_hf, f\"{k} : {k_hf}\"\n",
    "    assert v.shape == v_hf.shape, f\"{k} : {v.shape} != {v_hf.shape}\"\n",
    "    print(k, v.shape, \"--------\", k_hf, v_hf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained bert: dbmdz/bert-base-turkish-cased\n"
     ]
    }
   ],
   "source": [
    "model_from_hf = BertForPreTraining.from_pretrained()\n",
    "mm = model_from_hf.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05243540555238724,\n",
       "  'token': 3944,\n",
       "  'token_str': 'Mehmet',\n",
       "  'sequence': 'Merhaba Mehmet efendi nasıl gidiyor?'},\n",
       " {'score': 0.03593067452311516,\n",
       "  'token': 7709,\n",
       "  'token_str': 'hanım',\n",
       "  'sequence': 'Merhaba hanım efendi nasıl gidiyor?'},\n",
       " {'score': 0.03291669860482216,\n",
       "  'token': 3024,\n",
       "  'token_str': 'bey',\n",
       "  'sequence': 'Merhaba bey efendi nasıl gidiyor?'},\n",
       " {'score': 0.031684163957834244,\n",
       "  'token': 7983,\n",
       "  'token_str': 'hoca',\n",
       "  'sequence': 'Merhaba hoca efendi nasıl gidiyor?'},\n",
       " {'score': 0.030081957578659058,\n",
       "  'token': 10997,\n",
       "  'token_str': 'Salih',\n",
       "  'sequence': 'Merhaba Salih efendi nasıl gidiyor?'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "fill_masker = pipeline(task=\"fill-mask\", model=\"dbmdz/bert-base-turkish-cased\")\n",
    "fill_masker(\"Merhaba [MASK] efendi nasıl gidiyor?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9745508432388306,\n",
       "  'token': 22777,\n",
       "  'token_str': 'Muhammet',\n",
       "  'sequence': 'ışınlanma teknolojisini Muhammet Can bulmuştur. Muhammet Can bu buluş ile büyük alkış topladı'},\n",
       " {'score': 0.016110757365822792,\n",
       "  'token': 7397,\n",
       "  'token_str': 'Muhammed',\n",
       "  'sequence': 'ışınlanma teknolojisini Muhammed Can bulmuştur. Muhammet Can bu buluş ile büyük alkış topladı'},\n",
       " {'score': 0.0018351072212681174,\n",
       "  'token': 3944,\n",
       "  'token_str': 'Mehmet',\n",
       "  'sequence': 'ışınlanma teknolojisini Mehmet Can bulmuştur. Muhammet Can bu buluş ile büyük alkış topladı'},\n",
       " {'score': 0.00045096001122146845,\n",
       "  'token': 6001,\n",
       "  'token_str': 'Hasan',\n",
       "  'sequence': 'ışınlanma teknolojisini Hasan Can bulmuştur. Muhammet Can bu buluş ile büyük alkış topladı'},\n",
       " {'score': 0.0004167473816778511,\n",
       "  'token': 6391,\n",
       "  'token_str': 'Cem',\n",
       "  'sequence': 'ışınlanma teknolojisini Cem Can bulmuştur. Muhammet Can bu buluş ile büyük alkış topladı'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_masker(\"ışınlanma teknolojisini [MASK] Can  bulmuştur. Muhammet Can bu buluş ile büyük alkış topladı\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# auto-detect device\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"[INFO] using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Merhaba [MASK] efendi nasıl gidiyor? ----> Top 5 Predictions: {'token_str': ['Mehmet', 'doktor', 'Salih', 'hoca', 'hanım'], 'score': ['0.0415', '0.0300', '0.0276', '0.0280', '0.0369']}\n",
      "Text: [MASK] yaptın? ----> Top 5 Predictions: {'token_str': ['ne', 'neler', 'nasıl', 'neden', 'Ne'], 'score': ['0.5882', '0.0835', '0.0642', '0.0283', '0.0223']}\n"
     ]
    }
   ],
   "source": [
    "# WITHOUT PAD TOKEN\n",
    "from model import FillMaskPipeline, IsNextPipeline\n",
    "torch.random.manual_seed(42)\n",
    "model_from_hf.to(device)\n",
    "fill_mask = FillMaskPipeline(model=model_from_hf, tokenizer=tokenizer, strategy=\"greedy\")\n",
    "fill_mask([\"Merhaba [MASK] efendi nasıl gidiyor?\", \"[MASK] yaptın?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Merhaba [MASK] efendi nasıl gidiyor? ----> Top 5 Predictions: {'token_str': ['Mehmet', 'doktor', 'Salih', 'hoca', 'hanım'], 'score': ['0.0415', '0.0300', '0.0276', '0.0280', '0.0369']}\n",
      "Text: ışınlanma teknolojisini [MASK] Can  bulmuştur. Muhammet Can bu buluş ile büyük alkış topladı ----> Top 5 Predictions: {'token_str': ['Muhammet', 'Muhammed', 'Mehmet', 'Hasan', 'Cem'], 'score': ['0.9561', '0.0241', '0.0035', '0.0011', '0.0007']}\n"
     ]
    }
   ],
   "source": [
    "# WITH PAD TOKEN\n",
    "torch.random.manual_seed(42)\n",
    "fill_mask = FillMaskPipeline(model=model_from_hf, tokenizer=tokenizer, strategy=\"greedy\")\n",
    "fill_mask([\"Merhaba [MASK] efendi nasıl gidiyor?\", \"ışınlanma teknolojisini [MASK] Can  bulmuştur. Muhammet Can bu buluş ile büyük alkış topladı\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Merhaba [MASK] efendi nasıl gidiyor? ----> Top 5 Predictions: {'token_str': ['Mehmet', 'doktor', 'Salih', 'hoca', 'hanım'], 'score': ['0.0415', '0.0300', '0.0276', '0.0280', '0.0369']}\n",
      "Text: ışınlanma teknolojisini [MASK] Can  bulmuştur. Muhammet Can bu buluş ile büyük alkış topladı ----> Top 5 Predictions: {'token_str': ['Muhammet', 'Muhammed', 'Mehmet', 'Hasan', 'Cem'], 'score': ['0.9561', '0.0241', '0.0035', '0.0011', '0.0007']}\n"
     ]
    }
   ],
   "source": [
    "# WITH PAD TOKEN\n",
    "torch.random.manual_seed(42)\n",
    "fill_mask = FillMaskPipeline(model=model_from_hf, tokenizer=tokenizer, strategy=\"greedy\")\n",
    "fill_mask([\"Merhaba [MASK] efendi nasıl gidiyor?\", \"ışınlanma teknolojisini [MASK] Can  bulmuştur. Muhammet Can bu buluş ile büyük alkış topladı\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Merhaba [MASK] efendi nasıl gidiyor? ----> Top 5 Predictions: {'token_str': ['Mehmet', 'doktor', 'Salih', 'hoca', 'hanım'], 'score': ['0.0415', '0.0300', '0.0276', '0.0280', '0.0369']}\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "fill_mask = FillMaskPipeline(model=model_from_hf, tokenizer=tokenizer, strategy=\"greedy\")\n",
    "fill_mask([\"Merhaba [MASK] efendi nasıl gidiyor?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Merhaba [MASK] efendi nasıl gidiyor? ----> Top 5 Predictions: {'token_str': ['Mehmet', 'Mustafa', 'Bey', 'Ali', 'osman'], 'score': ['0.0415', '0.0253', '0.0073', '0.0242', '0.0032']}\n",
      "Text: ışınlanma teknolojisi [MASK] gitmektedir. ----> Top 5 Predictions: {'token_str': ['yavaş', 'hoşuma', 'ileri', 'yanlış', 'hoşuna'], 'score': ['0.0113', '0.0057', '0.0342', '0.0133', '0.0210']}\n"
     ]
    }
   ],
   "source": [
    "fill_mask = FillMaskPipeline(model=model_from_hf, tokenizer=tokenizer, strategy=\"multinomial\")\n",
    "fill_mask([\"Merhaba [MASK] efendi nasıl gidiyor?\", \"ışınlanma teknolojisi [MASK] gitmektedir.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Merhaba [MASK] efendi nasıl gidiyor? ----> Top 5 Predictions: {'token_str': ['istiyordum', 'mesafesinde', '##D', 'Kit', '##ırsın'], 'score': ['0.0002', '0.0002', '0.0003', '0.0003', '0.0002']}\n",
      "Text: ışınlanma teknolojisi [MASK] gitmektedir. ----> Top 5 Predictions: {'token_str': ['Ster', 'Hon', '##idge', 'mesafesinde', 'Kan'], 'score': ['0.0002', '0.0003', '0.0002', '0.0003', '0.0003']}\n"
     ]
    }
   ],
   "source": [
    "# random initialized, so garbage output expected\n",
    "fill_mask_custom = FillMaskPipeline(model=BertForPreTraining(BertConfig()), tokenizer=tokenizer, strategy=\"greedy\")    \n",
    "fill_mask_custom([\"Merhaba [MASK] efendi nasıl gidiyor?\", \"ışınlanma teknolojisi [MASK] gitmektedir.\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_pipeline = IsNextPipeline(model=model_from_hf, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['Çocuk sahibi çift sayısında inanılmaz bir artış var.', 'Uzaya ilk Bekiroğ Reis çıkmıştır'] Predictions ----> isNext: 0.072, notNext: 0.928\n",
      "Text: ['Bu nsp olayı bir garip oldu. Sanki kablolar ters bağlandı.', 'Bir şekilde kabloları ayarlamak gerekli'] Predictions ----> isNext: 1.000, notNext: 0.000\n",
      "Text: [\"çoğunuz yaş itibariyle tanımaz ama istanbul'un en iyi belediye başkanı justinianus'tur.\", 'Samsun Ayvacık belediyesi ne yapmak nereye varmak istemektedir!'] Predictions ----> isNext: 0.056, notNext: 0.944\n"
     ]
    }
   ],
   "source": [
    "nsp_pipeline([[\"Çocuk sahibi çift sayısında inanılmaz bir artış var.\", \"Uzaya ilk Bekiroğ Reis çıkmıştır\"],\n",
    "              [\"Bu nsp olayı bir garip oldu. Sanki kablolar ters bağlandı.\", \"Bir şekilde kabloları ayarlamak gerekli\"],\n",
    "              [\"çoğunuz yaş itibariyle tanımaz ama istanbul'un en iyi belediye başkanı justinianus'tur.\", \"Samsun Ayvacık belediyesi ne yapmak nereye varmak istemektedir!\"] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_implementation_tr.data.data_aux import load_xy_shard, visualize_sample, ModelInput, VisualizeModelInput, get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = load_xy_shard(0, block_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39062, 1024), 1024)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape, 256 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint16')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs_shard_0 = ModelInput.from_numpy_to_tensors(temp, block_size=256, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bool, torch.Size([39062, 256]), torch.Size([39062]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs_shard_0.attention_mask.dtype, model_inputs_shard_0.x.shape, model_inputs_shard_0.next_sentence_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_sample_model_input = ModelInput(\n",
    "                             x=model_inputs_shard_0.x[0],\n",
    "                             y=model_inputs_shard_0.y[0],\n",
    "                             attention_mask=model_inputs_shard_0.attention_mask[0],\n",
    "                             segment_ids=model_inputs_shard_0.segment_ids[0],\n",
    "                             next_sentence_label=model_inputs_shard_0.next_sentence_label[0]\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_decoded: [CLS] dune'un çocukları, [MASK] herbert'ın yazdığı dune adlı bilimkurgu serisinin üçüncü kitabı. roman 1 9 7 7 yılında hugo ödülüne aday gösterilmiştir. serinin ikinci [MASK] olan dune mesihi'nin sonunda paul atreides çöle giderek ortadan kaybolmuştu. kız kardeşi alia, i̇mparatorluğu'[MASK] naib'i olarak tahtta oturmakta ve paul ile chani'nin çocukları [MASK] [MASK] ve ganima fremenlerin koruması altında [MASK]. [MASK] [MASK] farklı yaratılışları kitap boyunca kendini yavaş yavaş ortaya [MASK] [MASK] alia [MASK] nın daveti üzerine arrakis'e geri dönen leydi jessica kızını değişmiş değişiklikler bulacaktır. serinin üçüncü kitabında [MASK] [MASK] duncan düzey önemli bir [MASK] oynamaktadır. [MASK] en büyük sırrı arrakis - yani dune - sokaklarında willie dine ve gezegen üzerindeki değişime küfreden vaiz'in gerçek kimliğidir [SEP] burada bazı alim­lerin sözleri, hayatları, kabirleri, kullanıcıların hakkındaki yaygın hatalara işaret edilmiş, hangi konulardaki rivayetlerin uydurma olduğu belirtilmiştir [MASK] yine bu [MASK] eserdeki hadislerin genellikle ilk iki [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK], ilim, taharet, salat, zekat, nikâh gibi başlıklar altında ve alfabetik sıra gözetilmeden fihrist niteli­ğinde yeni bir [MASK] yapılmıştır [MASK] kitap, ilk olarak beyrut [MASK] ta, daha [SEP]\n",
      "y_decoded: [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] frank [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] kitabı [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] n [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] leto [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] yaşamaktadırlar [PAD] i̇kizlerin [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] koyarken [PAD] [PAD]'[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] olarak [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] mentat [PAD] idaho [PAD] [PAD] rol [PAD] [PAD] kitabın [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] gezinirken [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] kitapları [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]. [PAD] [PAD] bölümde [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] keli­mesi zikredilerek iman [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] düzenleme [PAD]. [PAD] [PAD] [PAD] [PAD] [PAD]'[PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "attention_mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True], device='cuda:0')\n",
      "segment_ids: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "\n",
      "      X_FILLED        Y_FILLED\n",
      "    ----------      ----------\n",
      "        [MASK]           frank\n",
      "        [MASK]          kitabı\n",
      "        [MASK]               n\n",
      "        [MASK]             let\n",
      "        [MASK]             ##o\n",
      "        [MASK] yaşamaktadırlar\n",
      "        [MASK]           i̇kiz\n",
      "        [MASK]         ##lerin\n",
      "        [MASK]           koyar\n",
      "        [MASK]           ##ken\n",
      "        [MASK]               '\n",
      " değişiklikler          olarak\n",
      "        [MASK]             men\n",
      "        [MASK]           ##tat\n",
      "         düzey           idaho\n",
      "        [MASK]             rol\n",
      "        [MASK]         kitabın\n",
      "          will           gezin\n",
      "          ##ie         ##irken\n",
      "kullanıcıların       kitapları\n",
      "        [MASK]               .\n",
      "        [MASK]         bölümde\n",
      "        [MASK]            keli\n",
      "        [MASK]             ##­\n",
      "        [MASK]          ##mesi\n",
      "        [MASK]             zik\n",
      "        [MASK]           ##red\n",
      "        [MASK]        ##ilerek\n",
      "        [MASK]            iman\n",
      "        [MASK]       düzenleme\n",
      "        [MASK]               .\n",
      "        [MASK]               '\n",
      "\n",
      "isNext: False\n",
      "number_of_mask_token: 27\n",
      "number_of_filled_token: 32\n",
      "\n",
      "\n",
      "len_of_x: 256\n",
      "len_of_y: 256\n",
      "\n",
      "\n",
      "x_ids: tensor([    2,  9332,  1007,    11,  2211,  7100,    16,     4, 18518,    11,\n",
      "         2170,  4941,  9332,  1007,  2578, 11035,  9172,  3513,  4731,    18,\n",
      "         3296,    21,    29,    27,    27,  2090, 12595, 10811,  3515,  7931,\n",
      "           18,  9667,  2826,     4,  2065,  9332,  1007, 13394,  1006,    11,\n",
      "         2145,  3339,  5157, 23343, 16945,  4275,  9326,  1007,  6294,  6286,\n",
      "         7933,  8027,    18,  2643,  4355,  3337,  1011,    16,  3801,    11,\n",
      "            4,  2655,  2109,    11,    49,  2013, 30273, 19936, 13544,  1011,\n",
      "         1933,  5157,  2023,  6991,  1006,    11,  2145,  7100,     4,     4,\n",
      "         1933,  9047,  5673,  5441,  2246,  2140, 15933,  3157,     4,    18,\n",
      "            4,     4,  2815, 18098,  1991,  3260,  3036,  4811,  5378,  5378,\n",
      "         2968,     4,     4,  3337,  1011,     4,  2151, 27640,  2726, 16575,\n",
      "        25808,    11,    45,  2917,  7916,  9785,  1972, 20276, 19095, 27193,\n",
      "         9099, 20413,  6061,    18,  9667,  3513,  8035,     4,     4, 27526,\n",
      "         4628,  2574,  1941,     4,  7095,    18,     4,  2060,  2207, 20513,\n",
      "        16575, 25808,    17,  4077,  9332,  1007,    17, 30902, 12038,  3991,\n",
      "        22371,  1933,  7047,  4965, 19980, 16917,  4537,  1926, 30097,    11,\n",
      "         2028,  2513, 12920,  1990,     3,  3341,  2587, 16209,  1176,  2140,\n",
      "         7968,    16,  2981,  1991,    16,  3774,  5936,  1006,    16, 12277,\n",
      "         7372,  3721, 25969,  1011,  5338,  4380,    16,  7338, 10846,  2022,\n",
      "         9173,  2140, 16137,  1976,  2418, 11754,     4,  3416,  1989,     4,\n",
      "        14231,  2022, 10901,  2140,  3136,  2155,  2319,     4,     4,     4,\n",
      "            4,     4,     4,     4,    16, 17303,    16,  2823,  2855,    16,\n",
      "        27971,    16, 15217,  1009,    16, 22044,  2244,  6503,  1927,  3157,\n",
      "         1933, 27833,  3194,  8866, 20579,  4783,  8755,  2057,  7972,  1176,\n",
      "         3358,  2274,  1941,     4,  4788,     4,  3260,    16,  2155,  2013,\n",
      "        16843,     4,  2477,    16,  2126,     3], device='cuda:0')\n",
      "y_ids: tensor([    0,     0,     0,     0,     0,     0,     0,  5556,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,  4731,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           54,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,  7124,  1020,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0, 29537,     0,\n",
      "        16572,  2140,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0, 18021,  2152,     0,     0,    11,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "         2013,     0,     0,     0,     0,     0,     0,  3398, 31710,     0,\n",
      "        27572,     0,     0,  2887,     0,     0,  8240,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0, 25335,  7976,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,  9109,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,    18,     0,     0,  7185,\n",
      "            0,     0,     0,     0,     0,     0,     0,  3924,  1176,  2487,\n",
      "        18296,  4537,  4066, 20223,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,  6858,     0,    18,     0,     0,     0,     0,\n",
      "            0,    11,     0,     0,     0,     0], device='cuda:0')\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualize_sample(VisualizeModelInput(one_sample_model_input, show_ids=True, show_attention_and_segment=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build_and_train_bert_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
