{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Denemeleri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ã–nce HuggingFace'ten, hali hazÄ±rda tamamen tÃ¼rkÃ§e veriler Ã¼zerinde eÄŸitilmiÅŸ BerTurk modeli denenecek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForPreTraining\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "model = BertForPreTraining.from_pretrained(\"dbmdz/bert-base-turkish-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight torch.Size([32000, 768])\n",
      "bert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.pooler.dense.weight torch.Size([768, 768])\n",
      "bert.pooler.dense.bias torch.Size([768])\n",
      "cls.predictions.bias torch.Size([32000])\n",
      "cls.predictions.transform.dense.weight torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.bias torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.weight torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.bias torch.Size([768])\n",
      "cls.predictions.decoder.weight torch.Size([32000, 768])\n",
      "cls.predictions.decoder.bias torch.Size([32000])\n",
      "cls.seq_relationship.weight torch.Size([2, 768])\n",
      "cls.seq_relationship.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "sd_hf = model.state_dict()\n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(sd_hf[\"bert.embeddings.word_embeddings.weight\"], sd_hf[\"cls.predictions.decoder.weight\"]) # tying kontrolÃ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_hf[\"bert.embeddings.word_embeddings.weight\"].data_ptr() == sd_hf[\"cls.predictions.decoder.weight\"].data_ptr()  # tying kontrolÃ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(sd_hf[\"cls.predictions.decoder.bias\"], sd_hf[\"cls.predictions.bias\"]) # tying kontrolÃ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_hf[\"cls.predictions.decoder.bias\"].data_ptr() == sd_hf[\"cls.predictions.bias\"].data_ptr()  # tying kontrolÃ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Selamlar efendim\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 32000]), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.prediction_logits.shape, outputs.seq_relationship_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05243540555238724,\n",
       "  'token': 3944,\n",
       "  'token_str': 'Mehmet',\n",
       "  'sequence': 'Merhaba Mehmet efendi nasÄ±l gidiyor?'},\n",
       " {'score': 0.03593067452311516,\n",
       "  'token': 7709,\n",
       "  'token_str': 'hanÄ±m',\n",
       "  'sequence': 'Merhaba hanÄ±m efendi nasÄ±l gidiyor?'},\n",
       " {'score': 0.03291669860482216,\n",
       "  'token': 3024,\n",
       "  'token_str': 'bey',\n",
       "  'sequence': 'Merhaba bey efendi nasÄ±l gidiyor?'},\n",
       " {'score': 0.031684163957834244,\n",
       "  'token': 7983,\n",
       "  'token_str': 'hoca',\n",
       "  'sequence': 'Merhaba hoca efendi nasÄ±l gidiyor?'},\n",
       " {'score': 0.030081957578659058,\n",
       "  'token': 10997,\n",
       "  'token_str': 'Salih',\n",
       "  'sequence': 'Merhaba Salih efendi nasÄ±l gidiyor?'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_masker = pipeline(task=\"fill-mask\", model=\"dbmdz/bert-base-turkish-cased\")\n",
    "fill_masker(\"Merhaba [MASK] efendi nasÄ±l gidiyor?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BertForPreTraining' from 'model' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertForPreTraining, BertConfig\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BertForPreTraining' from 'model' (unknown location)"
     ]
    }
   ],
   "source": [
    "from model import BertForPreTraining, BertConfig\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = BertConfig()\n",
    "custom_model = BertForPreTraining(default_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight torch.Size([32000, 768]) -------- bert.embeddings.word_embeddings.weight torch.Size([32000, 768])\n",
      "bert.embeddings.position_embeddings.weight torch.Size([512, 768]) -------- bert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight torch.Size([2, 768]) -------- bert.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight torch.Size([768]) -------- bert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias torch.Size([768]) -------- bert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768]) -------- bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768]) -------- bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768]) -------- bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768]) -------- bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768]) -------- bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768]) -------- bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768]) -------- bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768]) -------- bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072]) -------- bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072]) -------- bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768]) -------- bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768]) -------- bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768]) -------- bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.pooler.dense.weight torch.Size([768, 768]) -------- bert.pooler.dense.weight torch.Size([768, 768])\n",
      "bert.pooler.dense.bias torch.Size([768]) -------- bert.pooler.dense.bias torch.Size([768])\n",
      "cls.predictions.bias torch.Size([32000]) -------- cls.predictions.bias torch.Size([32000])\n",
      "cls.predictions.transform.dense.weight torch.Size([768, 768]) -------- cls.predictions.transform.dense.weight torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.bias torch.Size([768]) -------- cls.predictions.transform.dense.bias torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.weight torch.Size([768]) -------- cls.predictions.transform.LayerNorm.weight torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.bias torch.Size([768]) -------- cls.predictions.transform.LayerNorm.bias torch.Size([768])\n",
      "cls.predictions.decoder.weight torch.Size([32000, 768]) -------- cls.predictions.decoder.weight torch.Size([32000, 768])\n",
      "cls.predictions.decoder.bias torch.Size([32000]) -------- cls.predictions.decoder.bias torch.Size([32000])\n",
      "cls.seq_relationship.weight torch.Size([2, 768]) -------- cls.seq_relationship.weight torch.Size([2, 768])\n",
      "cls.seq_relationship.bias torch.Size([2]) -------- cls.seq_relationship.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "custom_sd = custom_model.state_dict()\n",
    "\n",
    "assert len(custom_sd) == len(sd_hf)\n",
    "\n",
    "for custom, hf in zip(custom_sd.items(), sd_hf.items()):\n",
    "    k, v = custom\n",
    "    k_hf, v_hf = hf\n",
    "    assert k == k_hf, f\"{k} : {k_hf}\"\n",
    "    assert v.shape == v_hf.shape, f\"{k} : {v.shape} != {v_hf.shape}\"\n",
    "    print(k, v.shape, \"--------\", k_hf, v_hf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained bert: dbmdz/bert-base-turkish-cased\n"
     ]
    }
   ],
   "source": [
    "model_from_hf = BertForPreTraining.from_pretrained()\n",
    "mm = model_from_hf.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05243540555238724,\n",
       "  'token': 3944,\n",
       "  'token_str': 'Mehmet',\n",
       "  'sequence': 'Merhaba Mehmet efendi nasÄ±l gidiyor?'},\n",
       " {'score': 0.03593067452311516,\n",
       "  'token': 7709,\n",
       "  'token_str': 'hanÄ±m',\n",
       "  'sequence': 'Merhaba hanÄ±m efendi nasÄ±l gidiyor?'},\n",
       " {'score': 0.03291669860482216,\n",
       "  'token': 3024,\n",
       "  'token_str': 'bey',\n",
       "  'sequence': 'Merhaba bey efendi nasÄ±l gidiyor?'},\n",
       " {'score': 0.031684163957834244,\n",
       "  'token': 7983,\n",
       "  'token_str': 'hoca',\n",
       "  'sequence': 'Merhaba hoca efendi nasÄ±l gidiyor?'},\n",
       " {'score': 0.030081957578659058,\n",
       "  'token': 10997,\n",
       "  'token_str': 'Salih',\n",
       "  'sequence': 'Merhaba Salih efendi nasÄ±l gidiyor?'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "fill_masker = pipeline(task=\"fill-mask\", model=\"dbmdz/bert-base-turkish-cased\")\n",
    "fill_masker(\"Merhaba [MASK] efendi nasÄ±l gidiyor?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9745508432388306,\n",
       "  'token': 22777,\n",
       "  'token_str': 'Muhammet',\n",
       "  'sequence': 'Ä±ÅŸÄ±nlanma teknolojisini Muhammet Can bulmuÅŸtur. Muhammet Can bu buluÅŸ ile bÃ¼yÃ¼k alkÄ±ÅŸ topladÄ±'},\n",
       " {'score': 0.016110757365822792,\n",
       "  'token': 7397,\n",
       "  'token_str': 'Muhammed',\n",
       "  'sequence': 'Ä±ÅŸÄ±nlanma teknolojisini Muhammed Can bulmuÅŸtur. Muhammet Can bu buluÅŸ ile bÃ¼yÃ¼k alkÄ±ÅŸ topladÄ±'},\n",
       " {'score': 0.0018351072212681174,\n",
       "  'token': 3944,\n",
       "  'token_str': 'Mehmet',\n",
       "  'sequence': 'Ä±ÅŸÄ±nlanma teknolojisini Mehmet Can bulmuÅŸtur. Muhammet Can bu buluÅŸ ile bÃ¼yÃ¼k alkÄ±ÅŸ topladÄ±'},\n",
       " {'score': 0.00045096001122146845,\n",
       "  'token': 6001,\n",
       "  'token_str': 'Hasan',\n",
       "  'sequence': 'Ä±ÅŸÄ±nlanma teknolojisini Hasan Can bulmuÅŸtur. Muhammet Can bu buluÅŸ ile bÃ¼yÃ¼k alkÄ±ÅŸ topladÄ±'},\n",
       " {'score': 0.0004167473816778511,\n",
       "  'token': 6391,\n",
       "  'token_str': 'Cem',\n",
       "  'sequence': 'Ä±ÅŸÄ±nlanma teknolojisini Cem Can bulmuÅŸtur. Muhammet Can bu buluÅŸ ile bÃ¼yÃ¼k alkÄ±ÅŸ topladÄ±'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_masker(\"Ä±ÅŸÄ±nlanma teknolojisini [MASK] Can  bulmuÅŸtur. Muhammet Can bu buluÅŸ ile bÃ¼yÃ¼k alkÄ±ÅŸ topladÄ±\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# auto-detect device\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"[INFO] using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Merhaba [MASK] efendi nasÄ±l gidiyor? ----> Top 5 Predictions: {'token_str': ['Mehmet', 'doktor', 'Salih', 'hoca', 'hanÄ±m'], 'score': ['0.0415', '0.0300', '0.0276', '0.0280', '0.0369']}\n",
      "Text: [MASK] yaptÄ±n? ----> Top 5 Predictions: {'token_str': ['ne', 'neler', 'nasÄ±l', 'neden', 'Ne'], 'score': ['0.5882', '0.0835', '0.0642', '0.0283', '0.0223']}\n"
     ]
    }
   ],
   "source": [
    "# WITHOUT PAD TOKEN\n",
    "from model import FillMaskPipeline, IsNextPipeline\n",
    "torch.random.manual_seed(42)\n",
    "model_from_hf.to(device)\n",
    "fill_mask = FillMaskPipeline(model=model_from_hf, tokenizer=tokenizer, strategy=\"greedy\")\n",
    "fill_mask([\"Merhaba [MASK] efendi nasÄ±l gidiyor?\", \"[MASK] yaptÄ±n?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Merhaba [MASK] efendi nasÄ±l gidiyor? ----> Top 5 Predictions: {'token_str': ['Mehmet', 'doktor', 'Salih', 'hoca', 'hanÄ±m'], 'score': ['0.0415', '0.0300', '0.0276', '0.0280', '0.0369']}\n",
      "Text: Ä±ÅŸÄ±nlanma teknolojisini [MASK] Can  bulmuÅŸtur. Muhammet Can bu buluÅŸ ile bÃ¼yÃ¼k alkÄ±ÅŸ topladÄ± ----> Top 5 Predictions: {'token_str': ['Muhammet', 'Muhammed', 'Mehmet', 'Hasan', 'Cem'], 'score': ['0.9561', '0.0241', '0.0035', '0.0011', '0.0007']}\n"
     ]
    }
   ],
   "source": [
    "# WITH PAD TOKEN\n",
    "torch.random.manual_seed(42)\n",
    "fill_mask = FillMaskPipeline(model=model_from_hf, tokenizer=tokenizer, strategy=\"greedy\")\n",
    "fill_mask([\"Merhaba [MASK] efendi nasÄ±l gidiyor?\", \"Ä±ÅŸÄ±nlanma teknolojisini [MASK] Can  bulmuÅŸtur. Muhammet Can bu buluÅŸ ile bÃ¼yÃ¼k alkÄ±ÅŸ topladÄ±\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Merhaba [MASK] efendi nasÄ±l gidiyor? ----> Top 5 Predictions: {'token_str': ['Mehmet', 'doktor', 'Salih', 'hoca', 'hanÄ±m'], 'score': ['0.0415', '0.0300', '0.0276', '0.0280', '0.0369']}\n",
      "Text: Ä±ÅŸÄ±nlanma teknolojisini [MASK] Can  bulmuÅŸtur. Muhammet Can bu buluÅŸ ile bÃ¼yÃ¼k alkÄ±ÅŸ topladÄ± ----> Top 5 Predictions: {'token_str': ['Muhammet', 'Muhammed', 'Mehmet', 'Hasan', 'Cem'], 'score': ['0.9561', '0.0241', '0.0035', '0.0011', '0.0007']}\n"
     ]
    }
   ],
   "source": [
    "# WITH PAD TOKEN\n",
    "torch.random.manual_seed(42)\n",
    "fill_mask = FillMaskPipeline(model=model_from_hf, tokenizer=tokenizer, strategy=\"greedy\")\n",
    "fill_mask([\"Merhaba [MASK] efendi nasÄ±l gidiyor?\", \"Ä±ÅŸÄ±nlanma teknolojisini [MASK] Can  bulmuÅŸtur. Muhammet Can bu buluÅŸ ile bÃ¼yÃ¼k alkÄ±ÅŸ topladÄ±\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Merhaba [MASK] efendi nasÄ±l gidiyor? ----> Top 5 Predictions: {'token_str': ['Mehmet', 'doktor', 'Salih', 'hoca', 'hanÄ±m'], 'score': ['0.0415', '0.0300', '0.0276', '0.0280', '0.0369']}\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "fill_mask = FillMaskPipeline(model=model_from_hf, tokenizer=tokenizer, strategy=\"greedy\")\n",
    "fill_mask([\"Merhaba [MASK] efendi nasÄ±l gidiyor?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Merhaba [MASK] efendi nasÄ±l gidiyor? ----> Top 5 Predictions: {'token_str': ['Mehmet', 'Mustafa', 'Bey', 'Ali', 'osman'], 'score': ['0.0415', '0.0253', '0.0073', '0.0242', '0.0032']}\n",
      "Text: Ä±ÅŸÄ±nlanma teknolojisi [MASK] gitmektedir. ----> Top 5 Predictions: {'token_str': ['yavaÅŸ', 'hoÅŸuma', 'ileri', 'yanlÄ±ÅŸ', 'hoÅŸuna'], 'score': ['0.0113', '0.0057', '0.0342', '0.0133', '0.0210']}\n"
     ]
    }
   ],
   "source": [
    "fill_mask = FillMaskPipeline(model=model_from_hf, tokenizer=tokenizer, strategy=\"multinomial\")\n",
    "fill_mask([\"Merhaba [MASK] efendi nasÄ±l gidiyor?\", \"Ä±ÅŸÄ±nlanma teknolojisi [MASK] gitmektedir.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Merhaba [MASK] efendi nasÄ±l gidiyor? ----> Top 5 Predictions: {'token_str': ['istiyordum', 'mesafesinde', '##D', 'Kit', '##Ä±rsÄ±n'], 'score': ['0.0002', '0.0002', '0.0003', '0.0003', '0.0002']}\n",
      "Text: Ä±ÅŸÄ±nlanma teknolojisi [MASK] gitmektedir. ----> Top 5 Predictions: {'token_str': ['Ster', 'Hon', '##idge', 'mesafesinde', 'Kan'], 'score': ['0.0002', '0.0003', '0.0002', '0.0003', '0.0003']}\n"
     ]
    }
   ],
   "source": [
    "# random initialized, so garbage output expected\n",
    "fill_mask_custom = FillMaskPipeline(model=BertForPreTraining(BertConfig()), tokenizer=tokenizer, strategy=\"greedy\")    \n",
    "fill_mask_custom([\"Merhaba [MASK] efendi nasÄ±l gidiyor?\", \"Ä±ÅŸÄ±nlanma teknolojisi [MASK] gitmektedir.\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_pipeline = IsNextPipeline(model=model_from_hf, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['Ã‡ocuk sahibi Ã§ift sayÄ±sÄ±nda inanÄ±lmaz bir artÄ±ÅŸ var.', 'Uzaya ilk BekiroÄŸ Reis Ã§Ä±kmÄ±ÅŸtÄ±r'] Predictions ----> isNext: 0.072, notNext: 0.928\n",
      "Text: ['Bu nsp olayÄ± bir garip oldu. Sanki kablolar ters baÄŸlandÄ±.', 'Bir ÅŸekilde kablolarÄ± ayarlamak gerekli'] Predictions ----> isNext: 1.000, notNext: 0.000\n",
      "Text: [\"Ã§oÄŸunuz yaÅŸ itibariyle tanÄ±maz ama istanbul'un en iyi belediye baÅŸkanÄ± justinianus'tur.\", 'Samsun AyvacÄ±k belediyesi ne yapmak nereye varmak istemektedir!'] Predictions ----> isNext: 0.056, notNext: 0.944\n"
     ]
    }
   ],
   "source": [
    "nsp_pipeline([[\"Ã‡ocuk sahibi Ã§ift sayÄ±sÄ±nda inanÄ±lmaz bir artÄ±ÅŸ var.\", \"Uzaya ilk BekiroÄŸ Reis Ã§Ä±kmÄ±ÅŸtÄ±r\"],\n",
    "              [\"Bu nsp olayÄ± bir garip oldu. Sanki kablolar ters baÄŸlandÄ±.\", \"Bir ÅŸekilde kablolarÄ± ayarlamak gerekli\"],\n",
    "              [\"Ã§oÄŸunuz yaÅŸ itibariyle tanÄ±maz ama istanbul'un en iyi belediye baÅŸkanÄ± justinianus'tur.\", \"Samsun AyvacÄ±k belediyesi ne yapmak nereye varmak istemektedir!\"] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from bert_implementation_tr.data.data_aux import load_xy_shard, visualize_sample, ModelInput, VisualizeModelInput, get_tokenizer\n",
    "from model import *\n",
    "import torch\n",
    "\n",
    "# auto-detect device\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"[INFO] using device: {device}\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\" # let's override it right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_model = BertForPreTraining(BertConfig())\n",
    "random_model = random_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam Parametre SayÄ±sÄ±: 111243010\n"
     ]
    }
   ],
   "source": [
    "# Toplam parametre sayÄ±sÄ±nÄ± hesapla\n",
    "total_params = sum(p.numel() for p in random_model.parameters())\n",
    "print(\"Toplam Parametre SayÄ±sÄ±:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Bellek KullanÄ±mÄ± (GB): 0.4144125059247017\n"
     ]
    }
   ],
   "source": [
    "# Parametre baÅŸÄ±na byte cinsinden boyut (Ã¶rneÄŸin, float32 iÃ§in 4 byte)\n",
    "param_size = 4\n",
    "\n",
    "# Toplam boyutu hesapla (byte cinsinden)\n",
    "total_memory_bytes = total_params * param_size\n",
    "\n",
    "# GB cinsine Ã§evir\n",
    "total_memory_gb = total_memory_bytes / (1024 ** 3)\n",
    "\n",
    "print(\"Model Bellek KullanÄ±mÄ± (GB):\", total_memory_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_sample_model_input = ModelInput(\n",
    "#                             input_ids=model_inputs_shard_0.input_ids[0],\n",
    "#                             labels=model_inputs_shard_0.labels[0],\n",
    "#                             attention_mask=model_inputs_shard_0.attention_mask[0],\n",
    "#                             token_type_ids=model_inputs_shard_0.token_type_ids[0],\n",
    "#                             next_sentence_label=model_inputs_shard_0.next_sentence_label[0]\n",
    "#                         )\n",
    "#visualize_sample(VisualizeModelInput(one_sample_model_input, show_ids=True, show_attention_and_segment=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict = ModelInput.from_numpy_to_tensors_dict(load_xy_shard(0, block_size=256)[0:1], block_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict = { k: v.to(device) for k, v in temp_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = random_model(**temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss:  11.218903541564941\n",
      "seq_relationship_logits:  tensor([[0.0442, 0.1998]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "prediction_logits:  tensor([[[ 1.2935, -0.4987,  0.3163,  ..., -0.2023, -0.0688, -0.9404],\n",
      "         [ 0.8961, -0.6119, -0.9890,  ...,  0.4870, -0.4408, -0.4997],\n",
      "         [ 0.7256, -0.3088,  0.2867,  ...,  0.5798, -0.7951, -0.3199],\n",
      "         ...,\n",
      "         [ 0.1811, -0.0490, -1.1911,  ...,  1.0088,  0.1699, -0.0295],\n",
      "         [ 0.8136,  0.0698, -0.1525,  ...,  0.9332,  0.6340, -0.6499],\n",
      "         [ 0.2951, -0.3336, -0.3732,  ...,  0.3542,  0.2385, -0.3002]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"total loss: \", out.loss.item()) \n",
    "print(\"seq_relationship_logits: \", out.seq_relationship_logits) \n",
    "print(\"prediction_logits: \", out.prediction_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nextseqlabel en saÄŸa\n",
    "# labels'da clas tokeni pad token olacak (ya da buradaki tÃ¼m pad tokenlar -100 olacak: tek bir ce yeter o zaman)\n",
    "# benim kendi vocab'Ä±m 32001 tane aslÄ±nda (sonradan \"...\" eklediÄŸim iÃ§in) buna bi Ã§Ã¶zÃ¼m\n",
    "# ÅŸimdilik yukarÄ±daki problemleri geÃ§ici Ã§Ã¶zeceÄŸim\n",
    "\n",
    "# a.convert_tokens_to_ids(\"...\"), a.convert_tokens_to_ids(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bfloat16 desteÄŸi: True\n"
     ]
    }
   ],
   "source": [
    "#import torch\n",
    "#\n",
    "## CihazÄ± belirle\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#\n",
    "## GPU'da bfloat16 desteÄŸi var mÄ±?\n",
    "#if device.type == \"cuda\":\n",
    "#    bfloat16_supported = torch.cuda.get_device_capability(device) >= (8, 0)  # A100 (ampere) ve sonrasÄ± bfloat16 destekler\n",
    "#    print(\"bfloat16 desteÄŸi:\", bfloat16_supported)\n",
    "#else:\n",
    "#    print(\"GPU mevcut deÄŸil.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      "split: train\n",
      "\n",
      "data shards directory: bert_implementation_tr/data/xy_shards_256\n",
      "block size: 256\n",
      "batch size: 32\n",
      "total number of shards: 35\n",
      "total number of tokens: 350999552\n",
      "total number of samples: 1371092\n",
      "-------------------------------------------------------------------------\n",
      "1 batch: 8192 tokens\n",
      "1 epoch: 42846 batches\n",
      "1 epoch: 350999552 tokens\n",
      "1 shard: ~10028558 tokens\n",
      "1 shard: ~1224 batches\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "896\n",
      "928\n",
      "960\n",
      "992\n",
      "1024\n",
      "1056\n",
      "1088\n",
      "1120\n",
      "1152\n",
      "1184\n",
      "1216\n",
      "1248\n",
      "1280\n",
      "1312\n",
      "1344\n",
      "1376\n",
      "1408\n",
      "1440\n",
      "1472\n",
      "1504\n",
      "1536\n",
      "1568\n",
      "1600\n",
      "1632\n",
      "1664\n",
      "1696\n",
      "1728\n",
      "1760\n",
      "1792\n",
      "1824\n",
      "1856\n",
      "1888\n",
      "1920\n",
      "1952\n",
      "1984\n",
      "2016\n",
      "2048\n",
      "2080\n",
      "2112\n",
      "2144\n",
      "2176\n",
      "2208\n",
      "2240\n",
      "2272\n",
      "2304\n",
      "2336\n",
      "2368\n",
      "2400\n",
      "2432\n",
      "2464\n",
      "2496\n",
      "2528\n",
      "2560\n",
      "2592\n",
      "2624\n",
      "2656\n",
      "2688\n",
      "2720\n",
      "2752\n",
      "2784\n",
      "2816\n",
      "2848\n",
      "2880\n",
      "2912\n",
      "2944\n",
      "2976\n",
      "3008\n",
      "3040\n",
      "3072\n",
      "3104\n",
      "3136\n",
      "3168\n",
      "3200\n",
      "3232\n",
      "3264\n",
      "3296\n",
      "3328\n",
      "3360\n",
      "3392\n",
      "3424\n",
      "3456\n",
      "3488\n",
      "3520\n",
      "3552\n",
      "3584\n",
      "3616\n",
      "3648\n",
      "3680\n",
      "3712\n",
      "3744\n",
      "3776\n",
      "3808\n",
      "3840\n",
      "3872\n",
      "3904\n",
      "3936\n",
      "3968\n",
      "4000\n",
      "4032\n",
      "4064\n",
      "4096\n",
      "4128\n",
      "4160\n",
      "4192\n",
      "4224\n",
      "4256\n",
      "4288\n",
      "4320\n",
      "4352\n",
      "4384\n",
      "4416\n",
      "4448\n",
      "4480\n",
      "4512\n",
      "4544\n",
      "4576\n",
      "4608\n",
      "4640\n",
      "4672\n",
      "4704\n",
      "4736\n",
      "4768\n",
      "4800\n",
      "4832\n",
      "4864\n",
      "4896\n",
      "4928\n",
      "4960\n",
      "4992\n",
      "5024\n",
      "5056\n",
      "5088\n",
      "5120\n",
      "5152\n",
      "5184\n",
      "5216\n",
      "5248\n",
      "5280\n",
      "5312\n",
      "5344\n",
      "5376\n",
      "5408\n",
      "5440\n",
      "5472\n",
      "5504\n",
      "5536\n",
      "5568\n",
      "5600\n",
      "5632\n",
      "5664\n",
      "5696\n",
      "5728\n",
      "5760\n",
      "5792\n",
      "5824\n",
      "5856\n",
      "5888\n",
      "5920\n",
      "5952\n",
      "5984\n",
      "6016\n",
      "6048\n",
      "6080\n",
      "6112\n",
      "6144\n",
      "6176\n",
      "6208\n",
      "6240\n",
      "6272\n",
      "6304\n",
      "6336\n",
      "6368\n",
      "6400\n",
      "6432\n",
      "6464\n",
      "6496\n",
      "6528\n",
      "6560\n",
      "6592\n",
      "6624\n",
      "6656\n",
      "6688\n",
      "6720\n",
      "6752\n",
      "6784\n",
      "6816\n",
      "6848\n",
      "6880\n",
      "6912\n",
      "6944\n",
      "6976\n",
      "7008\n",
      "7040\n",
      "7072\n",
      "7104\n",
      "7136\n",
      "7168\n",
      "7200\n",
      "7232\n",
      "7264\n",
      "7296\n",
      "7328\n",
      "7360\n",
      "7392\n",
      "7424\n",
      "7456\n",
      "7488\n",
      "7520\n",
      "7552\n",
      "7584\n",
      "7616\n",
      "7648\n",
      "7680\n",
      "7712\n",
      "7744\n",
      "7776\n",
      "7808\n",
      "7840\n",
      "7872\n",
      "7904\n",
      "7936\n",
      "7968\n",
      "8000\n",
      "8032\n",
      "8064\n",
      "8096\n",
      "8128\n",
      "8160\n",
      "8192\n",
      "8224\n",
      "8256\n",
      "8288\n",
      "8320\n",
      "8352\n",
      "8384\n",
      "8416\n",
      "8448\n",
      "8480\n",
      "8512\n",
      "8544\n",
      "8576\n",
      "8608\n",
      "8640\n",
      "8672\n",
      "8704\n",
      "8736\n",
      "8768\n",
      "8800\n",
      "8832\n",
      "8864\n",
      "8896\n",
      "8928\n",
      "8960\n",
      "8992\n",
      "9024\n",
      "9056\n",
      "9088\n",
      "9120\n",
      "9152\n",
      "9184\n",
      "9216\n",
      "9248\n",
      "9280\n",
      "9312\n",
      "9344\n",
      "9376\n",
      "9408\n",
      "9440\n",
      "9472\n",
      "9504\n",
      "9536\n",
      "9568\n",
      "9600\n",
      "9632\n",
      "9664\n",
      "9696\n",
      "9728\n",
      "9760\n",
      "9792\n",
      "9824\n",
      "9856\n",
      "9888\n",
      "9920\n",
      "9952\n",
      "9984\n",
      "10016\n",
      "10048\n",
      "10080\n",
      "10112\n",
      "10144\n",
      "10176\n",
      "10208\n",
      "10240\n",
      "10272\n",
      "10304\n",
      "10336\n",
      "10368\n",
      "10400\n",
      "10432\n",
      "10464\n",
      "10496\n",
      "10528\n",
      "10560\n",
      "10592\n",
      "10624\n",
      "10656\n",
      "10688\n",
      "10720\n",
      "10752\n",
      "10784\n",
      "10816\n",
      "10848\n",
      "10880\n",
      "10912\n",
      "10944\n",
      "10976\n",
      "11008\n",
      "11040\n",
      "11072\n",
      "11104\n",
      "11136\n",
      "11168\n",
      "11200\n",
      "11232\n",
      "11264\n",
      "11296\n",
      "11328\n",
      "11360\n",
      "11392\n",
      "11424\n",
      "11456\n",
      "11488\n",
      "11520\n",
      "11552\n",
      "11584\n",
      "11616\n",
      "11648\n",
      "11680\n",
      "11712\n",
      "11744\n",
      "11776\n",
      "11808\n",
      "11840\n",
      "11872\n",
      "11904\n",
      "11936\n",
      "11968\n",
      "12000\n",
      "12032\n",
      "12064\n",
      "12096\n",
      "12128\n",
      "12160\n",
      "12192\n",
      "12224\n",
      "12256\n",
      "12288\n",
      "12320\n",
      "12352\n",
      "12384\n",
      "12416\n",
      "12448\n",
      "12480\n",
      "12512\n",
      "12544\n",
      "12576\n",
      "12608\n",
      "12640\n",
      "12672\n",
      "12704\n",
      "12736\n",
      "12768\n",
      "12800\n",
      "12832\n",
      "12864\n",
      "12896\n",
      "12928\n",
      "12960\n",
      "12992\n",
      "13024\n",
      "13056\n",
      "13088\n",
      "13120\n",
      "13152\n",
      "13184\n",
      "13216\n",
      "13248\n",
      "13280\n",
      "13312\n",
      "13344\n",
      "13376\n",
      "13408\n",
      "13440\n",
      "13472\n",
      "13504\n",
      "13536\n",
      "13568\n",
      "13600\n",
      "13632\n",
      "13664\n",
      "13696\n",
      "13728\n",
      "13760\n",
      "13792\n",
      "13824\n",
      "13856\n",
      "13888\n",
      "13920\n",
      "13952\n",
      "13984\n",
      "14016\n",
      "14048\n",
      "14080\n",
      "14112\n",
      "14144\n",
      "14176\n",
      "14208\n",
      "14240\n",
      "14272\n",
      "14304\n",
      "14336\n",
      "14368\n",
      "14400\n",
      "14432\n",
      "14464\n",
      "14496\n",
      "14528\n",
      "14560\n",
      "14592\n",
      "14624\n",
      "14656\n",
      "14688\n",
      "14720\n",
      "14752\n",
      "14784\n",
      "14816\n",
      "14848\n",
      "14880\n",
      "14912\n",
      "14944\n",
      "14976\n",
      "15008\n",
      "15040\n",
      "15072\n",
      "15104\n",
      "15136\n",
      "15168\n",
      "15200\n",
      "15232\n",
      "15264\n",
      "15296\n",
      "15328\n",
      "15360\n",
      "15392\n",
      "15424\n",
      "15456\n",
      "15488\n",
      "15520\n",
      "15552\n",
      "15584\n",
      "15616\n",
      "15648\n",
      "15680\n",
      "15712\n",
      "15744\n",
      "15776\n",
      "15808\n",
      "15840\n",
      "15872\n",
      "15904\n",
      "15936\n",
      "15968\n",
      "16000\n",
      "16032\n",
      "16064\n",
      "16096\n",
      "16128\n",
      "16160\n",
      "16192\n",
      "16224\n",
      "16256\n",
      "16288\n",
      "16320\n",
      "16352\n",
      "16384\n",
      "16416\n",
      "16448\n",
      "16480\n",
      "16512\n",
      "16544\n",
      "16576\n",
      "16608\n",
      "16640\n",
      "16672\n",
      "16704\n",
      "16736\n",
      "16768\n",
      "16800\n",
      "16832\n",
      "16864\n",
      "16896\n",
      "16928\n",
      "16960\n",
      "16992\n",
      "17024\n",
      "17056\n",
      "17088\n",
      "17120\n",
      "17152\n",
      "17184\n",
      "17216\n",
      "17248\n",
      "17280\n",
      "17312\n",
      "17344\n",
      "17376\n",
      "17408\n",
      "17440\n",
      "17472\n",
      "17504\n",
      "17536\n",
      "17568\n",
      "17600\n",
      "17632\n",
      "17664\n",
      "17696\n",
      "17728\n",
      "17760\n",
      "17792\n",
      "17824\n",
      "17856\n",
      "17888\n",
      "17920\n",
      "17952\n",
      "17984\n",
      "18016\n",
      "18048\n",
      "18080\n",
      "18112\n",
      "18144\n",
      "18176\n",
      "18208\n",
      "18240\n",
      "18272\n",
      "18304\n",
      "18336\n",
      "18368\n",
      "18400\n",
      "18432\n",
      "18464\n",
      "18496\n",
      "18528\n",
      "18560\n",
      "18592\n",
      "18624\n",
      "18656\n",
      "18688\n",
      "18720\n",
      "18752\n",
      "18784\n",
      "18816\n",
      "18848\n",
      "18880\n",
      "18912\n",
      "18944\n",
      "18976\n",
      "19008\n",
      "19040\n",
      "19072\n",
      "19104\n",
      "19136\n",
      "19168\n",
      "19200\n",
      "19232\n",
      "19264\n",
      "19296\n",
      "19328\n",
      "19360\n",
      "19392\n",
      "19424\n",
      "19456\n",
      "19488\n",
      "19520\n",
      "19552\n",
      "19584\n",
      "19616\n",
      "19648\n",
      "19680\n",
      "19712\n",
      "19744\n",
      "19776\n",
      "19808\n",
      "19840\n",
      "19872\n",
      "19904\n",
      "19936\n",
      "19968\n",
      "20000\n",
      "20032\n",
      "20064\n",
      "20096\n",
      "20128\n",
      "20160\n",
      "20192\n",
      "20224\n",
      "20256\n",
      "20288\n",
      "20320\n",
      "20352\n",
      "20384\n",
      "20416\n",
      "20448\n",
      "20480\n",
      "20512\n",
      "20544\n",
      "20576\n",
      "20608\n",
      "20640\n",
      "20672\n",
      "20704\n",
      "20736\n",
      "20768\n",
      "20800\n",
      "20832\n",
      "20864\n",
      "20896\n",
      "20928\n",
      "20960\n",
      "20992\n",
      "21024\n",
      "21056\n",
      "21088\n",
      "21120\n",
      "21152\n",
      "21184\n",
      "21216\n",
      "21248\n",
      "21280\n",
      "21312\n",
      "21344\n",
      "21376\n",
      "21408\n",
      "21440\n",
      "21472\n",
      "21504\n",
      "21536\n",
      "21568\n",
      "21600\n",
      "21632\n",
      "21664\n",
      "21696\n",
      "21728\n",
      "21760\n",
      "21792\n",
      "21824\n",
      "21856\n",
      "21888\n",
      "21920\n",
      "21952\n",
      "21984\n",
      "22016\n",
      "22048\n",
      "22080\n",
      "22112\n",
      "22144\n",
      "22176\n",
      "22208\n",
      "22240\n",
      "22272\n",
      "22304\n",
      "22336\n",
      "22368\n",
      "22400\n",
      "22432\n",
      "22464\n",
      "22496\n",
      "22528\n",
      "22560\n",
      "22592\n",
      "22624\n",
      "22656\n",
      "22688\n",
      "22720\n",
      "22752\n",
      "22784\n",
      "22816\n",
      "22848\n",
      "22880\n",
      "22912\n",
      "22944\n",
      "22976\n",
      "23008\n",
      "23040\n",
      "23072\n",
      "23104\n",
      "23136\n",
      "23168\n",
      "23200\n",
      "23232\n",
      "23264\n",
      "23296\n",
      "23328\n",
      "23360\n",
      "23392\n",
      "23424\n",
      "23456\n",
      "23488\n",
      "23520\n",
      "23552\n",
      "23584\n",
      "23616\n",
      "23648\n",
      "23680\n",
      "23712\n",
      "23744\n",
      "23776\n",
      "23808\n",
      "23840\n",
      "23872\n",
      "23904\n",
      "23936\n",
      "23968\n",
      "24000\n",
      "24032\n",
      "24064\n",
      "24096\n",
      "24128\n",
      "24160\n",
      "24192\n",
      "24224\n",
      "24256\n",
      "24288\n",
      "24320\n",
      "24352\n",
      "24384\n",
      "24416\n",
      "24448\n",
      "24480\n",
      "24512\n",
      "24544\n",
      "24576\n",
      "24608\n",
      "24640\n",
      "24672\n",
      "24704\n",
      "24736\n",
      "24768\n",
      "24800\n",
      "24832\n",
      "24864\n",
      "24896\n",
      "24928\n",
      "24960\n",
      "24992\n",
      "25024\n",
      "25056\n",
      "25088\n",
      "25120\n",
      "25152\n",
      "25184\n",
      "25216\n",
      "25248\n",
      "25280\n",
      "25312\n",
      "25344\n",
      "25376\n",
      "25408\n",
      "25440\n",
      "25472\n",
      "25504\n",
      "25536\n",
      "25568\n",
      "25600\n",
      "25632\n",
      "25664\n",
      "25696\n",
      "25728\n",
      "25760\n",
      "25792\n",
      "25824\n",
      "25856\n",
      "25888\n",
      "25920\n",
      "25952\n",
      "25984\n",
      "26016\n",
      "26048\n",
      "26080\n",
      "26112\n",
      "26144\n",
      "26176\n",
      "26208\n",
      "26240\n",
      "26272\n",
      "26304\n",
      "26336\n",
      "26368\n",
      "26400\n",
      "26432\n",
      "26464\n",
      "26496\n",
      "26528\n",
      "26560\n",
      "26592\n",
      "26624\n",
      "26656\n",
      "26688\n",
      "26720\n",
      "26752\n",
      "26784\n",
      "26816\n",
      "26848\n",
      "26880\n",
      "26912\n",
      "26944\n",
      "26976\n",
      "27008\n",
      "27040\n",
      "27072\n",
      "27104\n",
      "27136\n",
      "27168\n",
      "27200\n",
      "27232\n",
      "27264\n",
      "27296\n",
      "27328\n",
      "27360\n",
      "27392\n",
      "27424\n",
      "27456\n",
      "27488\n",
      "27520\n",
      "27552\n",
      "27584\n",
      "27616\n",
      "27648\n",
      "27680\n",
      "27712\n",
      "27744\n",
      "27776\n",
      "27808\n",
      "27840\n",
      "27872\n",
      "27904\n",
      "27936\n",
      "27968\n",
      "28000\n",
      "28032\n",
      "28064\n",
      "28096\n",
      "28128\n",
      "28160\n",
      "28192\n",
      "28224\n",
      "28256\n",
      "28288\n",
      "28320\n",
      "28352\n",
      "28384\n",
      "28416\n",
      "28448\n",
      "28480\n",
      "28512\n",
      "28544\n",
      "28576\n",
      "28608\n",
      "28640\n",
      "28672\n",
      "28704\n",
      "28736\n",
      "28768\n",
      "28800\n",
      "28832\n",
      "28864\n",
      "28896\n",
      "28928\n",
      "28960\n",
      "28992\n",
      "29024\n",
      "29056\n",
      "29088\n",
      "29120\n",
      "29152\n",
      "29184\n",
      "29216\n",
      "29248\n",
      "29280\n",
      "29312\n",
      "29344\n",
      "29376\n",
      "29408\n",
      "29440\n",
      "29472\n",
      "29504\n",
      "29536\n",
      "29568\n",
      "29600\n",
      "29632\n",
      "29664\n",
      "29696\n",
      "29728\n",
      "29760\n",
      "29792\n",
      "29824\n",
      "29856\n",
      "29888\n",
      "29920\n",
      "29952\n",
      "29984\n",
      "30016\n",
      "30048\n",
      "30080\n",
      "30112\n",
      "30144\n",
      "30176\n",
      "30208\n",
      "30240\n",
      "30272\n",
      "30304\n",
      "30336\n",
      "30368\n",
      "30400\n",
      "30432\n",
      "30464\n",
      "30496\n",
      "30528\n",
      "30560\n",
      "30592\n",
      "30624\n",
      "30656\n",
      "30688\n",
      "30720\n",
      "30752\n",
      "30784\n",
      "30816\n",
      "30848\n",
      "30880\n",
      "30912\n",
      "30944\n",
      "30976\n",
      "31008\n",
      "31040\n",
      "31072\n",
      "31104\n",
      "31136\n",
      "31168\n",
      "31200\n",
      "31232\n",
      "31264\n",
      "31296\n",
      "31328\n",
      "31360\n",
      "31392\n",
      "31424\n",
      "31456\n",
      "31488\n",
      "31520\n",
      "31552\n",
      "31584\n",
      "31616\n",
      "31648\n",
      "31680\n",
      "31712\n",
      "31744\n",
      "31776\n",
      "31808\n",
      "31840\n",
      "31872\n",
      "31904\n",
      "31936\n",
      "31968\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "# Test for dataloader\n",
    "from bert_implementation_tr.data.data_aux import DataLoaderCustom\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#train_loader = DataLoaderCustom(batch_size=16, block_size=256, split=\"train\", device=\"cuda\", verbose=False)\n",
    "val_loader = DataLoaderCustom(batch_size=32, block_size=512, split=\"train\", device=\"cuda\")\n",
    "acc = 0\n",
    "for batch in range(1000):\n",
    "    return_a = val_loader.next_batch()\n",
    "    if return_a is not None:\n",
    "        acc += return_a[\"input_ids\"].shape[0]\n",
    "        print(acc)\n",
    "    if return_a is None:\n",
    "        print(\"val finished, no update anymore\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 35)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader.current_position_in_shard, val_loader.current_shard_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(step % pretrain_config[\"val_check_interval\"] == 0) or last_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "40 == 0 or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "85 % 55 == 0 or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 % 55 == 0 or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "loading weights from pretrained bert: dbmdz/bert-base-turkish-cased\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForPreTraining(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELU(approximate='none')\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertPreTrainingHeads(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELU(approximate='none')\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=32000, bias=True)\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WITHOUT PAD TOKEN\n",
    "import torch\n",
    "from model import FillMaskPipeline, BertForPreTraining\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from bert_implementation_tr.data.data_aux import DataLoaderCustom\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "\n",
    "# model_hf = AutoModel.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "# model_hf.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "model_from_hf = BertForPreTraining.from_pretrained()\n",
    "model_from_hf.to(device)\n",
    "#fill_mask = FillMaskPipeline(model=model_from_hf, tokenizer=tokenizer, strategy=\"greedy\")\n",
    "#fill_mask([\"Merhaba [MASK] efendi nasÄ±l gidiyor?\", \"[MASK] yaptÄ±n?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights 0:notNext, 1:isNext ->  tensor([0.2428, 0.7572], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "val_loader = DataLoaderCustom(8, 512, device=\"cuda\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = val_loader.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model_from_hf(**x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.6798, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.BertForPreTrainingOutput"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Tensor\n",
      "attention_mask: torch.Tensor\n",
      "token_type_ids: torch.Tensor\n",
      "position_ids: torch.Tensor = None\n",
      "labels: Optional[torch.Tensor] = None\n",
      "next_sentence_label: Optional[torch.Tensor] = None\n",
      "class_weights: Optional[torch.Tensor] = None\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "signature = inspect.signature(model_from_hf.forward)\n",
    "\n",
    "# Parametreleri yazdÄ±r\n",
    "for parametre in signature.parameters.values():\n",
    "    print(parametre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.tokenization_bert.BertTokenizer"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.slow_tokenizer_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.tokenization_bert import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "pretokenizer = pre_tokenizers.Sequence([pre_tokenizers.WhitespaceSplit(), \n",
    "                                               pre_tokenizers.Digits(individual_digits=True),\n",
    "                                               pre_tokenizers.Punctuation()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Merhaba', (0, 7)),\n",
       " ('[', (8, 9)),\n",
       " ('MASK', (9, 13)),\n",
       " (']', (13, 14)),\n",
       " ('efendi', (15, 21)),\n",
       " ('nasÄ±l', (22, 27)),\n",
       " ('gidiyor', (28, 35)),\n",
       " ('?', (35, 36))]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretokenizer.pre_tokenize_str(\"Merhaba [MASK] efendi nasÄ±l gidiyor?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build_and_train_bert_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
