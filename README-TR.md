
ğŸ‡¬ğŸ‡§ [English](./README.md) &nbsp; | &nbsp; ğŸ‡¹ğŸ‡· [TÃ¼rkÃ§e](./README-TR.md)


Proje TanÄ±mÄ±
------------
Bu proje sÄ±fÄ±rdan ``BERT`` modeli oluÅŸturup pretrain taskler Ã¼zerinde eÄŸitim yaparak daha sonraki hedef taskler (downstream tasks) iÃ§in baseline bir model oluÅŸturma
sistemidir. ``BERT`` modeli Transformers mimarisinin sadece encoder kÄ±smÄ± olarak dÃ¼ÅŸÃ¼nÃ¼lebilir. Decoder'Ä±n aksine (GPT gibi modeller) her token context'i sadece solundaki tokenlardan deÄŸil ayrÄ±ca saÄŸ tarafÄ±ndaki tokenlardan da beslenerek oluÅŸur. 


``BERT`` modeli self supervised ÅŸeklinde eÄŸitilir. Girdi sinyalini belirli ÅŸekil veya formda bozarak (veya kapatarak) modelden sinyalin bu kÄ±smÄ± doÄŸru ÅŸekilde tahmin etmesi istenir. Bunun iÃ§in mask language modeling ve next sentence prediction ÅŸekline iki farklÄ± task mevcuttur (multi task learning denebilir).


Sample'lar ÅŸu formata sahiptir: ``[CLS] Seq_A [SEP] Seq_B [SEP]``. Next sentence prediction (NSP) task'i iÃ§in veri setinde 0.5 olasÄ±lÄ±k ile A ve B sequenceler ardÄ±ÅŸÄ±k veya rasgele olacak ÅŸekilde Ã§Ä±kartÄ±lÄ±r. Mask language modeling task'i iÃ§in ise sample Ã¼zerinde (A ve B sequence) belirli olasÄ±lÄ±klar ile rasgele kelimeler seÃ§ilir, ve her kelimenin tokenlarÄ± 3 farklÄ± duruma gÃ¶re deÄŸiÅŸtirilir(*): Mask, Replace, Identity. Mask durumunda kelime tokenlarÄ± [MASK] tokenlarÄ± ile doldurulur, replace durumunda kelime tokenlarÄ± aynÄ± token sayÄ±sÄ±na sahip rasgele seÃ§ilen bir kelimenin tokenlarÄ± ile doldurulur, identity durumunda ise kelime tokenlarÄ± aynÄ± kelimenin tokenlarÄ± ile doldurulur.


(*) : Orijinal [BERT paper](https://arxiv.org/abs/1810.04805)'inda mask, replace durumlarÄ± tek bir token Ã¼zerinden yapÄ±lÄ±rken, Bu projede ``Whole Word Masking`` yÃ¶ntemi uygulanmÄ±ÅŸtÄ±r.




Prepare Dataset
------------
DokÃ¼manlarÄ±n boÅŸ satÄ±rlar ile ayrÄ±ÅŸtÄ±ÄŸÄ± txt dosyalarÄ± ile de Ã§alÄ±ÅŸabilecek bir sistem  (``prepare_data.py``) geliÅŸtirildi. Sistem Ã¶nce raw klasÃ¶rÃ¼ndeki dosyalarÄ± alÄ±r doc shards oluÅŸturur. Daha sonra tÃ¼m bu doc'lar Ã¼zerinde conv operasyonuna benzer bir ÅŸekilde bir pencere hareket ettirerek ab sample'larÄ± oluÅŸturur. Ab shard'larÄ± oluÅŸumundan sonra xy shardlarÄ± ve stat dosyasÄ± oluÅŸturarak veriyi model eÄŸitimine hazÄ±r hale getirir.


Bu sistem default konfigurasyonlar/parametreler Ã§alÄ±ÅŸabileceÄŸi gibi kullanÄ±cÄ± tarafÄ±ndan da belirtilen parametreler/konfigurasyonlar ile de Ã§alÄ±ÅŸabilmektedir.


**Dikkat:** KullanÄ±cÄ±, veri seti oluÅŸturma aÅŸamasÄ±nda kullanÄ±lacak model ile uygun tokenizer tipini ("``custom``" veya "``hf``") doÄŸru belirtmelidir. "``custom``" ile "``hf``" hakkÄ±nda detaylara tokenizer eÄŸitimi ile alakalÄ± baÅŸlÄ±kta eriÅŸilebilir.


**Not:** Pencere Ã§oÄŸu durumda dokÃ¼manÄ±n sonunda dÄ±ÅŸarÄ±da kalacaÄŸÄ±ndan dolayÄ± boÅŸ kalan kÄ±sÄ±mÄ± [PAD] token'i ile doldurmak yerine rasgele B_seq almak tercih edildi (sample notNext oldu yani). Ancak bu durumda sample'lar arasÄ±nda notNext lehine olacak ÅŸekilde en az dokÃ¼man sayÄ±sÄ± kadar fazlalÄ±k oluÅŸtu. Bu durum overlap ve veya block size arttÄ±kÃ§a Ã§ok daha dengesiz bir hal aldÄ± Ã¶yle ki notNext'li sample sayÄ±sÄ± isNext'li sample sayÄ±sÄ±nÄ±n 9 katÄ± kadar olduÄŸu ekstrem durumlar da gÃ¶rÃ¼ldÃ¼. Bu duruma class imbalance denir. SaÄŸlÄ±klÄ±, stabil bir eÄŸitim ve model performansÄ± iÃ§in kÃ¶tÃ¼ bir durumdur. Bunu dengeleyebilmek iÃ§in weighted loss kullanÄ±ldÄ±




Model
------------
Model mimarisinin implementasyonu huggingface'in [modeling_bert.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py )'Ä±na bakÄ±larak yapÄ±ldÄ±. KullanÄ±cÄ± tarafÄ±ndan belirtilen konfigurasyonlara gÃ¶re farklÄ± derinliklerde, ÅŸekillerde, Ã¶zelliklerde vs. model oluÅŸturulabilir. SÄ±fÄ±rdan, parametreleri/aÄŸÄ±rlÄ±klarÄ± random ÅŸekilde initialized edilerek oluÅŸturulan modellere "``custom``" denenebilir. Bu tarz durumlarda kullanÄ±cÄ± istediÄŸi konfigÃ¼rasyonlarÄ± deÄŸiÅŸtirebilir.


"``custom``" olmadÄ±ÄŸÄ± durumlarda huggingface'deki halihazÄ±rda eÄŸitilmiÅŸ [BERTurk](https://huggingface.co/dbmdz/bert-base-turkish-cased) modelin parametrelerini "``custom``" oluÅŸturulan (randomly initialized, default configs) bir modele yÃ¼kleme/transfer etme iÅŸlemi yapÄ±lÄ±r, buna da "``hf``" denenebilir. Bu iÅŸlemin yapÄ±labilmesi iÃ§in model konfigurasyonu default olarak kullanÄ±lmalÄ±dÄ±r (default konfigurasyonlar default ``BERT`` modeline fix'tir) yani kullanÄ±cÄ± farklÄ± bir ayar girmemelidir. Bu "``hf``" modelinin parametre transferinin kolay olabilmesi iÃ§in model parametrelerinin keyleri aynÄ± isimde bÄ±rakÄ±ldÄ±.




Custom Tokenizer EÄŸitimi
------------
HuggingFace [Tokenizers](https://huggingface.co/docs/tokenizers/index) kÃ¼tÃ¼phanesi kullanÄ±larak kullanÄ±cÄ± tarafÄ±ndan belirtilen parametreler/konfigurasyonlar (konfigurasyonlar ile ilgili detaylar altta) ile "raw" klasÃ¶rÃ¼ndeki veriler ile tokenizer eÄŸitilir (``train_tokenizer.py``). Tokenizer pipeline'Ä± klasik Bert tokenizer'Ä±na benzer olup (Ã¶rn aynÄ± model'e sahip: WordPiece), bazÄ± komponentler ile tokenizer'Ä±n ve modelin daha baÅŸarÄ±lÄ± olabileceÄŸi varsayÄ±mÄ± ile deÄŸiÅŸtirilmiÅŸtir. Ã–rneÄŸin normalizer kÄ±smÄ±nda strip accent komponenti konulmamÄ±ÅŸtÄ±r. Pretokenizer'da sadece WhitespaceSplit deÄŸil digits ve punctuation komponentleri de eklenmiÅŸtir. Ancak bu deÄŸiÅŸikliklerin gerÃ§ektende model iÃ§in daha iyi olup olmadÄ±ÄŸÄ± test edilmemiÅŸtir. EÄŸer tokenizer pipeline'Ä±nda farklÄ± komponentler (normalizer, pretokenizer, model vs.) kullanmak veya deÄŸiÅŸtirmek istenirse direkt [train_tokenizer.py](./tokenizer/train_tokenizer.py) Ã¼zerinden deÄŸiÅŸiklik yapÄ±labilir. KullanÄ±cÄ± isteÄŸine gÃ¶re cased veya uncased ÅŸekilde tokenizer eÄŸitilebilir (default cased). Son olarak tokenizer eÄŸitimi sonucu Ã§Ä±kan tokenizer kaydedilir (Ã¶rn: ``tr_wordpiece_tokenizer_cased.json``). 


Hangi model kullanÄ±lacak ise o model iÃ§in uygun olan tokenizer klasÃ¶rde bulundurulmalÄ±dÄ±r. Model olarak "``custom``" kullanÄ±lacak ise bu klasÃ¶rdeki tokenizer kullanÄ±lacak. EÄŸer model "``custom``" deÄŸil de "``hf``" ise o zaman "HuggingFace BERTurk aÄŸÄ±rlÄ±klarÄ±" kullanÄ±lacaÄŸÄ± anlamÄ±na gelir, bu durumda sistem otomatik olarak BERTurk tokenizer'Ä±nÄ± Ã§ekecektir (``AutoTokenizer.from_pretrained("dbmdz/bert-base-turkish-cased") `` ile) 




Random Word Set OluÅŸturma
------------
Veri hazÄ±rlanÄ±rken yukarÄ±da "``replace``" durumlarÄ±ndan bahsedildi. Bir kelime yerine rasgele kelime koymak iÃ§in Ã¶nce kelimenin kaÃ§ tokendan oluÅŸtuÄŸu bilinmelidir. Daha sonra bu token sayÄ±sÄ±na eÅŸit olan rasgele bir kelime seÃ§ilip o kelimenin tokenlarÄ± dÃ¶ndÃ¼rÃ¼lebilmelidir. Bunun dÃ¼zgÃ¼n ve kolay ÅŸekilde saÄŸlanabilmesi iÃ§in kullanÄ±cÄ± tarafÄ±ndan belirtilen parametreler/konfigurasyonlar (konfigurasyonlar ile ilgili detaylar altta) ile sistem tarafÄ±ndan (``prepare_random_word_set.py``), ``random_word_set.json`` dosyasÄ± oluÅŸturulmalÄ±dÄ±r.


Dikkat kullanÄ±cÄ±, kullanÄ±lacak model ile uygun tokenizer tipini ("``custom``" veya "``hf``") doÄŸru belirtmelidir. AyrÄ±ca *use_number_of_line* parametresini bÃ¼yÃ¼k bir sayÄ± ile kullanÄ±mda operasyon baya uzun sÃ¼rebilir. *use_number_of_line* parametresi random word set oluÅŸtururken veri setimizde kaÃ§ line kullanÄ±lacaÄŸÄ±nÄ± belirtir.




Pretrain (MLM, NSP)
------------
Bu sistem (``pretrain_bert.py``) kullanÄ±cÄ± tarafÄ±ndan belirtilen konfigurasyon/parametreler ile pretrain taskler Ã¼zerinde sÄ±fÄ±rdan custom bir model veya hf weight'leri kullanÄ±larak (Ã¶nceden eÄŸitilmiÅŸ, random deÄŸil) eÄŸitim yapÄ±lÄ±r. Ckpt sistemi sayesinde son kalÄ±nÄ±lan noktadan itibaren eÄŸitim devam edebilir. EÄŸitim boyunca Ã¶nceden tanÄ±mlanmÄ±ÅŸ bazÄ± text'ler Ã¼zerinde model Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r ve Ã§Ä±ktÄ±larÄ± "generated_samples.txt" Ã¼zerinde kaydedilir. AyrÄ±ca *train_loss*, *val_loss*, *grad_norm* vs gibi metrik veya deÄŸiÅŸkenler Ã¼zerinde tracking/loglama yapÄ±lÄ±r. KullanÄ±cÄ± ayrÄ±ca konfigÃ¼rasyonlarda ayarlayarak mlflow ile tracking yapabilir. EÄŸtimin/optimizasyonun daha verimli stabil vs olmasÄ± iÃ§in bazÄ± sistemler algoritmalar kullanÄ±ldÄ±: *lr_schedular* ve *lr_warmup* (get_lr fonksiyonu iÃ§inde tanÄ±mlandÄ±lar.), *amp* (automatic mixed precision), *grad_accum*, *clip_norm*, *adamW fused* (weight decay sadece 2d parametre tensorleri Ã¼zerinde yapÄ±lacak ÅŸekilde ayarlandÄ±.). 


Pretrain taskler Ã¼zerinde model eÄŸitimi yapmadan hali hazÄ±rda eÄŸitilmiÅŸ bir model Ã¼zerinde sadece "evaluation" da yapÄ±labilir ("*do_eval_from_best_ckpt*" veya "*samples_for_nsp_generation*" ayarlarÄ± ile). Modeli farklÄ± text'ler Ã¼zerinde denemek istenirse (eÄŸitim esnasÄ±nda veya sadece evaluation'da) *pretrain_bert.py* iÃ§erisindeki "*samples_for_mlm_generation*" ve "*samples_for_nsp_generation*" list objeleri Ã¼zerinde deÄŸiÅŸiklikler yapÄ±labilir (formatlarÄ± bozmamak kaydÄ±yla).


Bu sistemde genel hatlarÄ±yla Andrej Karpathy'nin [build-nanogpt](https://github.com/karpathy/build-nanogpt/blob/master/train_gpt2.py) projesinden ilham alÄ±nmÄ±ÅŸtÄ±r.


**Dikkat**: "resume" en son kalÄ±nan ckpt Ã¼zerinden devam eder, sÄ±fÄ±rdan custom veya hf ile model oluÅŸturmada ise Ã¶nceki eÄŸitimden kalan tÃ¼m ckpt'ler ve log dosyalarÄ± (*log.txt* ve *generated_samples.txt*) resetlenir.




Kurulum
----------
Projenin dÃ¼zgÃ¼n Ã§alÄ±ÅŸabilmesi iÃ§in gerekli paketlerin yÃ¼klenmesi (Ã¶nce sanal env oluÅŸturulmasÄ± Ã¶nerilir):


```sh
pip install -r requirements.txt
```




KonfigÃ¼rasyonlar
----------
``train_tokenizer.py``'da kullanÄ±labilecek parametreler/konfigÃ¼rasyonlar:
```python
@dataclass
class TrainTokenizerConfig:
    vocab_size: int = field(default=32_000, metadata={"description": "Vocab size of token embeddings"})
    limit_alphabet: int = field(default=200, metadata={"description": "Limit for alphabet"})
    min_frequency: int = field(default=2, metadata={"description": "Min frequency"})
    cased: bool = field(default=True, metadata={"description": "Cased or uncased"})
```
```sh
python train_tokenizer.py --vocab_size=32000 --cased=True ...
```




``prepare_data.py``'da kullanÄ±labilecek parametreler/konfigÃ¼rasyonlar:
```python
@dataclass
class DataConfig:
    block_size: int = field(default=128, metadata={"description": "Block size"})
    num_of_docs_per_shard: int = field(default=8_000, metadata={"description": "Number of docs per shard (for doc_shards and ab_shards creation)"})
    num_tokens_per_shard: int = field(default=10_000_000, metadata={"description": "Number of tokens per shard (for xy_shards creation)"})
    overlap: int = field(default=64, metadata={"description": "Overlap, how much overlap between windows when ab samples are generated (suggestion: make half of the block size)"})
    edge_buffer: int = field(default=10, metadata={"description": "prevents the use of a and b seperators near window ends by this many tokens (makes a and b more similar length-wise) (for ab_shards creation)"})
    seed: int = field(default=13013, metadata={"description": "Seed, for reproducibility"})
    rate_of_untouched_words: float = field(default=0.85, metadata={"description": "Rate of untouched words, these are not gonna replaced by [mask, identity, replaced] tokens"})
    mask_ratio: float = field(default=0.80, metadata={"description": "Mask ratio"})
    replace_ratio: float = field(default=0.10, metadata={"description": "Replace ratio"})
    identity_ratio: float = field(default=0.10, metadata={"description": "Identity ratio"})
    tokenizer_type: str = field(default="custom", metadata={"description": "Tokenizer type, Data has been prepared/tokenized with. choices: [custom, hf]"})
```
```sh
python prepare_data.py --block_size=512 --seed=1881 --tokenizer_type=hf ...
```




``prepare_random_word_set.py``'da kullanÄ±labilecek parametreler/konfigÃ¼rasyonlar:
```python
@dataclass
class RandomWordSetConfig:
    limit_for_token_group: int = field(default=5, metadata={"description": "Limit for token group"})
    max_word_limit_for_token_group: int = field(default=5_000, metadata={"description": "Max word limit for token group"})
    min_freq_for_words: int = field(default=150, metadata={"description": "Min freq for words"})
    random_sample: bool = field(default=False, metadata={"description": "Randomly sample words (not directly most frequent ones)"})
    use_number_of_line: int = field(default=None, metadata={"description": "Use specified number of lines for training (if None then use all lines) use small number, otherwise it will take a long time!"})
    tokenizer_type: str = field(default="custom", metadata={"description": "Tokenizer type, random_word_set.json prepared/tokenized with. choices: [custom, hf]"})
```
```sh
python prepare_random_word_set.py --block_size=512 --seed=1881 --tokenizer_type=hf --use_number_of_line=100000 ...
```




``pretrain_bert.py``'da kullanÄ±labilecek parametreler/konfigÃ¼rasyonlar (BertConfig ile PreTrainBertConfig):
```python
@dataclass
class BertConfig:
    vocab_size: int = field(default=32000, metadata={"description": "Vocabulary size of token embeddings"})
    hidden_size: int = field(default=768, metadata={"description": "Hidden size of feed-forward layers"})
    num_hidden_layers: int = field(default=12, metadata={"description": "Number of layers in encoder"})
    num_attention_heads : int = field(default=12, metadata={"description": "Number of attention heads"})
    hidden_act: str = field(default="gelu", metadata={"description": "Activation function type choices: [gelu, relu]"})
    intermediate_size: int = field(default=3072, metadata={"description": "Size of the intermediate layer in feed-forward"}) 
    hidden_dropout_prob: float = field(default=0.1, metadata={"description": "Dropout probability for hidden layers"})
    attention_probs_dropout_prob: float = field(default=0.1, metadata={"description": "Dropout probability for attention"})
    max_position_embeddings: int = field(default=512, metadata={"description": "Maximum number of position embeddings (max block size of the model)"})
    initializer_range: float = field(default=0.02, metadata={"description": "Standard deviation for linear and embedding initialization"})
    layer_norm_eps: float = field(default=1e-12, metadata={"description": "Layer norm epsilon"})
    type_vocab_size: int = field(default=2, metadata={"description": "Type vocabulary size (for token type embeddings)"})
    classifier_dropout: float = field(default=0.1, metadata={"description": "Dropout probability for classifier"})

@dataclass
class PreTrainBertConfig:
    """This is from scratch scenario default config!"""
    do_train_custom: bool = field(default=True, metadata={"description": "Do train with custom model or with BERTurk hf model weights."})
    do_eval_from_best_ckpt: bool = field(default=False, metadata={"description": "Just evaluate the model from_best_ckpt but not training."})
    do_eval_from_huggingface: bool = field(default=False, metadata={"description": "Just evaluate the model from_huggingface but not training."})
    resume: bool = field(default=False, metadata={"description": "Resume training from the last step"})
    stage1_ratio: float = field(default=0.9, metadata={"description": "Ratio of stage1 (e.g. 0.9 means use block_size_s1 and train_batch_size_s1 until the end of %90 training then switch to stage2)"})
    block_size_s1: int = field(default=128, metadata={"description": "Block size for stage1"})
    block_size_s2: int = field(default=512, metadata={"description": "Block size for stage2"})
    train_batch_size_s1: int = field(default=64, metadata={"description": "Training batch size for stage1"})
    train_batch_size_s2: int = field(default=16, metadata={"description": "Training batch size for stage2"})
    val_block_size: int = field(default=512, metadata={"description": "Validation block size"})
    val_batch_size: int = field(default=8, metadata={"description": "Validation batch size"})
    grad_accum_steps: int = field(default=1, metadata={"description": "Gradient accumulation steps (micro steps)"})
    max_learning_rate: float = field(default=1e-4, metadata={"description": "Maximum learning rate"})
    min_learning_rate: float = field(default=1e-4 * 0.001, metadata={"description": "Minimum learning rate"})
    lr_scheduler: str = field(default="cosine", metadata={"description": "Learning rate scheduler choices: [linear, cosine]"})
    num_train_steps: int = field(default=100_000, metadata={"description": "Number of training steps"}) 
    num_warmup_steps: int = field(default=5_000, metadata={"description": "Number of warmup steps"})
    save_checkpoints_steps: int = field(default=50, metadata={"description": "Save checkpoints steps"})
    val_check_steps: int = field(default=50, metadata={"description": "Validation check steps"})
    device: str = field(default="cuda", metadata={"description": "Device choices: [cpu, cuda, mps]"})
    max_eval_steps: int = field(default=50, metadata={"description": "Maximum evaluation steps (if validation set is small then max_eval_steps is gonna be treated as validation set size)"})
    weight_decay: float = field(default=0.01, metadata={"description": "Weight decay (L2 regularization)"})
    max_ckpt: int = field(default=5, metadata={"description": "Maximum number of last checkpoints to keep"})
    seed: int = field(default=13013, metadata={"description": "Random seed"})
    generate_samples: bool = field(default=True, metadata={"description": "Try model on predefined samples"})
    mlflow_tracking: bool = field(default=True, metadata={"description": "MLflow tracking"})      
    tokenizer_type: str = field(default="custom", metadata={"description": "Tokenizer type, Data has been prepared/tokenized with. choices: [custom, hf]"})
```



**Dikkat**: belirtilen "*block_size*" veya "*tokenizer_type*"'lar ile veri setinin hali hazÄ±rda **prepere_data.py** ile Ã¶nceden hazÄ±rlanmÄ±ÅŸ olmasÄ± gerekmektedir



**SÄ±fÄ±rdan custom model oluÅŸturup eÄŸitme senaryosunda** kullanÄ±cÄ± bu parametreler dÄ±ÅŸÄ±nda herhangi bir parametreyi verebilir ``["do_eval_from_best_ckpt", "do_eval_from_huggingface", "tokenizer_type", "resume"]``:
```sh
python pretrain_bert.py --block_size_s1=128 --block_size_s2=512 \
                        --grad_accum_steps=5 --tokenizer_type=custom \
                        --train_batch_size_s1=64 --train_batch_size_s2=16 \
                        --vocab_size=15000 --num_hidden_layers=8 --hidden_size=512 ...
```



**En son kalÄ±nÄ±lan model Ã¼zerinde eÄŸitime devam etme senaryosunda** kullanÄ±cÄ± bu parametreler dÄ±ÅŸÄ±nda herhangi bir ``PreTrainBertConfig`` parametresi verebilir ``["do_train_custom", "do_eval_from_best_ckpt", "do_eval_from_huggingface", "max_ckpt", "tokenizer_type"]``, verdiÄŸi parametreler en sondaki ckpt'nin pretrain_cfg Ã¼zerinde override edilir. Ancak hiÃ§ bir ``BertConfig`` parametresi verilemez, model cfg direkt olarak en sondaki ckpt'den alÄ±nÄ±p kullanÄ±lÄ±r:
```sh
python pretrain_bert.py --resume=True ...
```



**do_eval_from_best_ckpt senaryosu**, kullanÄ±cÄ± sadece bu parametreleri girebilir ``["do_eval_from_best_ckpt", "val_block_size", "val_batch_size", "device", "max_eval_steps", "seed", "generate_samples", "tokenizer_type"]`` (**Dikkat** bazÄ± parametreler fix'tir yani farklÄ± deÄŸer girilemez, Ã¶rn ``--tokenizer_type=custom`` olmak zorunda):
```sh
python pretrain_bert.py --do_eval_from_best_ckpt=True --val_block_size=512 ...
```



**do_eval_from_huggingface senaryosu**, kullanÄ±cÄ± sadece bu parametreleri girebilir `["do_eval_from_huggingface", "val_block_size", "val_batch_size", "device", "max_eval_steps", "seed"]` (**Dikkat** bazÄ± parametreler fix'tir yani farklÄ± deÄŸer girilemez, Ã¶rn ``--tokenizer_type=hf`` olmak zorunda):
```sh
python pretrain_bert.py --do_eval_from_huggingface=True --val_batch_size=32 ...
```







TODO
----------

* Amp iÃ§in bfloat16 precision belirtildi ancak ampere mimarisine sahip olmayan ekran kartlarÄ±nda otomatik olarak float32 kullanÄ±lacaktÄ±r. Bu tarz donanÄ±mlar iÃ§in float16 default olarak seÃ§ilebilmeli ve tabiki bu durum iÃ§in gradscaler kullanÄ±lmalÄ±.

* Modeli direkt kullanabilmek iÃ§in ayrÄ± ve daha temiz bir tÃ¼r arayÃ¼z tasarlanabilir (ÅŸu an pretrain_bert.py'da Ã¶rnek sample'larÄ± elle deÄŸiÅŸtirerek bu mÃ¼mkÃ¼n)

* Train-val split orantÄ±sÄ±nÄ± parametrik/configurable hale getirme. (Sistem ÅŸu an sadece son shard'Ä± val iÃ§in ayÄ±rÄ±yor.) 

* Custom model oluÅŸturma dÄ±ÅŸÄ±nda yalnÄ±z BERTurk hf weightleri kullanÄ±labiliyor. FarklÄ± hf weightleri de kullanÄ±labilmeli (Ã¶rneÄŸin klasik google BERT weightleri de kullanÄ±labilmeli, dÃ¼z "hf" yerine "hf_google_bert", "hf_berturk" vs gibi)

* Class imbalance'Ä± kaldÄ±rmak iÃ§in, sistemin veri setindeki dokÃ¼man sayÄ±sÄ± ile Ã§Ä±kartÄ±lacak sample sayÄ±sÄ±nÄ± Ã¶nceden hesaplayÄ±p ona gÃ¶re randomB oranÄ±nÄ± (default 0.5 idi) ayarlayabilir hale getirmek. Bu sayede isNext ile notNext sample sayÄ±sÄ± 0.5 oranÄ±nda sabit tutulabilir.

* Ddp yapÄ±labilir (data distributed parallel)

* EÄŸitilen modeli deploy edebilme (huggingface'e)

* Pretrain edilen modeli farklÄ± taskler Ã¼zerinde fine-tune edebilme (sentiment analysis, ner, qa, pos vs.) ve buna gÃ¶re model pretraini daha iyi deÄŸerlendirebilme (benchmark 
taskleri, veri setleri vs kullanÄ±labilir)

* Compile (modeli windows Ã¼zerinde compile etmede bazÄ± sÄ±kÄ±ntÄ±lar yaÅŸandÄ± (iliÅŸkili gÃ¼ncel kÃ¼tÃ¼phanenin windows iÃ§in direkt bir daÄŸÄ±tÄ±mÄ± yok) problemler halledilebilirse (wsl kullanma, linux'ta Ã§alÄ±ÅŸtÄ±rma ya da ilgili kÃ¼tÃ¼phanenin son sÃ¼rÃ¼mÃ¼nÃ¼n windows'da dÃ¼zgÃ¼n ÅŸekilde Ã§alÄ±ÅŸabilmesini saÄŸlayacak wheel'in Ã§Ä±kmasÄ± durumunda) compile etme denenebilir ki eÄŸitimi baya hÄ±zlandÄ±racaktÄ±r)




----------

**Author:** *Muhammet Can GÃ¼mÃ¼ÅŸsu*

ğŸ”— [LinkedIn Profilim](https://www.linkedin.com/in/muhammet-canï¿¾g%C3%BCm%C3%BC%C5%9Fsu-876041174/)







